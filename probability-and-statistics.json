[{"title": "R Programming", "course_info": "About this course: In this course you will learn how to program in R and how to use R for effective data analysis. You will learn how to install and configure software necessary for a statistical programming environment and describe generic programming language concepts as they are implemented in a high-level statistical language. The course covers practical issues in statistical computing which includes programming in R, reading data into R, accessing R packages, writing R functions, debugging, profiling R code, and organizing and commenting R code. Topics in statistical data analysis will provide working examples.", "target_audience": "Who is this class for: Some programming experience (in any language) is recommended. ", "created_by": "Johns Hopkins University", "teach_by": [{"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "package_num": "2", "package_name": "Data Science Specialization ", "level": "Intermediate", "rating": "4.5", "week_data": [{"title": "Week 1: Background, Getting Started, and Nuts & Bolts", "description": "This week covers the basics to get you started up with R. The Background Materials lesson contains information about course mechanics and some videos on installing R. The Week 1 videos cover the history of R and S, go over the basic data types in R, and describe the functions for reading and writing data. I recommend that you watch the videos in the listed order, but watching the videos out of order isn't going to ruin the story. ", "video": ["Welcome to R Programming", "About the Instructor", "Pre-Course Survey", "Syllabus", "Course Textbook", "Course Supplement: The Art of Data Science", "Data Science Podcast: Not So Standard Deviations", "Installing R on a Mac", "Installing R on Windows", "Installing R Studio (Mac)", "Writing Code / Setting Your Working Directory (Windows)", "Writing Code / Setting Your Working Directory (Mac)", "Getting Started and R Nuts and Bolts", "Introduction", "Overview and History of R", "Getting Help", "R Console Input and Evaluation", "Data Types - R Objects and Attributes", "Data Types - Vectors and Lists", "Data Types - Matrices", "Data Types - Factors", "Data Types - Missing Values", "Data Types - Data Frames", "Data Types - Names Attribute", "Data Types - Summary", "Reading Tabular Data", "Reading Large Tables", "Textual Data Formats", "Connections: Interfaces to the Outside World", "Subsetting - Basics", "Subsetting - Lists", "Subsetting - Matrices", "Subsetting - Partial Matching", "Subsetting - Removing Missing Values", "Vectorized Operations", "Introduction to swirl", "Practical R Exercises in swirl Part 1", "swirl Lesson 1: Basic Building Blocks", "swirl Lesson 2: Workspace and Files", "swirl Lesson 3: Sequences of Numbers", "swirl Lesson 4: Vectors", "swirl Lesson 5: Missing Values", "swirl Lesson 6: Subsetting Vectors", "swirl Lesson 7: Matrices and Data Frames", "Week 1 Quiz"]}, {"title": "Week 2: Programming with R", "description": "Welcome to Week 2 of R Programming. This week, we take the gloves off, and the lectures cover key topics like control structures and functions. We also introduce the first programming assignment for the course, which is due at the end of the week.", "video": ["Week 2: Programming with R", "Control Structures - Introduction", "Control Structures - If-else", "Control Structures - For loops", "Control Structures - While loops", "Control Structures - Repeat, Next, Break", "Your First R Function", "Functions (part 1)", "Functions (part 2)", "Scoping Rules - Symbol Binding", "Scoping Rules - R Scoping Rules", "Scoping Rules - Optimization Example (OPTIONAL)", "Coding Standards", "Dates and Times", "Practical R Exercises in swirl Part 2", "swirl Lesson 1: Logic", "swirl Lesson 2: Functions", "swirl Lesson 3: Dates and Times", "Programming Assignment 1 INSTRUCTIONS: Air Pollution", "Week 2 Quiz", "Programming Assignment 1: Quiz"]}, {"title": "Week 3: Loop Functions and Debugging", "description": "We have now entered the third week of R Programming, which also marks the halfway point. The lectures this week cover loop functions and the debugging tools in R. These aspects of R make R useful for both interactive work and writing longer code, and so they are commonly used in practice.", "video": ["Week 3: Loop Functions and Debugging", "Loop Functions - lapply", "Loop Functions - apply", "Loop Functions - mapply", "Loop Functions - tapply", "Loop Functions - split", "Debugging Tools - Diagnosing the Problem", "Debugging Tools - Basic Tools", "Debugging Tools - Using the Tools", "Practical R Exercises in swirl Part 3", "swirl Lesson 1: lapply and sapply", "swirl Lesson 2: vapply and tapply", "Week 3 Quiz", "Programming Assignment 2: Lexical Scoping "]}, {"title": "Week 4: Simulation & Profiling", "description": "This week covers how to simulate data in R, which serves as the basis for doing simulation studies. We also cover the profiler in R which lets you collect detailed information on how your R functions are running and to identify bottlenecks that can be addressed. The profiler is a key tool in helping you optimize your programs. Finally, we cover the str function, which I personally believe is the most useful function in R.", "video": ["Week 4: Simulation & Profiling", "The str Function", "Simulation - Generating Random Numbers", "Simulation - Simulating a Linear Model", "Simulation - Random Sampling", "R Profiler (part 1)", "R Profiler (part 2)", "Practical R Exercises in swirl Part 4", "swirl Lesson 1: Looking at Data", "swrl Lesson 2: Simulation", "swirl Lesson 3: Base Graphics", "Programming Assignment 3 INSTRUCTIONS: Hospital Quality", "Post-Course Survey", "Week 4 Quiz", "Programming Assignment 3: Quiz"]}]}, {"title": "Introduction to Probability and Data", "course_info": "About this course: This course introduces you to sampling and exploring data, as well as basic probability theory and Bayes' rule. You will examine various types of sampling methods, and discuss how such methods can impact the scope of inference. A variety of exploratory data analysis techniques will be covered, including numeric summary statistics and basic data visualization. You will be guided through installing and using R and RStudio (free statistical software), and will use this software for lab exercises and a final project. The concepts and techniques in this course will serve as building blocks for the inference and modeling courses in the Specialization.", "target_audience": null, "created_by": "Duke University", "teach_by": [{"name": "Mine Çetinkaya-Rundel", "department": "Department of Statistical Science"}], "package_num": "1", "package_name": "Statistics with R Specialization ", "level": "Beginner", "rating": "4.7", "week_data": [{"title": "About Introduction to Probability and Data", "description": "<p>This course introduces you to sampling and exploring data, as well as basic probability theory. You will examine various types of sampling methods and discuss how such methods can impact the utility of a data analysis. The concepts in this module will serve as building blocks for our later courses.<p>Each lesson comes with a set of learning objectives that will be covered in a series of short videos. Supplementary readings and practice problems will also be suggested from <a href=\"https://leanpub.com/openintro-statistics/\" target=\"_blank\">OpenIntro Statistics, 3rd Edition</a> (a free online introductory statistics textbook, that I co-authored). There will be weekly quizzes designed to assess your learning and mastery of the material covered that week in the videos. In addition, each week will also feature a lab assignment, in which you will use R to apply what you are learning to real data. There will also be a data analysis project designed to enable you to answer research questions of your own choosing.<p>Since this is a Coursera course, you are welcome to participate as much or as little as you’d like, though I hope that you will begin by participating fully. One of the most rewarding aspects of a Coursera course is participation in forum discussions about the course materials. Please take advantage of other students' feedback and insight and contribute your own perspective where you see fit to do so. You can also check out the <a href=\"https://www.coursera.org/learn/probability-intro/resources/crMc4\" target=\"_blank\">resource page</a> listing useful resources for this course. <p>Thank you for joining the Introduction to Probability and Data community! Say hello in the Discussion Forums. We are looking forward to your participation in the course.</p>", "video": ["Introduction to Statistics with R", "More about Introduction to Probability and Data", "Feedback Surveys"]}, {"title": "Introduction to Data", "description": "<p>Welcome to Introduction to Probability and Data! I hope you are just as excited about this course as I am! In the next five weeks, we will learn about designing studies, explore data via numerical summaries and visualizations, and learn about rules of probability and commonly used probability distributions. If you have any questions, feel free to post them on <a href=\"https://www.coursera.org/learn/probability-intro/module/rQ9Al/discussions?sort=lastActivityAtDesc&page=1\" target=\"_blank\"><b>this module's forum</b></a> and discuss with your peers! To get started, view the <a href=\"https://www.coursera.org/learn/probability-intro/supplement/rooeY/lesson-learning-objectives\" target=\"_blank\"><b>learning objectives</b></a> of Lesson 1 in this module.</p>", "video": ["Lesson Learning Objectives", "Introduction", "Data Basics", "Observational Studies & Experiments", "Sampling and sources of bias", "Experimental Design", "(Spotlight) Random Sample Assignment", "Suggested Readings and Practice", "Week 1 Practice Quiz", "About Lesson Choices (Read Before Selection)", "Week 1 Lab Instructions (RStudio)", "Feedback survey", "Week 1 Quiz", "Week 1 Lab: Introduction to R and RStudio"]}, {"title": "Exploratory Data Analysis and Introduction to Inference", "description": "<p>Welcome to Week 2 of Introduction to Probability and Data! Hope you enjoyed materials from Week 1. This week we will delve into numerical and categorical data in more depth, and introduce inference. </p>", "video": ["Lesson Learning Objectives", "Visualizing Numerical Data", "Measures of Center", "Measures of Spread", "Robust Statistics", "Transforming Data", "Lesson Learning Objectives", "Exploring Categorical Variables", "Introduction to Inference", "Suggested Readings and Practice", "Week 2 Practice Quiz", "Week 2 Lab Instructions (RStudio)", "Feedback survey", "Week 2 Quiz", "Week 2 Lab: Introduction to Data"]}, {"title": "Introduction to Probability", "description": "<p>Welcome to Week 3 of Introduction to Probability and Data! Last week we explored numerical and categorical data. This week we will discuss probability, conditional probability, the Bayes’ theorem, and provide a light introduction to Bayesian inference. </p><p>Thank you for your enthusiasm and participation, and have a great week! I’m looking forward to working with you on the rest of this course. </p>", "video": ["Lesson Learning Objectives", "Introduction", "Disjoint Events + General Addition Rule", "Independence", "Probability Examples", "(Spotlight) Disjoint vs. Independent", "Lesson Learning Objectives", "Conditional Probability", "Probability Trees", "Bayesian Inference", "Examples of Bayesian Inference", "Suggested Readings and Practice", "Week 3 Practice Quiz", "Week 3 Lab Instructions (RStudio)", "Feedback survey", "Week 3 Quiz", "Week 3 Lab: Probability"]}, {"title": "Probability Distributions", "description": "<p>Great work so far! Welcome to Week 4 -- the last content week of Introduction to Probability and Data! This week we will introduce two probability distributions: the normal and the binomial distributions in particular. As usual, you can evaluate your knowledge in this week's quiz. There will be <b>no labs</b> for this week. Please don't hesitate to post any questions, discussions and related topics on <a href=\"https://www.coursera.org/learn/probability-intro/module/VdVNg/discussions?sort=lastActivityAtDesc&page=1\" target=\"_blank\"><b>this week's forum</b></a>.</p>", "video": ["Lesson Learning Objectives", "Normal Distribution", "Evaluating the Normal Distribution", "Working with the Normal Distribution", "Lesson Learning Objectives", "Binomial Distribution", "Normal Approximation to Binomial", "Working with the Binomial Distribution", "Suggested Readings and Practice", "Week 4 Practice Quiz", "Feedback survey", "Data Analysis Project Example", "Week 4 Quiz"]}, {"title": "Data Analysis Project", "description": "<p>Well done! You have reached the last week of Introduction to Probability and Data! There will not be any new videos in this week, instead, you will be asked to complete an initial data analysis project with a real-world data set. The project is designed to help you discover and explore research questions of your own, using real data and statistical methods we learn in this class. The the project will be graded via peer assessments, meaning that you will need to evaluate three peers' projects after submitting your own.</p><p>Get started with your data analysis in this week! It should be interesting and very exciting! As usual, feel free to post questions, concerns, and comments about the project on <a href=\"https://www.coursera.org/learn/probability-intro/module/BaTDb/discussions?sort=lastActivityAtDesc&page=1\" target=\"_blank\"><b>this week's forum</b></a>.</p> ", "video": ["Project Information", "Feedback survey", "Data Analysis Project"]}]}, {"title": "Bayesian Statistics: From Concept to Data Analysis", "course_info": "About this course: This course introduces the Bayesian approach to statistics, starting with the concept of probability and moving to the analysis of data. We will learn about the philosophy of the Bayesian approach as well as how to implement it for common types of data. We will compare the Bayesian approach to the more commonly-taught Frequentist approach, and see some of the benefits of the Bayesian approach. In particular, the Bayesian approach allows for better accounting of uncertainty, results that have more intuitive and interpretable meaning, and more explicit statements of assumptions. This course combines lecture videos, computer demonstrations, readings, exercises, and discussion boards to create an active learning experience. For computing, you have the choice of using Microsoft Excel or the open-source, freely available statistical package R, with equivalent content for both options. The lectures provide some of the basic mathematical development as well as explanations of philosophy and interpretation. Completion of this course will give you an understanding of the concepts of the Bayesian approach, understanding the key differences between Bayesian and Frequentist approaches, and the ability to do basic data analyses.", "target_audience": "Who is this class for: This course is for people interested in learning an alternative to the Frequentist approach that is typically taught in statistics classes. The course covers both concepts and basic computing, and so it is applicable both to people doing data analysis as well as people who read the analysis of others, such as decision makers.\n\nThis course expects that learners have previous exposure to statistics at the introductory level or higher, and previous exposure to calculus. In both cases, the expectation is that concepts have been seen previously, possibly many years earlier, but that the details may have been forgotten. ", "created_by": "University of California, Santa Cruz", "teach_by": [{"name": "Herbert Lee", "department": "Applied Mathematics and Statistics"}], "package_num": null, "package_name": null, "level": "Intermediate", "rating": "4.5", "week_data": [{"title": "Probability and Bayes' Theorem", "description": "In this module, we review the basics of probability and Bayes’ theorem. In Lesson 1, we introduce the different paradigms or definitions of probability and discuss why probability provides a coherent framework for dealing with uncertainty. In Lesson 2, we review the rules of conditional probability and introduce Bayes’ theorem. Lesson 3 reviews common probability distributions for discrete and continuous random variables.", "video": ["Course introduction", "Module 1 objectives, assignments, and supplementary materials", "Background for Lesson 1", "Lesson 1.1 Classical and frequentist probability", "Lesson 1.2 Bayesian probability and coherence", "Objectivity", "Lesson 2.1 Conditional probability", "Lesson 2.2 Bayes' theorem", "Supplementary material for Lesson 2", "Lesson 3.1 Bernoulli and binomial distributions", "Lesson 3.2 Uniform distribution", "Lesson 3.3 Exponential and normal distributions", "Supplementary material for Lesson 3", "Lesson 1", "Lesson 2", "Lesson 3.1", "Lesson 3.2-3.3", "Module 1 Honors"]}, {"title": "Statistical Inference", "description": "This module introduces concepts of statistical inference from both frequentist and Bayesian perspectives. Lesson 4 takes the frequentist view, demonstrating maximum likelihood estimation and confidence intervals for binomial data. Lesson 5 introduces the fundamentals of Bayesian inference. Beginning with a binomial likelihood and prior probabilities for simple hypotheses, you will learn how to use Bayes’ theorem to update the prior with data to obtain posterior probabilities. This framework is extended with the continuous version of Bayes theorem to estimate continuous model parameters, and calculate posterior probabilities and credible intervals.", "video": ["Module 2 objectives, assignments, and supplementary materials", "Background for Lesson 4", "Lesson 4.1 Confidence intervals", "Lesson 4.2 Likelihood function and maximum likelihood", "Lesson 4.3 Computing the MLE", "Lesson 4.4 Computing the MLE: examples", "Supplementary material for Lesson 4", "Introduction to R", "Plotting the likelihood in R", "Plotting the likelihood in Excel", "Background for Lesson 5", "Lesson 5.1 Inference example: frequentist", "Lesson 5.2 Inference example: Bayesian", "Lesson 5.3 Continuous version of Bayes' theorem", "Lesson 5.4 Posterior intervals", "Confidence intervals and credible intervals", "Supplementary material for Lesson 5", "Lesson 4", "Lesson 5.1-5.2", "Lesson 5.3-5.4", "Module 2 Honors"]}, {"title": "Priors and Models for Discrete Data", "description": "In this module, you will learn methods for selecting prior distributions and building models for discrete data. Lesson 6 introduces prior selection and predictive distributions as a means of evaluating priors. Lesson 7 demonstrates Bayesian analysis of Bernoulli data and introduces the computationally convenient concept of conjugate priors. Lesson 8 builds a conjugate model for Poisson data and discusses strategies for selection of prior hyperparameters.", "video": ["Module 3 objectives, assignments, and supplementary materials", "Lesson 6.1 Priors and prior predictive distributions", "Lesson 6.2 Prior predictive: binomial example", "Lesson 6.3 Posterior predictive distribution", "Lesson 7.1 Bernoulli/binomial likelihood with uniform prior", "Lesson 7.2 Conjugate priors", "Lesson 7.3 Posterior mean and effective sample size", "Data analysis example in R", "Data analysis example in Excel", "Prior elicitation", "R and Excel code from example analysis", "Lesson 8.1 Poisson data", "Lesson 6", "Lesson 7", "Lesson 8", "Module 3 Honors"]}, {"title": "Models for Continuous Data", "description": "This module covers conjugate and objective Bayesian analysis for continuous data. Lesson 9 presents the conjugate model for exponentially distributed data. Lesson 10 discusses models for normally distributed data, which play a central role in statistics. In Lesson 11, we return to prior selection and discuss ‘objective’ or ‘non-informative’ priors. Lesson 12 presents Bayesian linear regression with non-informative priors, which yield results comparable to those of classical regression.\n", "video": ["Module 4 objectives, assignments, and supplementary materials", "Lesson 9.1 Exponential data", "Lesson 10.1 Normal likelihood with variance known", "Lesson 10.2 Normal likelihood with variance unknown", "Supplementary material for Lesson 10", "Lesson 11.1 Non-informative priors", "Lesson 11.2 Jeffreys prior", "A non-informative prior", "Supplementary material for Lesson 11", "Background for Lesson 12", "Linear regression in R", "Linear regression in Excel (Analysis ToolPak)", "Linear regression in Excel (StatPlus by AnalystSoft)", "R and Excel code for regression", "Conclusion", "Lesson 9", "Lesson 10", "Lesson 11", "Regression", "Module 4 Honors"]}]}, {"title": "Getting and Cleaning Data", "course_info": "About this course: Before you can work with data you have to get some. This course will cover the basic ways that data can be obtained. The course will cover obtaining data from the web, from APIs, from databases and from colleagues in various formats. It will also cover the basics of data cleaning and how to make data “tidy”. Tidy data dramatically speed downstream data analysis tasks. The course will also cover the components of a complete data set including raw data, processing instructions, codebooks, and processed data. The course will cover the basics needed for collecting, cleaning, and sharing data.", "target_audience": null, "created_by": "Johns Hopkins University", "teach_by": [{"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "package_num": "3", "package_name": "Data Science Specialization ", "level": null, "rating": "4.5", "week_data": [{"title": "Week 1", "description": "In this first week of the course, we look at finding data and reading different file types.", "video": ["Welcome to Week 1", "Syllabus", "Pre-Course Survey", "Obtaining Data Motivation", "Raw and Processed Data", "Components of Tidy Data", "Downloading Files", "Reading Local Files", "Reading Excel Files", "Reading XML", "Reading JSON", "The data.table Package", "Practical R Exercises in swirl Part 1", "Week 1 Quiz"]}, {"title": "Week 2", "description": "Welcome to Week 2 of Getting and Cleaning Data! The primary goal is to introduce you to the most common data storage systems and the appropriate tools to extract data from web or from databases like MySQL. ", "video": ["Reading from MySQL", "Reading from HDF5", "Reading from The Web", "Reading From APIs", "Reading From Other Sources", "Week 2 Quiz"]}, {"title": "Week 3", "description": "Welcome to Week 3 of Getting and Cleaning Data! This week the lectures will focus on organizing, merging and managing the data you have collected using the lectures from Weeks 1 and 2. ", "video": ["Subsetting and Sorting", "Summarizing Data", "Creating New Variables", "Reshaping Data", "Managing Data Frames with dplyr - Introduction", "Managing Data Frames with dplyr - Basic Tools", "Merging Data", "Practical R Exercises in swirl Part 2", "swirl Lesson 1: Manipulating Data with dplyr", "swirl Lesson 2: Grouping and Chaining with dplyr", "swirl Lesson 3: Tidying Data with tidyr", "Week 3 Quiz"]}, {"title": "Week 4", "description": "Welcome to Week 4 of Getting and Cleaning Data! This week we finish up with lectures on text and date manipulation in R. In this final week we will also focus on peer grading of Course Projects. \n", "video": ["Editing Text Variables", "Regular Expressions I", "Regular Expressions II", "Working with Dates", "Data Resources", "Practical R Exercises in swirl Part 4", "swirl Lesson 1: Dates and Times with lubridate", "Post-Course Survey", "Week 4 Quiz", "Getting and Cleaning Data Course Project"]}]}, {"title": "Basic Statistics", "course_info": "About this course: Understanding statistics is essential to understand research in the social and behavioral sciences. In this course you will learn the basics of statistics; not just how to calculate them, but also how to evaluate them. This course will also prepare you for the next course in the specialization - the course Inferential Statistics. \n\nIn the first part of the course we will discuss methods of descriptive statistics. You will learn what cases and variables are and how you can compute measures of central tendency (mean, median and mode) and dispersion (standard deviation and variance). Next, we discuss how to assess relationships between variables, and we introduce the concepts correlation and regression. \n\nThe second part of the course is concerned with the basics of probability: calculating probabilities, probability distributions and sampling distributions. You need to know about these things in order to understand how inferential statistics work. \n\nThe third part of the course consists of an introduction to methods of inferential statistics - methods that help us decide whether the patterns we see in our data are strong enough to draw conclusions about the underlying population we are interested in. We will discuss confidence intervals and significance tests.\n\nYou will not only learn about all these statistical concepts, you will also be trained to calculate and generate these statistics yourself using freely available statistical software.", "target_audience": null, "created_by": "University of Amsterdam", "teach_by": [{"name": "Matthijs Rooduijn", "department": "Department of Political Science"}, {"name": "Emiel van Loon", "department": "Institute for Biodiversity and Ecosystem Dynamics"}], "package_num": "3", "package_name": "Methods and Statistics in Social Sciences Specialization ", "level": "Beginner", "rating": "4.7", "week_data": [{"title": "Before we get started...", "description": "In this module we'll consider the basics of statistics. But before we start, we'll give you a broad sense of what the course is about and how it's organized. Are you new to Coursera or still deciding whether this is the course for you? Then make sure to check out the 'Course introduction' and 'What to expect from this course' sections below, so you'll have the essential information you need to decide and to do well in this course! If you have any questions about the course format, deadlines or grading, you'll probably find the answers here. Are you a Coursera veteran and ready to get started? Then you might want to skip ahead to the first course topic: 'Exploring data'. You can always check the general information later. Veterans and newbies alike: Don't forget to introduce yourself in the 'meet and greet' forum!", "video": ["Hi there!", "Welcome to Basic Statistics!", "How to navigate this course", "How to contribute", "General info - What will I learn in this course?", "Course format - How is this course structured?", "Requirements - What resources do I need?", "Grading - How do I pass this course?", "Team - Who created this course?", "Honor Code - Integrity in this course", "Useful literature and documents", "Research on Feedback", "Use of your data for research"]}, {"title": "Exploring Data", "description": "In this first module, we’ll introduce the basic concepts of descriptive statistics. We’ll talk about cases and variables, and we’ll explain how you can order them in a so-called data matrix. We’ll discuss various levels of measurement and we’ll show you how you can present your data by means of tables and graphs. We’ll also introduce measures of central tendency (like mode, median and mean) and dispersion (like range, interquartile range, variance and standard deviation). We’ll not only tell you how to interpret them; we’ll also explain how you can compute them. Finally, we’ll tell you more about z-scores. In this module we’ll only discuss situations in which we analyze one single variable. This is what we call univariate analysis. In the next module we will also introduce studies in which more variables are involved.", "video": ["Data and visualisation", "1.01 Cases, variables and levels of measurement", "1.02 Data matrix and frequency table", "1.03 Graphs and shapes of distributions", "Measures of central tendency and dispersion", "1.04 Mode, median and mean", "1.05 Range, interquartile range and box plot", "1.06 Variance and standard deviation", "Z-scores and example", "1.07 Z-scores", "1.08 Example", "Transcripts - Exploring data", "About the R labs", "R lab - Getting started (part 1)", "R lab - Getting started (part 2)", "Exploring Data", "R lab - Exploring data"]}, {"title": "Correlation and Regression", "description": "In this second module we’ll look at bivariate analyses: studies with two variables. First we’ll introduce the concept of correlation. We’ll investigate contingency tables (when it comes to categorical variables) and scatterplots (regarding quantitative variables). We’ll also learn how to understand and compute one of the most frequently used measures of correlation: Pearson's r. In the next part of the module we’ll introduce the method of OLS regression analysis. We’ll explain how you (or the computer) can find the regression line and how you can describe this line by means of an equation. We’ll show you that you can assess how well the regression line fits your data by means of the so-called r-squared. We conclude the module with a discussion of why you should always be very careful when interpreting the results of a regression analysis.     ", "video": ["Correlation", "2.01 Crosstabs and scatterplots", "2.02 Pearson's r", "Regression", "2.03 Regression - Finding the line", "2.04 Regression - Describing the line", "2.05 Regression - How good is the line?", "Reference", "Caveats and examples", "2.06 Correlation is not causation", "2.07 Example contingency table", "2.08 Example Pearson's r and regression", "Reference", "Transcripts - Correlation and regression", "Correlation and Regression", "R lab - Correlation and Regression"]}, {"title": "Probability", "description": "This module introduces concepts from probability theory and the rules for calculating with probabilities. This is not only useful for answering various kinds of applied statistical questions but also to understand the statistical analyses that will be introduced in subsequent modules. We start by describing randomness, and explain how random events surround us. Next, we provide an intuitive definition of probability through an example and relate this to the concepts of events, sample space and random trials. A graphical tool to understand these concepts is introduced here as well, the tree-diagram.Thereafter a number of concepts from set theory are explained and related to probability calculations. Here the relation is made to tree-diagrams again, as well as contingency tables. We end with a lesson where conditional probabilities, independence and Bayes rule are explained. All in all, this is quite a theoretical module on a topic that is not always easy to grasp. That's why we have included as many intuitive examples as possible.", "video": ["Probability & randomness", "3.01 Randomness", "3.02 Probability", "Sample space, events & tree diagrams", "3.03 Sample space, event, probability of event and tree diagram", "3.04 Quantifying probabilities with tree diagram", "Probability & sets", "3.05 Basic set-theoretic concepts", "3.06 Practice with sets", "3.07 Union", "Conditional probability & independence", "3.08 Joint and marginal probabilities", "3.09 Conditional probability", "3.10 Independence between random events", "3.11 More conditional probability, decision trees and Bayes' Law", "Transcripts - Probability", "Probability", "R lab - Probability"]}, {"title": "Probability Distributions", "description": "Probability distributions form the core of many statistical calculations. They are used as mathematical models to represent some random phenomenon and subsequently answer statistical questions about that phenomenon. This module starts by explaining the basic properties of a probability distribution, highlighting how it quantifies a random variable and also pointing out how it differs between discrete and continuous random variables. Subsequently the cumulative probability distribution is introduced and its properties and usage are explained as well. In a next lecture it is shown how a random variable with its associated probability distribution can be characterized by statistics like a mean and variance, just like observational data. The effects of changing random variables by multiplication or addition on these statistics are explained as well.The lecture thereafter introduces the normal distribution, starting by explaining its functional form and some general properties. Next, the basic usage of the normal distribution to calculate probabilities is explained. And in a final lecture the binomial distribution, an important probability distribution for discrete data, is introduced and further explained. By the end of this module you have covered quite some ground and have a solid basis to answer the most frequently encountered statistical questions. Importantly, the fundamental knowledge about probability distributions that is presented here will also provide a solid basis to learn about inferential statistics in the next modules.", "video": ["Probability distributions", "4.01 Random variables and probability distributions", "4.02 Cumulative probability distributions", "Mean and variance of a random variable", "4.03 The mean of a random variable", "4.04 Variance of a random variable", "The normal distribution", "4.05 Functional form of the normal distribution", "4.06 The normal distribution: probability calculations", "4.07 The standard normal distribution", "The binomial distribution", "4.08 The binomial distribution", "Transcripts - Probability distributions", "Probability distributions", "R lab - Probability distributions"]}, {"title": "Sampling Distributions", "description": "Methods for summarizing sample data are called descriptive statistics. However, in most studies we’re not interested in samples, but in underlying populations. If we employ data obtained from a sample to draw conclusions about a wider population, we are using methods of inferential statistics. It is therefore of essential importance that you know how you should draw samples. In this module we’ll pay attention to good sampling methods as well as some poor practices. To draw conclusions about the population a sample is from, researchers make use of a probability distribution that is very important in the world of statistics: the sampling distribution. We’ll discuss sampling distributions in great detail and compare them to data distributions and population distributions. We’ll look at the sampling distribution of the sample mean and the sampling distribution of the sample proportion. ", "video": ["Sample and sampling", "5.01 Sample and population", "5.02 Sampling", "Sampling distribution of sample mean and central limit theorem", "5.03 The sampling distribution", "5.04 The central limit theorem", "5.05 Three distributions", "Reference", "Sampling distribution of sample proportion and example", "5.06 Sampling distribution proportion", "5.07 Example", "Transcripts - Sampling distributions", "Sampling distributions", "R lab - Sampling distributions"]}, {"title": "Confidence Intervals", "description": "We can distinguish two types of statistical inference methods. We can: (1) estimate population parameters; and (2) test hypotheses about these parameters. In this module we’ll talk about the first type of inferential statistics: estimation by means of a confidence interval. A confidence interval is a range of numbers, which, most likely, contains the actual population value. The  probability that the interval actually contains the population value is what we call the confidence level. In this module we’ll show you how you can construct confidence intervals for means and proportions and how you should interpret them. We’ll also pay attention to how you can decide how large your sample size should be.", "video": ["Inference and confidence interval for mean", "6.01 Statistical inference", "6.02 CI for mean with known population sd", "6.03 CI for mean with unknown population sd", "Confidence interval for proportion and confidence levels", "6.04 CI for proportion", "6.05 Confidence levels", "Sample size and example", "6.06 Choosing the sample size", "6.07 Example", "Transcripts - Confidence intervals", "Confidence intervals", "R lab - Confidence intervals"]}, {"title": "Significance Tests", "description": "In this module we’ll talk about statistical hypotheses. They form the main ingredients of the method of significance testing. An hypothesis is nothing more than an expectation about a population. When we conduct a significance test, we use (just like when we construct a confidence interval) sample data to draw inferences about population parameters. The significance test is, therefore, also a method of inferential statistics. We’ll show that each significance test is based on two hypotheses:  the null hypothesis and  the alternative hypothesis. When you do a significance test, you assume that the null hypothesis is true unless your data provide strong evidence against it. We’ll show you how you can conduct a significance test about a mean and how you can conduct a test about a proportion. We’ll also demonstrate that significance tests and confidence intervals are closely related. We conclude the module by arguing that you can make right and wrong decisions while doing a test. Wrong decisions are referred to as Type I and Type II errors.", "video": ["Hypotheses and significance tests", "7.01 Hypotheses", "7.02 Test about proportion", "7.03 Test about mean", "Step-by-step plan and confidence interval", "7.04 Step-by-step plan", "7.05 Significance test and confidence interval", "Type I and Type II errors and example", "7.06 Type I and Type II errors", "7.07 Example", "Transcripts - Significance tests", "Significance tests", "R lab - Significance tests"]}, {"title": "Exam time!", "description": "This is the final module, where you can apply everything you've learned until now in the final exam. Please note that you can only take the final exam once a month, so make sure you are fully prepared to take the test. Please follow the honor code and do not communicate or confer with others while taking this exam. Good luck! ", "video": ["Final Exam"]}]}, {"title": "Exploratory Data Analysis", "course_info": "About this course: This course covers the essential exploratory techniques for summarizing data. These techniques are typically applied before formal modeling commences and can help inform the development of more complex statistical models. Exploratory techniques are also important for eliminating or sharpening potential hypotheses about the world that can be addressed by the data. We will cover in detail the plotting systems in R as well as some of the basic principles of constructing data graphics. We will also cover some of the common multivariate statistical techniques used to visualize high-dimensional data.", "target_audience": null, "created_by": "Johns Hopkins University", "teach_by": [{"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "package_num": "4", "package_name": "Data Science Specialization ", "level": null, "rating": "4.6", "week_data": [{"title": "Week 1", "description": "This week covers the basics of analytic graphics and the base plotting system in R. We've also included some background material to help you install R if you haven't done so already. ", "video": ["Welcome to Exploratory Data Analysis", "Syllabus", "Pre-Course Survey", "Introduction", "Exploratory Data Analysis with R Book", "The Art of Data Science", "Installing R on Windows (3.2.1)", "Installing R on a Mac (3.2.1)", "Installing R Studio (Mac)", "Setting Your Working Directory (Windows)", "Setting Your Working Directory (Mac)", "Principles of Analytic Graphics", "Exploratory Graphs (part 1)", "Exploratory Graphs (part 2) ", "Plotting Systems in R", "Base Plotting System (part 1)", "Base Plotting System (part 2)", "Base Plotting Demonstration", "Graphics Devices in R (part 1)", "Graphics Devices in R (part 2)", "Practical R Exercises in swirl Part 1", "swirl Lesson 1: Principles of Analytic Graphs", "swirl Lesson 2: Exploratory Graphs", "swirl Lesson 3: Graphics Devices in R", "swirl Lesson 4: Plotting Systems", "swirl Lesson 5: Base Plotting System", "Week 1 Quiz", "Course Project 1"]}, {"title": "Week 2", "description": "Welcome to Week 2 of Exploratory Data Analysis. This week covers some of the more advanced graphing systems available in R: the Lattice system and the ggplot2 system. While the base graphics system provides many important tools for visualizing data, it was part of the original R system and lacks many features that may be desirable in a plotting system, particularly when visualizing high dimensional data. The Lattice and ggplot2 systems also simplify the laying out of plots making it a much less tedious process.", "video": ["Lattice Plotting System (part 1)", "Lattice Plotting System (part 2)", "ggplot2 (part 1)", "ggplot2 (part 2)", "ggplot2 (part 3)", "ggplot2 (part 4)", "ggplot2 (part 5)", "Practical R Exercises in swirl Part 2", "swirl Lesson 1: Lattice Plotting System", "swirl Lesson 2: Working with Colors", "swirl Lesson 3: GGPlot2 Part1", "swirl Lesson 4: GGPlot2 Part2", "swirl Lesson 5: GGPlot2 Extras", "Week 2 Quiz"]}, {"title": "Week 3", "description": "Welcome to Week 3 of Exploratory Data Analysis. This week covers some of the workhorse statistical methods for exploratory analysis. These methods include clustering and dimension reduction techniques that allow you to make graphical displays of very high dimensional data (many many variables). We also cover novel ways to specify colors in R so that you can use color as an important and useful dimension when making data graphics. All of this material is covered in chapters 9-12 of my book Exploratory Data Analysis with R.", "video": ["Hierarchical Clustering (part 1)", "Hierarchical Clustering (part 2)", "Hierarchical Clustering (part 3)", "K-Means Clustering (part 1)", "K-Means Clustering (part 2)", "Dimension Reduction (part 1)", "Dimension Reduction (part 2)", "Dimension Reduction (part 3)", "Working with Color in R Plots (part 1)", "Working with Color in R Plots (part 2)", "Working with Color in R Plots (part 3)", "Working with Color in R Plots (part 4)", "Practical R Exercises in swirl Part 3", "swirl Lesson 1: Hierarchical Clustering", "swirl Lesson 2: K Means Clustering", "swirl Lesson 3: Dimension Reduction", "swirl Lesson 4: Clustering Example"]}, {"title": "Week 4", "description": "This week, we'll look at two case studies in exploratory data analysis. The first involves the use of cluster analysis techniques, and the second is a more involved analysis of some air pollution data. How one goes about doing EDA is often personal, but I'm providing these videos to give you a sense of how you might proceed with a specific type of dataset. ", "video": ["Clustering Case Study", "Air Pollution Case Study", "Practical R Exercises in swirl Part 4", "swirl Lesson 1: CaseStudy", "Post-Course Survey", "Course Project 2"]}]}, {"title": "Regression Models", "course_info": "About this course: Linear models, as their name implies, relates an outcome to a set of predictors of interest using linear assumptions.  Regression models, a subset of linear models, are the most important statistical analysis tool in a data scientist’s toolkit. This course covers regression analysis, least squares and inference using regression models. Special cases of the regression model, ANOVA and ANCOVA will be covered as well. Analysis of residuals and variability will be investigated. The course will cover modern thinking on model selection and novel uses of regression models including scatterplot smoothing.", "target_audience": null, "created_by": "Johns Hopkins University", "teach_by": [{"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}], "package_num": "7", "package_name": "Data Science Specialization ", "level": null, "rating": "4.4", "week_data": [{"title": "Week 1: Least Squares and Linear Regression", "description": "This week, we focus on least squares and linear regression.", "video": ["Welcome to Regression Models", "Book: Regression Models for Data Science in R", "Syllabus", "Pre-Course Survey", "Data Science Specialization Community Site", "Where to get more advanced material", "Regression", "Introduction to Regression", "Introduction: Basic Least Squares", "Technical details", "Technical Details (Skip if you'd like)", "Introductory Data Example", "Least squares", "Notation and Background", "Linear Least Squares", "Linear Least Squares Coding Example", "Technical Details (Skip if you'd like)", "Regression to the mean", "Regression to the Mean", "Practical R Exercises in swirl Part 1", "swirl Lesson 1: Introduction", "swirl Lesson 2: Residuals", "swirl Lesson 3: Least Squares Estimation", "Quiz 1"]}, {"title": "Week 2: Linear Regression & Multivariable Regression", "description": "This week, we will work through the remainder of linear regression and then turn to the first part of  multivariable regression.", "video": ["*Statistical* linear regression models", "Statistical Linear Regression Models", "Interpreting Coefficients", "Linear Regression for Prediction", "Residuals", "Residuals", "Residuals, Coding Example", "Residual Variance", "Inference in regression", "Inference in Regression", "Coding Example", "Prediction", "Looking ahead to the project", "Really, really quick intro to knitr", "Practical R Exercises in swirl Part 2", "swirl Lesson 1: Residual Variation", "swirl Lesson 2: Introduction to Multivariable Regression", "swirl Lesson 3: MultiVar Examples", "Quiz 2"]}, {"title": "Week 3: Multivariable Regression, Residuals, & Diagnostics", "description": "This week, we'll build on last week's introduction to multivariable regression with some examples and then cover residuals, diagnostics, variance inflation, and model comparison. ", "video": ["Multivariable regression", "Multivariable Regression part I", "Multivariable Regression part II", "Multivariable Regression Continued", "Multivariable Regression Examples part I", "Multivariable Regression Examples part II", "Multivariable Regression Examples part III", "Multivariable Regression Examples part IV", "Adjustment", "Adjustment Examples", "Residuals", "Residuals and Diagnostics part I", "Residuals and Diagnostics part II", "Residuals and Diagnostics part III", "Model selection", "Model Selection part I", "Model Selection part II", "Model Selection part III", "Practical R Exercises in swirl Part 3", "swirl Lesson 1: MultiVar Examples2", "swirl Lesson 2: MultiVar Examples3", "swirl Lesson 3: Residuals Diagnostics and Variation", "(OPTIONAL) Data analysis practice with immediate feedback (NEW! 10/18/2017)", "Quiz 3"]}, {"title": "Week 4: Logistic Regression and Poisson Regression", "description": "This week, we will work on generalized linear models, including binary outcomes and Poisson regression. ", "video": ["GLMs", "GLMs", "Logistic regression", "Logistic Regression part I", "Logistic Regression part II", "Logistic Regression part III", "Count Data", "Poisson Regression part I", "Poisson Regression part II", "Mishmash", "Hodgepodge", "Practical R Exercises in swirl Part 4", "swirl Lesson 1: Variance Inflation Factors", "swirl Lesson 2: Overfitting and Underfitting", "swirl Lesson 3: Binary Outcomes", "swirl Lesson 4: Count Outcomes", "Post-Course Survey", "Quiz 4", "Regression Models Course Project"]}]}, {"title": "Statistical Inference", "course_info": "About this course: Statistical inference is the process of drawing conclusions about populations or scientific truths from data. There are many modes of performing inference including statistical modeling, data oriented strategies and explicit use of designs and randomization in analyses. Furthermore, there are broad theories (frequentists, Bayesian, likelihood, design based, …) and numerous complexities (missing data, observed and unobserved confounding, biases) for performing inference. A practitioner can often be left in a debilitating maze of techniques, philosophies and nuance. This course presents the fundamentals of inference in a practical approach for getting things done. After taking this course, students will understand the broad directions of statistical inference and use this information for making informed choices in analyzing data.", "target_audience": null, "created_by": "Johns Hopkins University", "teach_by": [{"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}], "package_num": "6", "package_name": "Data Science Specialization ", "level": null, "rating": "4.1", "week_data": [{"title": "Week 1: Probability & Expected Values", "description": "This week, we'll focus on the fundamentals including probability, random variables, expectations and more. ", "video": ["Introductory video", "Welcome to Statistical Inference", "Some introductory comments", "Pre-Course Survey", "Syllabus", "Course Book: Statistical Inference for Data Science", "Data Science Specialization Community Site", "Homework Problems", "Probability", "02 01 Introduction to probability", "02 02 Probability mass functions", "02 03 Probability density functions", "Conditional probability", "03 01 Conditional Probability", "03 02 Bayes' rule", "03 03 Independence", "Expected values", "04 01 Expected values", "04 02 Expected values, simple examples", "04 03 Expected values for PDFs", "Practical R Exercises in swirl 1", "swirl Lesson 1: Introduction", "swirl Lesson 2: Probability1", "swirl Lesson 3: Probability2", "swirl Lesson 4: ConditionalProbability", "swirl Lesson 5: Expectations", "Quiz 1"]}, {"title": "Week 2: Variability, Distribution, & Asymptotics", "description": "We're going to tackle variability, distributions, limits, and confidence intervals.", "video": ["Variability", "05 01 Introduction to variability", "05 02 Variance simulation examples", "05 03 Standard error of the mean", "05 04 Variance data example", "Distributions", "06 01 Binomial distrubtion", "06 02 Normal distribution", "06 03 Poisson", "Asymptotics", "07 01 Asymptotics and LLN", "07 02 Asymptotics and the CLT", "07 03 Asymptotics and confidence intervals", "Practical R Exercises in swirl Part 2", "swirl Lesson 1: Variance", "swirl Lesson 2: CommonDistros", "swirl Lesson 3: Asymptotics", "Quiz 2"]}, {"title": "Week: Intervals, Testing, & Pvalues", "description": "We will be taking a look at intervals, testing, and pvalues in this lesson.", "video": ["Confidence intervals", "08 01 T confidence intervals", "08 02 T confidence intervals example", "08 03 Independent group T intervals", "08 04 A note on unequal variance", "Hypothesis testing", "09 01 Hypothesis testing", "09 02 Example of choosing a rejection region", "09 03 T tests", "09 04 Two group testing", "P-values", "10 01 Pvalues", "10 02 Pvalue further examples", "Knitr", "Just enough knitr to do the project", "Practical R Exercises in swirl Part 3", "swirl Lesson 1: T Confidence Intervals", "swirl Lesson 2: Hypothesis Testing", "swirl Lesson 3: P Values", "Quiz 3"]}, {"title": "Week 4: Power, Bootstrapping, & Permutation Tests", "description": "We will begin looking into power, bootstrapping, and permutation tests.", "video": ["Power", "11 01 Power", "11 02 Calculating Power", "11 03 Notes on power", "11 04 T test power", "12 01 Multiple Comparisons", "Resampling", "13 01 Bootstrapping", "13 02 Bootstrapping example", "13 03 Notes on the bootstrap", "13 04 Permutation tests", "Practical R Exercises in swirl Part 4", "swirl Lesson 1: Power", "swirl Lesson 2: Multiple Testing", "swirl Lesson 3: Resampling", "Post-Course Survey", "Quiz 4", "Statistical Inference Course Project"]}]}, {"title": "Machine Learning: Regression", "course_info": "About this course: Case Study - Predicting Housing Prices\n\nIn our first case study, predicting house prices, you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,...).  This is just one of the many places where regression can be applied.  Other applications range from predicting health outcomes in medicine, stock prices in finance, and power usage in high-performance computing, to analyzing which regulators are important for gene expression.\n\nIn this course, you will explore regularized linear regression models for the task of prediction and feature selection.  You will be able to handle very large sets of features and select between models of various complexity.  You will also analyze the impact of aspects of your data -- such as outliers -- on your selected models and predictions.  To fit these models, you will implement optimization algorithms that scale to large datasets.\n\nLearning Outcomes:  By the end of this course, you will be able to:\n   -Describe the input and output of a regression model.\n   -Compare and contrast bias and variance when modeling data.\n   -Estimate model parameters using optimization algorithms.\n   -Tune parameters with cross validation.\n   -Analyze the performance of the model.\n   -Describe the notion of sparsity and how LASSO leads to sparse solutions.\n   -Deploy methods to select between models.\n   -Exploit the model to form predictions. \n   -Build a regression model to predict prices using a housing dataset.\n   -Implement these techniques in Python.", "target_audience": null, "created_by": "University of Washington", "teach_by": [{"name": "Emily Fox", "department": "Statistics"}, {"name": "Carlos Guestrin", "department": "Computer Science and Engineering"}], "package_num": "2", "package_name": "Machine Learning Specialization ", "level": null, "rating": "4.8", "week_data": [{"title": "Welcome", "description": "Regression is one of the most important and broadly used machine learning and statistics tools out there.  It allows you to make predictions from data by learning the relationship between features of your data and some observed, continuous-valued response.  Regression is used in a massive number of applications ranging from predicting stock prices to understanding gene regulatory networks.<p>This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have.", "video": ["Slides presented in this module", "Welcome!", "What is the course about?", "Outlining the first half of the course", "Outlining the second half of the course", "Assumed background", "Reading: Software tools you'll need"]}, {"title": "Simple Linear Regression", "description": "Our course starts from the most basic regression model: Just fitting a line to data.  This simple model for forming predictions from a single, univariate feature of the data is appropriately called \"simple linear regression\".<p> In this module, we describe the high-level regression task and then specialize these concepts to the simple linear regression case. You will learn how to formulate a simple regression model and fit the model to data using both a closed-form solution as well as an iterative optimization algorithm called gradient descent.  Based on this fitted function, you will interpret the estimated model parameters and form predictions.  You will also analyze the sensitivity of your fit to outlying observations.<p> You will examine all of these concepts in the context of a case study of predicting house prices from the square feet of the house.", "video": ["Slides presented in this module", "A case study in predicting house prices", "Regression fundamentals: data & model", "Regression fundamentals: the task", "Regression ML block diagram", "The simple linear regression model", "The cost of using a given line", "Using the fitted line", "Interpreting the fitted line", "Defining our least squares optimization objective", "Finding maxima or minima analytically", "Maximizing a 1d function: a worked example", "Finding the max via hill climbing", "Finding the min via hill descent", "Choosing stepsize and convergence criteria", "Gradients: derivatives in multiple dimensions", "Gradient descent: multidimensional hill descent", "Computing the gradient of RSS", "Approach 1: closed-form solution", "Optional reading: worked-out example for closed-form solution", "Approach 2: gradient descent", "Optional reading: worked-out example for gradient descent", "Comparing the approaches", "Download notebooks to follow along", "Influence of high leverage points: exploring the data", "Influence of high leverage points: removing Center City", "Influence of high leverage points: removing high-end towns", "Asymmetric cost functions", "A brief recap", "Reading: Fitting a simple linear regression model on housing data", "Simple Linear Regression", "Fitting a simple linear regression model on housing data"]}, {"title": "Multiple Regression", "description": "The next step in moving beyond simple linear regression is to consider \"multiple regression\" where multiple features of the data are used to form predictions.  <p> More specifically, in this module, you will learn how to build models of more complex relationship between a single variable (e.g., 'square feet') and the observed response (like 'house sales price').  This includes things like fitting a polynomial to your data, or capturing seasonal changes in the response value.  You will also learn how to incorporate multiple input variables (e.g., 'square feet', '# bedrooms', '# bathrooms').  You will then be able to describe how all of these models can still be cast within the linear regression framework, but now using multiple \"features\".   Within this multiple regression framework, you will fit models to data, interpret estimated coefficients, and form predictions. <p>Here, you will also implement a gradient descent algorithm for fitting a multiple regression model.", "video": ["Slides presented in this module", "Multiple regression intro", "Polynomial regression", "Modeling seasonality", "Where we see seasonality", "Regression with general features of 1 input", "Motivating the use of multiple inputs", "Defining notation", "Regression with features of multiple inputs", "Interpreting the multiple regression fit", "Optional reading: review of matrix algebra", "Rewriting the single observation model in vector notation", "Rewriting the model for all observations in matrix notation", "Computing the cost of a D-dimensional curve", "Computing the gradient of RSS", "Approach 1: closed-form solution", "Discussing the closed-form solution", "Approach 2: gradient descent", "Feature-by-feature update", "Algorithmic summary of gradient descent approach", "A brief recap", "Reading: Exploring different multiple regression models for house price prediction", "Numpy tutorial", "Reading: Implementing gradient descent for multiple regression", "Multiple Regression", "Exploring different multiple regression models for house price prediction", "Implementing gradient descent for multiple regression"]}, {"title": "Assessing Performance", "description": "Having learned about linear regression models and algorithms for estimating the parameters of such models, you are now ready to assess how well your considered method should perform in predicting new data.  You are also ready to select amongst possible models to choose the best performing.  <p> This module is all about these important topics of model selection and assessment.  You will examine both theoretical and practical aspects of such analyses. You will first explore the concept of measuring the \"loss\" of your predictions, and use this to define training, test, and generalization error.  For these measures of error, you will analyze how they vary with model complexity and how they might be utilized to form a valid assessment of predictive performance.  This leads directly to an important conversation about the bias-variance tradeoff, which is fundamental to machine learning.  Finally, you will devise a method to first select amongst models and then assess the performance of the selected model. <p>The concepts described in this module are key to all machine learning problems, well-beyond the regression setting addressed in this course.", "video": ["Slides presented in this module", "Assessing performance intro", "What do we mean by \"loss\"?", "Training error: assessing loss on the training set", "Generalization error: what we really want", "Test error: what we can actually compute", "Defining overfitting", "Training/test split", "Irreducible error and bias", "Variance and the bias-variance tradeoff", "Error vs. amount of data", "Formally defining the 3 sources of error", "Formally deriving why 3 sources of error", "Training/validation/test split for model selection, fitting, and assessment", "A brief recap", "Reading: Exploring the bias-variance tradeoff", "Assessing Performance", "Exploring the bias-variance tradeoff"]}, {"title": "Ridge Regression", "description": "You have examined how the performance of a model varies with increasing model complexity, and can describe the potential pitfall of complex models becoming overfit to the training data.   In this module, you will explore a very simple, but extremely effective technique for automatically coping with this issue.  This method is called \"ridge regression\".  You start out with a complex model, but now fit the model in a manner that not only incorporates a measure of fit to the training data, but also a term that biases the solution away from overfitted functions.  To this end, you will explore symptoms of overfitted functions and use this to define a quantitative measure to use in your revised optimization objective.  You will derive both a closed-form and gradient descent algorithm for fitting the ridge regression objective; these forms are small modifications from the original algorithms you derived for multiple regression.  To select the strength of the bias away from overfitting, you will explore a general-purpose method called \"cross validation\". <p>You will implement both cross-validation and gradient descent to fit a ridge regression model and select the regularization constant.", "video": ["Slides presented in this module", "Symptoms of overfitting in polynomial regression", "Download the notebook and follow along", "Overfitting demo", "Overfitting for more general multiple regression models", "Balancing fit and magnitude of coefficients", "The resulting ridge objective and its extreme solutions", "How ridge regression balances bias and variance", "Download the notebook and follow along", "Ridge regression demo", "The ridge coefficient path", "Computing the gradient of the ridge objective", "Approach 1: closed-form solution", "Discussing the closed-form solution", "Approach 2: gradient descent", "Selecting tuning parameters via cross validation", "K-fold cross validation", "How to handle the intercept", "A brief recap", "Reading: Observing effects of L2 penalty in polynomial regression", "Reading: Implementing ridge regression via gradient descent", "Ridge Regression", "Observing effects of L2 penalty in polynomial regression", "Implementing ridge regression via gradient descent"]}, {"title": "Feature Selection & Lasso", "description": "A fundamental machine learning task is to select amongst a set of features to include in a model.  In this module, you will explore this idea in the context of multiple regression, and describe how such feature selection is important for both interpretability and efficiency of forming predictions. <p> To start, you will examine methods that search over an enumeration of models including different subsets of features.  You will analyze both exhaustive search and greedy algorithms.  Then, instead of an explicit enumeration, we turn to Lasso regression, which implicitly performs feature selection in a manner akin to ridge regression: A complex model is fit based on a measure of fit to the training data plus a measure of overfitting different than that used in ridge.  This lasso method has had impact in numerous applied domains, and the ideas behind the method have fundamentally changed machine learning and statistics. You will also implement a coordinate descent algorithm for fitting a Lasso model. <p>Coordinate descent is another, general, optimization technique, which is useful in many areas of machine learning. ", "video": ["Slides presented in this module", "The feature selection task", "All subsets", "Complexity of all subsets", "Greedy algorithms", "Complexity of the greedy forward stepwise algorithm", "Can we use regularization for feature selection?", "Thresholding ridge coefficients?", "The lasso objective and its coefficient path", "Visualizing the ridge cost", "Visualizing the ridge solution", "Visualizing the lasso cost and solution", "Download the notebook and follow along", "Lasso demo", "What makes the lasso objective different", "Coordinate descent", "Normalizing features", "Coordinate descent for least squares regression (normalized features)", "Coordinate descent for lasso (normalized features)", "Assessing convergence and other lasso solvers", "Coordinate descent for lasso (unnormalized features)", "Deriving the lasso coordinate descent update", "Choosing the penalty strength and other practical issues with lasso", "A brief recap", "Reading: Using LASSO to select features", "Reading: Implementing LASSO using coordinate descent", "Feature Selection and Lasso", "Using LASSO to select features", "Implementing LASSO using coordinate descent"]}, {"title": "Nearest Neighbors & Kernel Regression", "description": "Up to this point, we have focused on methods that fit parametric functions---like polynomials and hyperplanes---to the entire dataset.  In this module, we instead turn our attention to a class of \"nonparametric\" methods.  These methods allow the complexity of the model to increase as more data are observed, and result in fits that adapt locally to the observations.  <p> We start by considering the simple and intuitive example of nonparametric methods, nearest neighbor regression: The prediction for a query point is based on the outputs of the most related observations in the training set.  This approach is extremely simple, but can provide excellent predictions, especially for large datasets. You will deploy algorithms to search for the nearest neighbors and form predictions based on the discovered neighbors.  Building on this idea, we turn to kernel regression.  Instead of forming predictions based on a small set of neighboring observations, kernel regression uses all observations in the dataset, but the impact of these observations on the predicted value is weighted by their similarity to the query point.  You will analyze the theoretical performance of these methods in the limit of infinite training data, and explore the scenarios in which these methods work well versus struggle.  You will also implement these techniques and observe their practical behavior.", "video": ["Slides presented in this module", "Limitations of parametric regression", "1-Nearest neighbor regression approach", "Distance metrics", "1-Nearest neighbor algorithm", "k-Nearest neighbors regression", "k-Nearest neighbors in practice", "Weighted k-nearest neighbors", "From weighted k-NN to kernel regression", "Global fits of parametric models vs. local fits of kernel regression", "Performance of NN as amount of data grows", "Issues with high-dimensions, data scarcity, and computational complexity", "k-NN for classification", "A brief recap", "Reading: Predicting house prices using k-nearest neighbors regression", "Nearest Neighbors & Kernel Regression", "Predicting house prices using k-nearest neighbors regression"]}, {"title": "Closing Remarks", "description": "In the conclusion of the course, we will recap what we have covered.  This represents both techniques specific to regression, as well as foundational machine learning concepts that will appear throughout the specialization.  We also briefly discuss some important regression techniques we did not cover in this course.<p> We conclude with an overview of what's in store for you in the rest of the specialization.  ", "video": ["Slides presented in this module", "Simple and multiple regression", "Assessing performance and ridge regression", "Feature selection, lasso, and nearest neighbor regression", "What we covered and what we didn't cover", "Thank you!"]}]}, {"title": "Econometrics: Methods and Applications", "course_info": "About this course: Welcome!\nDo you wish to know how to analyze and solve business and economic questions with data analysis tools? Then Econometrics by Erasmus University Rotterdam is the right course for you, as you learn how to translate data into models to make forecasts and to support decision making.\n\n* What do I learn?\nWhen you know econometrics, you are able to translate data into models to make forecasts and to support decision making in a wide variety of fields, ranging from macroeconomics to finance and marketing. Our course starts with introductory lectures on simple and multiple regression, followed by topics of special interest to deal with model specification, endogenous variables, binary choice data, and time series data.  You learn these key topics in econometrics by watching the videos with in-video quizzes and by making post-video training exercises. \n\n* Do I need prior knowledge?\nThe course is suitable for (advanced undergraduate) students in economics, finance, business, engineering, and data analysis, as well as for those who work in these fields. The course requires some basics of matrices, probability, and statistics, which are reviewed in the Building Blocks module. \n\n* What literature can I consult to support my studies?\nYou can follow the MOOC without studying additional sources. Further reading of the discussed topics (including the Building Blocks) is provided in the textbook that we wrote and on which the MOOC is based: Econometric Methods with Applications in Business and Economics, Oxford University Press. The connection between the MOOC modules and the book chapters is shown in the Course Guide – Further Information – How can I continue my studies.\n\n* Will there be teaching assistants active to guide me through the course?\nStaff and PhD students of our Econometric Institute will provide guidance in January and February of each year. In other periods, we provide only elementary guidance. We always advise you to connect with fellow learners of this course to discuss topics and exercises.\n\n* How will I get a certificate?\nTo gain the certificate of this course, you are asked to make six Test Exercises (one per module) and a Case Project. Further, you perform peer-reviewing activities of the work of three of your fellow learners of this MOOC. You gain the certificate if you pass all seven assignments.\n\nHave a nice journey into the world of Econometrics!\nThe Econometrics team", "target_audience": null, "created_by": "Erasmus University Rotterdam", "teach_by": [{"name": "Philip Hans Franses", "department": "Econometric Institute, Erasmus School of Economics"}, {"name": "Christiaan Heij", "department": "Econometric Institute, Erasmus School of Economics"}, {"name": "Michel van der Wel", "department": "Econometric Institute, Erasmus School of Economics"}, {"name": "Dennis Fok", "department": "Econometric Institute, Erasmus School of Economics"}, {"name": "Richard Paap", "department": "Econometric Institute, Erasmus School of Economics"}, {"name": "Dick van Dijk ", "department": "Econometric Institute, Erasmus School of Economics"}, {"name": "Erik Kole", "department": "Econometric Institute, Erasmus School of Economics"}, {"name": "Francine Gresnigt", "department": "Econometric Institute, Erasmus School of Economics"}, {"name": "Myrthe van Dieijen", "department": "Econometric Institute, Erasmus School of Economics"}], "package_num": null, "package_name": null, "level": null, "rating": "4.5", "week_data": [{"title": "Welcome Module", "description": "", "video": ["Welcome to our MOOC on Econometrics", "About this course", "Course Guide - Structure of the MOOC", "Course Guide - Further information"]}, {"title": "Simple Regression", "description": "", "video": ["Dataset Simple Regression", "Lecture 1.1 on Simple Regression: Motivation", "Training Exercise 1.1", "Solution Training Exercise 1.1", "Lecture 1.2 on Simple Regression: Representation", "Training Exercise 1.2", "Solution Training Exercise 1.2", "Lecture 1.3 on Simple Regression: Estimation", "Training Exercise 1.3", "Solution Training Exercise 1.3", "Lecture 1.4 on Simple Regression: Evaluation", "Training Exercise 1.4", "Solution Training Exercise 1.4", "Lecture 1.5 on Simple Regression: Application", "Training Exercise 1.5", "Solution Training Exercise 1.5", "Test Exercise 1"]}, {"title": "Multiple Regression", "description": "", "video": ["Dataset Multiple Regression", "Lecture 2.1 on Multiple Regression: Motivation", "Training Exercise 2.1", "Solution Training Exercise 2.1", "Lecture 2.2 on Multiple Regression: Representation", "Training Exercise 2.2", "Solution Training Exercise 2.2", "Lecture 2.3 on Multiple Regression: Estimation", "Training Exercise 2.3", "Solution Training Exercise 2.3", "Lecture 2.4.1 on Multiple Regression: Evaluation - Statistical Properties", "Training Exercise 2.4.1", "Solution Training Exercise 2.4.1", "Lecture 2.4.2 on Multiple Regression: Evaluation - Statistical Tests", "Training Exercise 2.4.2", "Solution Training Exercise 2.4.2", "Lecture 2.5 on Multiple Regression: Application", "Training Exercise 2.5", "Solution Training Exercise 2.5", "Test Exercise 2"]}, {"title": "Model Specification", "description": "", "video": ["Dataset Model Specification", "Lecture 3.1 on Model Specification: Motivation", "Training Exercise 3.1", "Solution Training Exercise 3.1", "Lecture 3.2 on Model Specification: Specification", "Training Exercise 3.2", "Solution Training Exercise 3.2", "Lecture 3.3 on Model Specification: Transformation", "Training Exercise 3.3", "Solution Training Exercise 3.3", "Lecture 3.4 on Model Specification: Evaluation", "Training Exercise 3.4", "Solution Training Exercise 3.4", "Lecture 3.5 on Model Specification: Application", "Training Exercise 3.5", "Solution Training Exercise 3.5", "Test Exercise 3"]}, {"title": "Endogeneity", "description": "", "video": ["Dataset Endogeneity", "Lecture 4.1 on Endogeneity: Motivation", "Training Exercise 4.1", "Solution Training Exercise 4.1", "Lecture 4.2 on Endogeneity: Consequences", "Training Exercise 4.2", "Solution Training Exercise 4.2", "Lecture 4.3 on Endogeneity: Estimation", "Training Exercise 4.3", "Solution Training Exercise 4.3", "Lecture 4.4 on Endogeneity: Testing", "Training Exercise 4.4", "Solution Training Exercise 4.4", "Lecture 4.5 on Endogeneity: Application", "Training Exercise 4.5", "Solution Training Exercise 4.5", "Test Exercise 4"]}, {"title": "Binary Choice", "description": "", "video": ["Dataset Binary Choice", "Lecture 5.1 on Binary Choice: Motivation", "Training Exercise 5.1", "Solution Training Exercise 5.1", "Lecture 5.2 on Binary Choice: Representation", "Training Exercise 5.2", "Solution Training Exercise 5.2", "Lecture 5.3 on Binary Choice: Estimation", "Training Exercise 5.3", "Solution Training Exercise 5.3", "Lecture 5.4 on Binary Choice: Evaluation", "Training Exercise 5.4", "Solution Training Exercise 5.4", "Dataset for Lecture 5.5 on Binary Choice: Application", "Lecture 5.5 on Binary Choice: Application", "Training Exercise 5.5", "Solution Training Exercise 5.5", "Test Exercise 5"]}, {"title": "Time Series", "description": "", "video": ["Dataset Time Series", "Lecture 6.1 on Time Series: Motivation", "Training Exercise 6.1", "Solution Training Exercise 6.1", "Lecture 6.2 on Time Series: Representation", "Training Exercise 6.2", "Solution Training Exercise 6.2", "Lecture 6.3 on Time Series: Specification and Estimation", "Training Exercise 6.3", "Solution Training Exercise 6.3", "Lecture 6.4 on Time Series: Evaluation and Illustration", "Training Exercise 6.4", "Solution Training Exercise 6.4", "Lecture 6.5 on Time Series: Application", "Training Exercise 6.5", "Solution Training Exercise 6.5", "Test Exercise 6"]}, {"title": "Case Project", "description": "", "video": ["Case Project"]}, {"title": "OPTIONAL: Building Blocks", "description": "By studying this module, you get the required background on matrices, probability and statistics. Each topic is illustrated with simple examples, and you get hands-on training by doing the training exercise that concludes each lecture. Three lectures on matrices show you the basic terminology and properties of matrices, including transpose, trace, rank, inverse, and positive definiteness. Two lectures on probability teach you the basics of univariate and multivariate probability distributions, especially the normal and associated distributions, including mean, variance, and covariance. Finally, two lectures on statistics present you with the basic ideas of statistical inference, in particular parameter estimation and testing, including the use of matrix methods and probability methods. ", "video": ["Structure", "Lecture M.1: Introduction to Vectors and Matrices", "Training Exercise M.1", "Solution Training Exercise M.1", "Lecture M.2: Special Matrix Operations", "Training Exercise M.2", "Solution Training Exercise M.2", "Lecture M.3: Vectors and Differentiation", "Training Exercise M.3", "Solution Training Exercise M.3", "Lecture P.1: Random Variables", "Training Exercise P.1", "Solution Training Exercise P.1", "Lecture P.2: Probability Distributions", "Training Exercise P.2", "Solution Training Exercise P.2", "Dataset for Lecture S.1 on Parameter Estimation", "Lecture S.1: Parameter Estimation", "Training Exercise S.1", "Solution Training Exercise S.1", "Lecture S.2: Statistical Testing", "Training Exercise S.2", "Solution Training Exercise S.2"]}]}, {"title": "Inferential Statistics", "course_info": "About this course: This course covers commonly used statistical inference methods for numerical and categorical data. You will learn how to set up and perform hypothesis tests, interpret p-values, and report the results of your analysis in a way that is interpretable for clients or the public. Using numerous data examples, you will learn to report estimates of quantities in a way that expresses the uncertainty of the quantity of interest. You will be guided through installing and using R and RStudio (free statistical software), and will use this software for lab exercises and a final project. The course introduces practical tools for performing data analysis and explores the fundamental concepts necessary to interpret and report results for both categorical and numerical data", "target_audience": null, "created_by": "Duke University", "teach_by": [{"name": "Mine Çetinkaya-Rundel", "department": "Department of Statistical Science"}], "package_num": "2", "package_name": "Statistics with R Specialization ", "level": "Beginner", "rating": "4.8", "week_data": [{"title": "About the Specialization and the Course", "description": "This short module introduces basics about Coursera specializations and courses in general, this specialization: Statistics with R, and this course: Inferential Statistics. Please take several minutes to browse them through. Thanks for joining us in this course!", "video": ["About Statistics with R Specialization", "More about Inferential Statistics"]}, {"title": "Central Limit Theorem and Confidence Interval", "description": "Welcome to Inferential Statistics! In this course we will discuss Foundations for Inference. Check out the learning objectives, start watching the videos, and finally work on the quiz and the labs of this week. In addition to videos that introduce new concepts, you will also see a few videos that walk you through application examples related to the week's topics. In the first week we will introduce Central Limit Theorem (CLT) and confidence interval.", "video": ["Lesson Learning Objectives", "Introduction", "Sampling Variability and CLT", "CLT (for the mean) examples", "Lesson Learning Objectives", "Confidence Interval (for a mean)", "Accuracy vs. Precision", "Required Sample Size for ME", "CI (for the mean) examples", "Week 1 Suggested Readings and Practice Exercises", "Week 1 Practice Quiz", "Week 1 Lab Instructions", "Week 1 Quiz", "Week 1 Lab"]}, {"title": "Inference and Significance", "description": "Welcome to Week Two! This week we will discuss formal hypothesis testing and relate testing procedures back to estimation via confidence intervals. These topics will be introduced within the context of working with a population mean, however we will also give you a brief peek at what's to come in the next two weeks by discussing how the methods we're learning can be extended to other estimators. We will also discuss crucial considerations like decision errors and statistical vs. practical significance. The labs for this week will illustrate concepts of sampling distributions and confidence levels.", "video": ["Lesson Learning Objectives", "Another Introduction to Inference", "Hypothesis Testing (for a mean)", "HT (for the mean) examples", "Lesson Learning Objectives", "Inference for Other Estimators", "Decision Errors", "Significance vs. Confidence Level", "Statistical vs. Practical Significance", "Week 2 Suggested Readings and Practice Exercises", "Week 2 Practice Quiz", "Week 2 Lab Instructions", "Week 2 Quiz", "Week 2 Lab"]}, {"title": "Inference for Comparing Means", "description": "Welcome to Week Three of the course! This week we will introduce the t-distribution and comparing means as well as a simulation based method for creating a confidence interval: bootstrapping. If you have questions or discussions, please use this week's forum to ask/discuss with peers.", "video": ["Lesson Learning Objectives", "Introduction", "t-distribution", "Inference for a mean", "Inference for comparing two independent means", "Inference for comparing two paired means", "Power", "Lesson Learning Objectives", "Comparing more than two means", "ANOVA", "Conditions for ANOVA", "Multiple comparisons", "Bootstrapping", "Week 3 Suggested Readings and Practice Exercises", "Week 3 Practice Quiz", "Week 3 Lab Instructions", "Week 3 Quiz", "Week 3 Lab"]}, {"title": "Inference for Proportions", "description": "Welcome to Week Four of our course! In this unit, we’ll discuss inference for categorical data. We use methods introduced this week to answer questions like “What proportion of the American public approves of the job of the Supreme Court is doing?”.", "video": ["Lesson Learning Objectives", "Introduction", "Sampling Variability and CLT for Proportions", "Confidence Interval for a Proportion", "Hypothesis Test for a Proportion", "Estimating the Difference Between Two Proportions", "Hypothesis Test for Comparing Two Proportions", "Lesson Learning Objectives", "Small Sample Proportions", "Examples", "Comparing Two Small Sample Proportions", "Chi-Square GOF Test", "The Chi-Square Independence Test", "Week 4 Suggested Readings and Practice Exercises", "Week 4 Practice Quiz", "Week 4 Lab Instructions", "Week 4 Quiz", "Week 4 Lab"]}, {"title": " Data Analysis Project", "description": "In this week you will use the data set provided to complete and report on a data analysis question. Please read the background information, review the report template (downloaded from the link in Lesson Project Information), and then complete the peer review assignment. ", "video": ["Project Information", "Data Analysis Project"]}]}, {"title": "Bayesian Statistics: Techniques and Models", "course_info": "About this course: This is the second of a two-course sequence introducing the fundamentals of Bayesian statistics. It builds on the course Bayesian Statistics: From Concept to Data Analysis, which introduces Bayesian methods through use of simple conjugate models. Real-world data often require more sophisticated models to reach realistic conclusions. This course aims to expand our “Bayesian toolbox” with more general models, and computational techniques to fit them. In particular, we will introduce Markov chain Monte Carlo (MCMC) methods, which allow sampling from posterior distributions that have no analytical solution. We will use the open-source, freely available software R (some experience is assumed, e.g., completing the previous course in R) and JAGS (no experience required). We will learn how to construct, fit, assess, and compare Bayesian statistical models to answer scientific questions involving continuous, binary, and count data. This course combines lecture videos, computer demonstrations, readings, exercises, and discussion boards to create an active learning experience. The lectures provide some of the basic mathematical development, explanations of the statistical modeling process, and a few basic modeling techniques commonly used by statisticians. Computer demonstrations provide concrete, practical walkthroughs. Completion of this course will give you access to a wide range of Bayesian analytical tools, customizable to your data.", "target_audience": null, "created_by": "University of California, Santa Cruz", "teach_by": [{"name": "Matthew Heiner", "department": "Applied Mathematics and Statistics"}], "package_num": null, "package_name": null, "level": "Intermediate", "rating": "4.8", "week_data": [{"title": "Statistical modeling and Monte Carlo estimation", "description": "Statistical modeling, Bayesian modeling, Monte Carlo estimation", "video": ["Course introduction", "Module 1 assignments and materials", "Objectives", "Modeling process", "Statistical modeling process", "Components of Bayesian models", "Model specification", "Posterior derivation", "Non-conjugate models", "Reference: Common probability distributions", "Monte Carlo integration", "Monte Carlo error and marginalization", "Computing examples", "Computing Monte Carlo error", "Code for Lesson 3", "Markov chains", "Lesson 1", "Lesson 2", "Lesson 3", "Markov chains"]}, {"title": "Markov chain Monte Carlo (MCMC)", "description": "Metropolis-Hastings, Gibbs sampling, assessing convergence", "video": ["Module 2 assignments and materials", "Algorithm", "Demonstration", "Random walk example, Part 1", "Random walk example, Part 2", "Code for Lesson 4", "Download, install, setup", "Model writing, running, and post-processing", "Alternative MCMC software", "Code from JAGS introduction", "Multiple parameter sampling and full conditional distributions", "Conditionally conjugate prior example with Normal likelihood", "Computing example with Normal likelihood", "Code for Lesson 5", "Trace plots, autocorrelation", "Autocorrelation", "Multiple chains, burn-in, Gelman-Rubin diagnostic", "Code for Lesson 6", "Lesson 4", "Lesson 5", "Lesson 6", "MCMC"]}, {"title": "Common statistical models", "description": "Linear regression, ANOVA, logistic regression, multiple factor ANOVA", "video": ["Module 3 assignments and materials", "Introduction to linear regression", "Setup in R", "JAGS model (linear regression)", "Model checking", "Alternative models", "Deviance information criterion (DIC)", "Code for Lesson 7", "Introduction to ANOVA", "One way model using JAGS", "Code for Lesson 8", "Introduction to logistic regression", "JAGS model (logistic regression)", "Prediction", "Why linear models?", "Code for Lesson 9", "Multiple factor ANOVA", "Lesson 7 Part A", "Lesson 7 Part B", "Lesson 8", "Lesson 9", "Common models and multiple factor ANOVA"]}, {"title": "Count data and hierarchical modeling", "description": "Poisson regression, hierarchical modeling", "video": ["Module 4 assignments and materials", "Introduction to Poisson regression", "JAGS model (Poisson regression)", "Predictive distributions", "Prior sensitivity analysis", "Code for Lesson 10", "Correlated data", "Normal hierarchical model", "Prior predictive simulation", "JAGS model and model checking (hierarchical modeling)", "Posterior predictive simulation", "Linear regression example", "Linear regression example in JAGS", "Applications of hierarchical modeling", "Selecting prior distributions", "Code and data for Lesson 11", "Mixture model introduction, data, and code", "Mixture model in JAGS", "Lesson 10", "Lesson 11 Part A", "Lesson 11 Part B", "Predictive distributions and mixture models"]}, {"title": "Capstone project", "description": "Peer-reviewed data analysis project", "video": ["Course conclusion", "Further reading and acknowledgements", "Data Analysis Project"]}]}, {"title": "Data Visualization", "course_info": "About this course: Learn the general concepts of data mining along with basic methodologies and applications. Then dive into one subfield in data mining: pattern discovery. Learn in-depth concepts, methods, and applications of pattern discovery in data mining. We will also introduce methods for pattern-based classification and some interesting applications of pattern discovery. This course provides you the opportunity to learn skills and content to practice and engage in scalable pattern discovery methods on massive transactional data, discuss pattern evaluation measures, and study methods for mining diverse kinds of patterns, sequential patterns, and sub-graph patterns.", "target_audience": null, "created_by": "University of Illinois at Urbana-Champaign", "teach_by": [{"name": "John C. Hart", "department": "Department of Computer Science"}], "package_num": "1", "package_name": "Data Mining  Specialization ", "level": null, "rating": "4.4", "week_data": [{"title": "Course Orientation", "description": "You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course.", "video": ["Welcome to Data Visualization!", "Syllabus", "About the Discussion Forums", "Updating Your Profile", "Social Media", "Resources", "Orientation Quiz"]}, {"title": "Week 1: The Computer and the Human", "description": "In this week's module, you will learn what data visualization is, how it's used, and how computers display information. You'll also explore different types of visualization and how humans perceive information.", "video": ["Week 1 Overview", "Week 1 Introduction", "How the Programming Assignments Work", "1.1.1. Some Books on Data Visualization", "1.1.2. Overview of Visualization", "1.2.1. 2-D Graphics", "SVG-example", "1.2.2. 2-D Drawing", "1.2.3. 3-D Graphics", "1.2.4. Photorealism", "1.2.5. Non-Photorealism", "1.3.1. The Human", "1.3.2. Memory", "1.3.3. Reasoning", "1.3.4. The Human Retina", "1.3.5. Perceiving Two Dimensions", "1.3.6. Perceiving Perspective", "Week 1 Discussion ", "Week 1 Quiz"]}, {"title": "Week 2: Visualization of Numerical Data", "description": "In this week's module, you will start to think about how to visualize data effectively. This will include assigning data to appropriate chart elements, using glyphs, parallel coordinates, and streamgraphs, as well as implementing principles of design and color to make your visualizations more engaging and effective.", "video": ["Week 2 Overview", "Week 2 Introduction", "2.1.1. Data", "2.1.2. Mapping", "2.1.3. Charts", "2.2.1. Glyphs (Part 1)", "2.2.1. Glyphs (Part 2)", "2.2.2. Parallel Coordinates", "2.2.3. Stacked Graphs (Part 1)", "2.2.3. Stacked Graphs (Part 2)", "2.3.1. Tufte's Design Rules", "2.3.2. Using Color", "Programming Assignment 1: Visualize Data Using a Chart", "Programming Assignment 1 Rubric", "Programming Assignment 1 Help Forum", "Programming Assignment 1"]}, {"title": "Week 3: Visualization of Non-Numerical Data", "description": "In this week's module, you will learn how to visualize graphs that depict relationships between data items. You'll also plot data using coordinates that are not specifically provided by the data set.", "video": ["Week 3 Overview", "Week 3 Introduction", "3.1.1. Graphs and Networks", "3.1.2. Embedding Planar Graphs", "3.1.3. Graph Visualization", "3.1.4. Tree Maps", "3.2.1. Principal Component Analysis", "3.2.2. Multidimensional Scaling", "3.3.1. Packing", "Programming Assignment 2: Visualize Network Data", "Programming Assignment 2 Rubric", "Programming Assignment 2 Help Forum", "Programming Assignment 2"]}, {"title": "Week 4: The Visualization Dashboard", "description": "In this week's module, you will start to put together everything you've learned by designing your own visualization system for large datasets and dashboards. You'll create and interpret the visualization you created from your data set, and you'll also apply techniques from user-interface design to create an effective visualization system.", "video": ["Week 4 Overview", "Week 4 Introduction", "4.1.1. Visualization Systems", "4.1.2. The Information Visualization Mantra: Part 1", "4.1.2. The Information Visualization Mantra: Part 2", "4.1.2. The Information Visualization Mantra: Part 3", "4.1.3. Database Visualization Part: 1", "4.1.3. Database Visualization Part: 2", "4.1.3. Database Visualization Part: 3", "4.2.1. Visualization System Design", "Week 4 Quiz"]}]}, {"title": "Bayesian Statistics", "course_info": "About this course: This course describes Bayesian statistics, in which one's inferences about parameters or hypotheses are updated as evidence accumulates. You will learn to use Bayes’ rule to transform prior probabilities into posterior probabilities, and be introduced to the underlying theory and perspective of the Bayesian paradigm. The course will apply Bayesian methods to several practical problems, to show end-to-end Bayesian analyses that move from framing the question to building models to eliciting prior probabilities to implementing in R (free statistical software) the final posterior distribution. Additionally, the course will introduce credible regions, Bayesian comparisons of means and proportions, Bayesian regression and inference using multiple models, and discussion of Bayesian prediction.\n\nWe assume learners in this course have background knowledge equivalent to what is covered in the earlier three courses in this specialization: \"Introduction to Probability and Data,\" \"Inferential Statistics,\" and \"Linear Regression and Modeling.\"", "target_audience": null, "created_by": "Duke University", "teach_by": [{"name": "Mine Çetinkaya-Rundel", "department": "Department of Statistical Science"}, {"name": "David Banks", "department": "Statistical Science"}, {"name": "Colin Rundel ", "department": "Statistical Science"}, {"name": "Merlise A Clyde", "department": "Department of Statistical Science"}], "package_num": "4", "package_name": "Statistics with R Specialization ", "level": "Intermediate", "rating": "3.8", "week_data": [{"title": "About the Specialization and the Course", "description": "This short module introduces basics about Coursera specializations and courses in general, this specialization: Statistics with R, and this course: Bayesian Statistics. Please take several minutes read this information. Thanks for joining us in this course!", "video": ["Introduction to Statistics with R", "About Statistics with R Specialization", "About Bayesian Statistics", "Special Thanks", "Weekly Feedback Forms", "Introduce Yourself"]}, {"title": "The Basics of Bayesian Statistics", "description": "<p>Welcome! Over the next several weeks, we will together explore Bayesian statistics. <p>In this module, we will work with conditional probabilities, which is the probability of event B given event A. Conditional probabilities are very important in medical decisions. By the end of the week, you will be able to solve problems using Bayes' rule, and update prior probabilities.</p><p>Please use the learning objectives and practice quiz to help you learn about Bayes' Rule, and apply what you have learned in the lab and on the quiz. ", "video": ["The Basics of Bayesian Statistics", "Module Learning Objectives", "Conditional Probabilities and Bayes' Rule", "Bayes' Rule and Diagnostic Testing", "Bayes Updating", "Bayesian vs. frequentist definitions of probability", "Inference for a Proportion: Frequentist Approach", "Inference for a Proportion: Bayesian Approach", "Effect of Sample Size on the Posterior", "Frequentist vs. Bayesian Inference", "Week 1 Practice Quiz", "Week 1 Lab Instructions", "Week 1 Feedback Form", "Week 1 Quiz", "Week 1 Lab"]}, {"title": "Bayesian Inference", "description": "In this week, we will discuss the continuous version of Bayes' rule and show you how to use it in a conjugate family, and discuss credible intervals. By the end of this week, you will be able to understand and define the concepts of prior, likelihood, and posterior probability and identify how they relate to one another.", "video": ["Bayesian Inference", "Module Learning Objectives", "From the Discrete to the Continuous", "Elicitation", "Conjugacy", "Inference on a Binomial Proportion", "The Gamma-Poisson Conjugate Families", "The Normal-Normal Conjugate Families", "Non-Conjugate Priors", "Credible Intervals", "Predictive Inference", "Week 2 Practice Quiz", "Week 2 Lab Instructions", "Weekly Feedback Form", "Week 2 Quiz", "Week 2 Lab"]}, {"title": "Decision Making", "description": "In this module, we will discuss Bayesian decision making, hypothesis testing, and Bayesian testing. By the end of this week, you will be able to make optimal decisions based on Bayesian statistics and compare multiple hypotheses using Bayes Factors. ", "video": ["Decision making", "Module Learning Objectives", "Losses and decision making", "Working with loss functions", "Minimizing expected loss for hypothesis testing", "Posterior probabilities of hypotheses and Bayes factors", "Comparing two proportions using Bayes factors: assumptions", "Comparing two proportions using Bayes factors", "Comparing two paired means using Bayes factors", "What to report?", "Posterior probability, p-values and paradoxes", "Comparing two independent means", "Comparing two independent means: hypothesis testing", "Week 3 Practice Quiz", "Week 3 Lab Instructions", "Weekly Feedback Form", "Week 3 Quiz", "Week 3 Lab"]}, {"title": "Bayesian Regression", "description": "This week, we will look at Bayesian linear regressions and model averaging, which allows you to make inferences and predictions using several models. By the end of this week, you will be able to implement Bayesian model averaging, interpret Bayesian multiple linear regression and understand its relationship to the frequentist linear regression approach. ", "video": ["Bayesian regression", "Module Learning Objectives", "Bayesian simple linear regression", "Checking for outliers", "Bayesian multiple regression", "Model selection criteria", "Bayesian model uncertainty", "Bayesian model averaging", "Stochastic exploration", "Priors for Bayesian model uncertainty", "R demo: crime and punishment", "Decisions under model uncertainty", "Week 4 Practice Quiz", "Week 4 Lab Instructions", "Week 4 Lab Supplement", "Weekly Feedback Form", "Week 4 Quiz", "Week 4 Lab"]}, {"title": "Perspectives on Bayesian Applications", "description": "This week consists of interviews with statisticians on how they use Bayesian statistics in their work, as well as the final project in the course.", "video": ["About this module", "Bayesian inference: a talk with Jim Berger", "Bayesian methods and big data: a talk with David Dunson", "Bayesian methods in biostatistics and public health: a talk with Amy Herring", "Weekly Feedback Form"]}, {"title": "Data Analysis Project", "description": "In this module you will use the data set provided to complete and report on a data analysis question. Please read the background information, review the report template (downloaded from the link in Lesson Project Information), and then complete the peer review assignment. ", "video": ["Project information", "Weekly Feedback Form", "Data Analysis Project"]}]}, {"title": "Linear Regression and Modeling ", "course_info": "About this course: This course introduces simple and multiple linear regression models. These models allow you to assess the relationship between variables in a data set and a continuous response variable. Is there a relationship between the physical attractiveness of a professor and their student evaluation scores? Can we predict the test score for a child based on certain characteristics of his or her mother? In this course, you will learn the fundamental theory behind linear regression and, through data examples, learn to fit, examine, and utilize regression models to examine relationships between multiple variables, using the free statistical software R and RStudio.", "target_audience": null, "created_by": "Duke University", "teach_by": [{"name": "Mine Çetinkaya-Rundel", "department": "Department of Statistical Science"}], "package_num": "3", "package_name": "Statistics with R Specialization ", "level": "Beginner", "rating": "4.7", "week_data": [{"title": "About Linear Regression and Modeling", "description": "This short module introduces basics about Coursera specializations and courses in general, this specialization: Statistics with R, and this course: Linear Regression and Modeling. Please take several minutes to browse them through. Thanks for joining us in this course!", "video": ["Introduction to Statistics with R", "About Statistics with R Specialization", "More about Linear Regression and Modeling"]}, {"title": "Linear Regression", "description": "In this week we’ll introduce linear regression. Many of you may be familiar with regression from reading the news, where graphs with straight lines are overlaid on scatterplots. Linear models can be used for prediction or to evaluate whether there is a linear relationship between two numerical variables. ", "video": ["Lesson Learning Objectives", "Introduction", "Correlation", "Residuals", "Least Squares Line", "Lesson Learning Objectives", "Prediction and Extrapolation", "Conditions for Linear Regression", "R Squared", "Regression with Categorical Explanatory Variables", "Week 1 Suggested Readings and Practice", "Week 1 Practice Quiz", "Week 1 Quiz"]}, {"title": "More about Linear Regression", "description": "Welcome to week 2! In this week, we will look at outliers, inference in linear regression and variability partitioning. Please use this week to strengthen your understanding on linear regression. Don't forget to post your questions, concerns and suggestions in the discussion forum!", "video": ["Lesson Learning Objectives", "Outliers in Regression", "Inference for Linear Regression", "Variability Partitioning", "Week 2 Suggested Readings and Exercises", "Week 2 Practice Quiz", "Instructions for Week 1 & 2 Lab", "Week 2 Quiz", "Week 1 & 2 Lab"]}, {"title": "Multiple Regression", "description": "In this week, we’ll explore multiple regression, which allows us to model numerical response variables using multiple predictors (numerical and categorical). We will also cover inference for multiple linear regression, model selection, and model diagnostics. Hope you enjoy!", "video": ["Introduction", "Lesson Learning Objectives", "Multiple Predictors", "Adjusted R Squared", "Collinearity and Parsimony", "Lesson Learning Objectives", "Inference for MLR", "Model Selection", "Diagnostics for MLR", "Week 3 Suggested Readings and Exercises", "Week 3 Practice Quiz", "Instructions for Week 3 Lab", "Week 3 Quiz", "Week 3 Lab"]}, {"title": "Final Project", "description": "In this week you will use the data set provided to complete and report on a data analysis question. Please read the background information, review the report template (downloaded from the link in Lesson Project Information), and then complete the peer review assignment. ", "video": ["Project Files and Rubric", "Data Analysis Project"]}]}, {"title": "Reproducible Research", "course_info": "About this course: This course focuses on the concepts and tools behind reporting modern data analyses in a reproducible manner. Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them.  The need for reproducibility is increasing dramatically as data analyses become more complex, involving larger datasets and more sophisticated computations. Reproducibility allows for people to focus on the actual content of a data analysis, rather than on superficial details reported in a written summary. In addition, reproducibility makes an analysis more useful to others because the data and code that actually conducted the analysis are available. This course will focus on literate statistical analysis tools which allow one to publish data analyses in a single document that allows others to easily execute the same analysis to obtain the same results.", "target_audience": null, "created_by": "Johns Hopkins University", "teach_by": [{"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "package_num": "5", "package_name": "Data Science Specialization ", "level": null, "rating": "4.5", "week_data": [{"title": "Week 1: Concepts, Ideas, & Structure", "description": "This week will cover the basic ideas of reproducible research since they may be unfamiliar to some of you. We also cover structuring and organizing a data analysis to help make it more reproducible. I recommend that you watch the videos in the order that they are listed on the web page, but watching the videos out of order isn't going to ruin the story. ", "video": ["Introduction", "Syllabus", "Pre-course survey", "Course Book: Report Writing for Data Science in R", "What is Reproducible Research About?", "Reproducible Research: Concepts and Ideas (part 1)", "Reproducible Research: Concepts and Ideas (part 2) ", "Reproducible Research: Concepts and Ideas (part 3) ", "Scripting Your Analysis ", "Structure of a Data Analysis (part 1)", "Structure of a Data Analysis (part 2)", "Organizing Your Analysis", "Week 1 Quiz"]}, {"title": "Week 2: Markdown & knitr", "description": "This week we cover some of the core tools for developing reproducible documents. We cover the literate programming tool knitr and show how to integrate it with Markdown to publish reproducible web documents. We also introduce the first peer assessment which will require you to write up a reproducible data analysis using knitr. ", "video": ["Coding Standards in R", "Markdown", "R Markdown", "R Markdown Demonstration", "knitr (part 1)", "knitr (part 2) ", "knitr (part 3) ", "knitr (part 4) ", "Introduction to Course Project 1", "Week 2 Quiz", "Course Project 1"]}, {"title": "Week 3: Reproducible Research Checklist & Evidence-based Data Analysis", "description": "This week covers what one could call a basic check list for ensuring that a data analysis is reproducible. While it's not absolutely sufficient to follow the check list, it provides a necessary minimum standard that would be applicable to almost any area of analysis.", "video": ["Communicating Results", "RPubs ", "Reproducible Research Checklist (part 1)", "Reproducible Research Checklist (part 2) ", "Reproducible Research Checklist (part 3) ", "Evidence-based Data Analysis (part 1)", "Evidence-based Data Analysis (part 2) ", "Evidence-based Data Analysis (part 3) ", "Evidence-based Data Analysis (part 4) ", "Evidence-based Data Analysis (part 5) "]}, {"title": "Week 4: Case Studies & Commentaries", "description": "This week there are two \ncase studies involving the importance of reproducibility in science for you to watch.", "video": ["Caching Computations", "Case Study: Air Pollution", "Case Study: High Throughput Biology", "Commentaries on Data Analysis", "Introduction to Peer Assessment 2", "Post-Course Survey", "Course Project 2"]}]}, {"title": "Improving your statistical inferences", "course_info": "About this course: This course aims to help you to draw better statistical inferences from empirical research. First, we will discuss how to correctly interpret p-values, effect sizes, confidence intervals, Bayes Factors, and likelihood ratios, and how these statistics answer different questions you might be interested in. Then, you will learn how to design experiments where the false positive rate is controlled, and how to decide upon the sample size for your study, for example in order to achieve high statistical power. Subsequently, you will learn how to interpret evidence in the scientific literature given widespread publication bias, for example by learning about p-curve analysis. Finally, we will talk about how to do philosophy of science, theory construction, and cumulative science, including how to perform replication studies, why and how to pre-register your experiment, and how to share your results following Open Science principles. \n\nIn practical, hands on assignments, you will learn how to simulate t-tests to learn which p-values you can expect, calculate likelihood ratio's and get an introduction the binomial Bayesian statistics, and learn about the positive predictive value which expresses the probability published research findings are true. We will experience the problems with optional stopping and learn how to prevent these problems by using sequential analyses. You will calculate effect sizes, see how confidence intervals work through simulations, and practice doing a-priori power analyses. Finally, you will learn how to examine whether the null hypothesis is true using equivalence testing and Bayesian statistics, and how to pre-register a study, and share your data on the Open Science Framework.\n\nAll videos now have Chinese subtitles. More than 10.000 learners have enrolled so far!", "target_audience": "Who is this class for: This course is aimed at anyone who wants to improve their statistical inferences, either because you are preparing to do empirical research for the first time, or because you were never taught these important statistical concepts in a clear and accessible manner in the past. I didn't know most of the things you will learn in this course until well after I got my PhD, and I've tried to create the course I would have liked to have gotten when I started to do research. You should have some basic knowledge about calculating descriptive statistics, and how to perform t-tests, correlations, and ANOVA's (If you don't have this knowledge, try https://www.coursera.org/learn/basic-statistics first). We will use R in many of the assignments, but you don't need any previous knowledge of R - we will mainly use it as a fancy calculator. ", "created_by": "Eindhoven University of Technology", "teach_by": [{"name": "Daniel Lakens", "department": "Department of Human-Technology Interaction"}], "package_num": null, "package_name": null, "level": "Intermediate", "rating": "4.9", "week_data": [{"title": "Introduction + Frequentist Statistics", "description": "", "video": ["Introduction", "Structure of the Course", "Passing the Course", "Research on Quizzes", "Consent Form for Use of Data", "Week 1: Overview", "Frequentism, Likelihoods, Bayesian statistics", "What is a p-value", "Type 1 and Type 2 errors", "Assignment 1: Which p-values can you expect?", "Answer Form Assignment 1 : Which p-values can you expect?", "Exam Week 1"]}, {"title": "Likelihoods & Bayesian Statistics", "description": "", "video": ["Week 2: Overview", "Interview with Professor Zoltan Dienes", "Interview: Zoltan Dienes", "Likelihoods", "Assignment 2.1: Likelihoods", "Answer Form Assignment 2.1", "Binomial Bayesian Inference", "Assignment 2.2: Bayesian Statistics", "Answer Form Assignment 2.2: Bayesian Statistics", "Bayesian Thinking", "Exam Week 2"]}, {"title": "Multiple Comparisons, Statistical Power, Pre-Registration", "description": "", "video": ["Week 3: Overview", "Type 1 error control", "Type 2 error control", "Assignment 3.1: Positive Predictive Value", "Answer Form Assignment 3.1: Positive Predictive Value", "Assignment 3.2: Optional Stopping", "Answer Form Assignment 3.2: Optional Stopping", "Interview Professor Dan Simons", "Interview Professor Dan Simons", "Pre-registration", "Exam Week 3"]}, {"title": "Effect Sizes", "description": "", "video": ["Week 4: Overview", "Effect Sizes", "Cohen's d", "Correlations", "Assignment 4: Calculating Effect Sizes", "Answer Form Assignment 4: Effect Sizes", "Exam Week 4"]}, {"title": "Confidence Intervals, Sample Size Justification, P-Curve analysis", "description": "", "video": ["Week 5: Overview", "Confidence Intervals", "Assignment 5.1: Confidence Intervals", "Answer Form Assignment 5.1: Confidence Intervals and Capture Percentages", "Sample Size Justification", "Assignment 5.2: Random Variation and Power Analysis", "Answer Form Assignment 5.2: Random Variation and Power Analysis", "P-Curve Analysis", "Exam Week 5"]}, {"title": "Philosophy of Science & Theory", "description": "", "video": ["Week 6: Overview", "Philosophy of Science", "The Null is Always False", "Assignment 6: Equivalence Testing", "Answer Form Assignment 6: Equivalence Testing", "Theory Construction", "Exam Week 6"]}, {"title": "Open Science", "description": "", "video": ["Week 7: Overview", "Replications", "Publication Bias", "Open Science", "Assignment 7: Open Science"]}, {"title": "Final Exam", "description": "This module contains a practice exam and a graded exam. Both quizzes cover content from the entire course. We recommend making these exams only after you went through all the other modules.", "video": ["Practice Exam", "Graded Final Exam"]}]}, {"title": "Understanding Clinical Research: Behind the Statistics", "course_info": "About this course: If you’ve ever skipped over`the results section of a medical paper because terms like “confidence interval” or “p-value” go over your head, then you’re in the right place. You may be a clinical practitioner reading research articles to keep up-to-date with developments in your field or a medical student wondering how to approach your own research. Greater confidence in understanding statistical analysis and the results can benefit both working professionals and those undertaking research themselves. \n\nIf you are simply interested in properly understanding the published literature or if you are embarking on conducting your own research, this course is your first step. It offers an easy entry into interpreting common statistical concepts without getting into nitty-gritty mathematical formulae. To be able to interpret and understand these concepts is the best way to start your journey into the world of clinical literature. That’s where this course comes in - so let’s get started!\n\nThe course is free to enroll and take. You will be offered the option of purchasing a certificate of completion which you become eligible for, if you successfully complete the course requirements. This can be an excellent way of staying motivated!  Financial Aid is also available.", "target_audience": null, "created_by": "University of Cape Town", "teach_by": [{"name": "Juan H Klopper", "department": "Department of Surgery "}], "package_num": null, "package_name": null, "level": null, "rating": "4.7", "week_data": [{"title": "Getting things started by defining study types", "description": "Welcome to the first week of this course. We’ll be tackling five broad topics to provide you with an intuitive understanding of clinical research results. This isn’t a comprehensive statistics course, but it offers a practical orientation to the field of medical research and commonly used statistical analysis. The first topics will look at research methods and the collection of data - with a specific focus on study types. By the end of the lectures you should be able to identify which study types are being used and why the researchers selected them when you are reading a paper.", "video": ["Introduction to Understanding Clinical Research", "About the course", "How this course works", "Pre-course survey", "Introduce yourself to your peers", "Study types", "Observing and intervening: Observational & experimental studies", "Key notes: Observational and experimental studies", "Observing and describing: Case series studies", "Key notes: Case series studies", "Comparing groups: Case-control studies", "Key notes: Case-control studies", "Collecting data at one point in time: Cross-sectional studies", "Key notes: Cross-sectional studies", "Studying a group with common traits: Cohort studies", "Key notes: Cohort studies", "Let's intervene: Experimental studies", "Key notes: Experimental studies", "Working with existing research: Meta-analysis and Systematic Review", "Key notes: Meta-analysis and systematic review", "Test your knowledge: Study types", "Peer review introduction", "Doing a literature search: Part 1", "Doing a literature search: Part 2", "Week 1: Navigating Clinical Research "]}, {"title": "Describing your data ", "description": "With the next topics, we finally get started with the statistics. Have you ever looked at the methods and results section of any healthcare research publication and noted the variety of statistical tests used?  You would have come across terms like t-test, Mann-Whitney-U test, Wilcoxon test, Fisher’s exact test and the ubiquitous chi-squared test.  Why so many tests you might wonder?  It’s all about types of data.  In this week,  I am going to tackle the differences in data which determine what type of statistical test we can use in making sense of our data.", "video": ["Introduction", "Some key concepts: Definitions", "Key notes: Definitions", "Data types", "Key notes: Data types", "Arbitary classification: Nominal categorical data", "Key notes: Nominal categorical data", "Natural ordering of attributes: Ordinal categorical data", "Key notes: Ordinal categorical data", "Measurements and numbers: Numerical data types", "Key notes: Numerical data types", "How to tell the difference: Discrete and continuous variables", "Key notes: Discrete and continuous variables", "Test your knowledge: Data types", "Introduction", "Key notes: Describing the data", "Measures of central tendency", "Key notes: Measures of central tendency", "Measures of dispersion", "Key notes: Measures of dispersion", "Visual representation of data", "(Optional) Setting up spreadsheet software to do your own analysis", "(Optional) Descriptive statistics using spreadsheet software", "Test your knowledge: Measures of central tendency and dispersion", "Making inferences: Sampling", "Key notes: Sampling", "Types of sampling", "Key notes: Types of sampling", "Test your knowledge: Sampling", "Case study 1", "Share an example of clinical research", "Week 2 Graded Quiz"]}, {"title": "Building an intuitive understanding of statistical analysis", "description": "There is hardly any healthcare professional who is unfamiliar with the p-value.  It is usually understood to have a watershed value of 0.05.  If a research question is evaluated through the collection of data points and statistical analysis reveals a value less that 0.05, we accept this a proof that some significant difference was found, at least statistically.In reality things are a bit more complicated than that.  The literature is currently full of questions about the ubiquitous p-vale and why it is not the panacea many of us have used it as. During this week you will develop an intuitive understanding of concept of a p-value. From there, I'll move on to the heart of probability theory, the Central Limit Theorem and data distribution.", "video": ["P-values: P is for probability", "Key notes: P-values", "Working out the probability: Rolling dice", "Key notes: Rolling dice", "Area under the curve: Continuous data types", "Key notes: Continuous data types", "Test your knowledge: Probability", "Introduction to the central limit theorem: The heart of probability theory", "Introduction to the central limit theorem", "Asymmetry and peakedness: Skewness and Kurtosis", "Key notes: Skewness and kurtosis", "Learning from the lotto: Combinations", "Key notes: Combinations", "Approximating a bell-shaped curve: The central limit theorem", "Key notes: Central limit theorem", "Test your knowledge: The central limit theorem", "Patterns in the data: Distributions", "Key notes: Distributions", "The bell-shaped curve: Normal distribution", "Key notes: Normal distribution", "Plotting a sample statistic: Sampling distribution", "Key notes: Sampling distribution", "Standard normal distribution: Z distribution", "Key notes: Z-distribution", "Estimating population parameters: t-distribution", "Key notes: The t-distibution", "(Optional) Generating random data point values using spreadsheet software", "Test your knowledge: Distributions", "Case study 2", "Week 3 Graded Quiz"]}, {"title": "The important first steps: Hypothesis testing and confidence levels", "description": "In general, a researcher has a question in mind that he or she needs an answer to.  Everyone might have an opinion on the question (or answer), but an investigator looks for the answer by designing an experiment and investigating the outcome. In the first lesson we will look at hypotheses and how they relate to ethical and unbiased research and reporting.We'll also tackle Confidence intervals which I believe are one of the least understood and often misrepresented values in healthcare research. The most common tests used in the literature to compare numerical data point values are t-tests, analysis of variance, and linear regression.  In the last lesson we take a closer look at these tests, but perhaps more importantly, their strict assumptions. ", "video": ["Introduction to Hypothesis Testing", "Testing assumptions: Null and alternative hypothesis", "Key notes: Null and alternative hypothesis", "Is there a difference?: Alternative Hypothesis", "Key notes: Alternative hypothesis", "Type I and II: Hypothesis testing errors", "Key notes: Hypothesis errors", "Testing your knowledge: Hypothesis", "Introduction to confidence intervals", "Key notes: Introduction to confidence intervals", "How confident are you?: Confidence levels", "Key notes: Confidence levels", "Interval estimation: Confidence intervals", "Key notes: Confidence intervals", "(Optional) Calculating confidence intervals using spreadsheet software", "Test your knowledge: Confidence intervals", "Week 4 Peer review "]}, {"title": "Which test should you use?", "description": "The most common statistical test that you might come across in the literature is the t-test.  There are, in actual fact, a few t-tests, but the one most are familiar with, is of course, Student’s t-test and its ubiquitous p-value.  Not everyone, though, knows that the name Student was actually a pseudonym, used by William Gosset (1876 - 1937).  Parametric tests have very strict assumptions that must be met before their use is justified.  In this lesson we take a closer look at these tests, but perhaps more importantly, their strict assumptions.  Once you know these, you will be able to identify when these tests are used inappropriately.", "video": ["Introduction to parametric tests", "Key notes: Parametric tests", "Student's t-test", "Key notes: Student's t-test", "ANOVA", "Key notes: ANOVA", "Linear Regression", "Key notes: Linear regression", "(Optional) Student's t-test in action", "Test your knowledge: Parametric tests", "Introduction to nonparametric tests", "Checking for normality", "Key notes: Nonparametric tests", "Thinking nonparametrically", "Comparing paired observations: Signs", "Ordering values: Ranking", "Paired comparisons: Sign ranks", "Summation of ranks: Rank sums", "Comparing two populations: Mann-Whitney-U test", "More nonparametric tests", "Key notes: Nonparametric tests", "Test your knowledge: Non-parametric tests", "Case study 3", "Week 5 Graded Quiz"]}, {"title": "Categorical data and analyzing accuracy of results ", "description": "Congratulations! You've reached the final week of the course Understanding Clinical Research. In this lesson we will take a look at how good tests are at picking up the presence or absence of disease, helping us choose appropriate tests, and how to interpret positive and negative results.  We’ll decipher sensitivity, specificity, positive and negative predictive values. You'll end of this course with a final exam, to test the knowledge and application you've learned in this course. I hope you've enjoyed this course and it helps your understanding of clinical research. ", "video": ["Introduction to comparing categorical data", "Observed frequencies: Contingency tables", "Comparing observed and expected values: Chi-square test", "Association between two variables: Fisher's exact test", "Key notes: Comparing categorical data", "(Optional) Calculating chi-square test using spreadsheet software", "Testing your knowledge: Comparing categorical data", "Introduction to sensitivity and specificity", "Measuring performance: Sensitivity and specificity", "Proportions of results: Positive and negative predictive values", "Keynotes: Sensitivity, specificity, positive and negative predictive values", "Test your knowledge: Sensitivity, specificity and predictive values", "Interesting online videos", "Congratulations on completing the course", "Week 6 Final examination"]}]}, {"title": "Developing Data Products", "course_info": "About this course: A data product is the production output from a statistical analysis. Data products automate complex analysis tasks or use technology to expand the utility of a data informed model, algorithm or inference. This course covers the basics of creating data products using Shiny, R packages, and interactive graphics. The course will focus on the statistical fundamentals of creating a data product that can be used to tell a story about data to a mass audience.", "target_audience": null, "created_by": "Johns Hopkins University", "teach_by": [{"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}], "package_num": "9", "package_name": "Data Science Specialization ", "level": null, "rating": "4.5", "week_data": [{"title": "Course Overview", "description": "In this overview module, we'll go over some information and resources to help you get started and succeed in the course. ", "video": ["Welcome to Developing Data Products", "Syllabus", "Welcome", "Book: Developing Data Products in R", "Community Site", "R and RStudio Links & Tutorials"]}, {"title": "Shiny, GoogleVis, and Plotly", "description": "Now we can turn to the first substantive lessons. In this module, you'll learn how to develop basic applications and interactive graphics in shiny, compose interactive HTML graphics with GoogleVis, and prepare data visualizations with Plotly.", "video": ["Shiny", "Shinyapps.io Project", "Shiny 1.1", "Shiny 1.2", "Shiny 1.3", "Shiny 1.4", "Shiny 1.5", "Shiny 2.1", "Shiny 2.2", "Shiny 2.3", "Shiny 2.4", "Shiny 2.5", "Shiny 2.6", "Shiny Gadgets 1.1", "Shiny Gadgets 1.2", "Shiny Gadgets 1.3", "GoogleVis 1.1", "GoogleVis 1.2", "Plotly 1.1", "Plotly 1.2", "Plotly 1.3", "Plotly 1.4", "Plotly 1.5", "Plotly 1.6", "Plotly 1.7", "Plotly 1.8", "Quiz 1"]}, {"title": "R Markdown and Leaflet", "description": "During this module, we'll learn how to create R Markdown files and embed R code in an Rmd. We'll also explore Leaflet and use it to create interactive annotated maps.", "video": ["R Markdown 1.1", "R Markdown 1.2", "R Markdown 1.3", "R Markdown 1.4", "R Markdown 1.5", "R Markdown 1.6", "Three Ways to Share R Markdown Products", "Leaflet 1.1", "Leaflet 1.2", "Leaflet 1.3", "Leaflet 1.4", "Leaflet 1.5", "Leaflet 1.6", "Quiz 2", "R Markdown and Leaflet"]}, {"title": "R Packages", "description": "In this module, we'll dive into the world of creating R packages and practice developing an R Markdown presentation that includes a data visualization built using Plotly.", "video": ["R Packages", "R Packages (Part 1)", "R Packages (Part 2)", "Building R Packages Demo", "R Classes and Methods (Part 1)", "R Classes and Methods (Part 2)", "Quiz 3", "R Markdown Presentation & Plotly"]}, {"title": "Swirl and Course Project ", "description": "Week 4 is all about the Course Project, producing a Shiny Application and reproducible pitch.", "video": ["Swirl 1.1", "Swirl 1.2", "Swirl 1.3", "Post-Course Survey", "Course Project: Shiny Application and Reproducible Pitch"]}]}, {"title": "Statistical Reasoning for Public Health 1: Estimation, Inference, & Interpretation", "course_info": "About this course: A conceptual and interpretive public health approach to some of the most commonly used methods from basic statistics.", "target_audience": null, "created_by": "Johns Hopkins University", "teach_by": [{"name": "John McGready, PhD, MS", "department": "Bloomberg School of Public Health"}], "package_num": null, "package_name": null, "level": null, "rating": "4.8", "week_data": [{"title": "Introduction and Module 1", "description": "This module, consisting of one lecture set, is intended to whet your appetite for the course, and examine the role of biostatistics in public health and medical research.  Topics covered include study design types, data types, and data summarization.", "video": ["Welcome to the Course! ", "Syllabus", "Introduction to Module 1 ", "Lecture 1A: The Role of Statistics in Public Health Research", "Lecture 1B: Samples Versus Population ", "Lecture 1C: Considerations with Regard to Study Design ", "Lecture 1D: Data Types and Summarization ", "Lecture 1E: Self-Assessment/Active Learning Exercise "]}, {"title": "Module 2A: Summarization and Measurement", "description": "Module 2A consists of two lecture sets that cover measurement and summarization of continuous data outcomes for both single samples, and the comparison of two or more samples.  Please see the posted learning objectives for these two lecture sets for more detail.", "video": ["Introduction to Module 2 ", "Learning Objectives, Lecture Set 2", "Lecture 2A: Continuous Data: Useful Summary Statistics", "Lecture 2B: Continuous Data: Visual Displays ", "Lecture 2C: Continuous Data: The Role of Sample Size on Sample Based Estimates ", "Lecture 2D: Continuous Data: Comparing Distributions ", "Lecture 2E: Self Assessment/Active Learning Exercise ", "Learning Objectives, Lecture Set 3", "Lecture 3A: The Standard Normal Distribution Defined", "Lecture 3B: Applying the Principles of the Normal Distribution to Sample Data ", "Lecture 3C: What Happens When We Apply the Properties of the Normal Distribution to Data Not Approximately Normal: A Warning ", "Lecture 3D: Some Practice Exercises . ", "Supporting Documents for Homework 1A", "Homework 1A", "Homework 1B", "Homework 1C", "Supporting Information for Homework 1D", "Homework 1D", "Supporting Information for Homework 1E", "Homework 1E", "Quiz 1 Solutions", "Quiz 1 (Covers Material Through Lecture 3)"]}, {"title": "Module 2B: Summarization and Measurement ", "description": "Module 2B includes a single lecture set on summarizing binary outcomes.  While at first, summarization of binary outcome may seem simpler than that of continuous outcomes, things get more complicated with group comparisons.  Included in the module are examples of and comparisons between risk differences, relative risk and odds ratios. Please see the posted learning objectives for these this module for more details.", "video": ["Learning Objectives, Lecture Set 4", "Lecture 4A: Binary Data: Definition and Summarization (Binomial Distribution, P-Hat, SD)", "Lecture 4B, part 1", "Lecture 4B, part 2", "Lecture 4C: Comparing Distributions of Binary Data: Odds Ratios ", "Lecture 4D:  A Brief Note About Ratios ", "Lecture 4E: Self Assessment/Active Learning Exercise "]}, {"title": "Module 2C: Summarization and Measurement ", "description": "This module consists of a single lecture set on time-to-event outcomes.   Time-to-event data comes primarily from prospective cohort studies with subjects who haven to had the outcome of interest at their time of enrollment.   These subjects are followed for a pre-established period of time until they either have there outcome, dropout during the active study period, or make it to the end of the study without having the outcome.  The challenge with these data is that the time to the outcome is fully observed on some subjects, but not on those who do not have the outcome during their tenure in the study. Please see the posted learning objectives for each lecture set in this module for more details.", "video": ["Learning Objectives, Lecture Set 5", "Lecture 5A: Time to Event Data: Definition (Censoring) and Numerical Summary Measures (Incidence Rates)", "Lecture 5B: Numerically Comparing Groups on Time to Event Outcomes ", "Lecture 5C Part 1 Time to Event Data: Graphical Summarization: Kapalan-Meier Approach", "Lecture 5C Part 2 Time to Event Data: Graphical Summarization: Kapalan-Meier Approach", "Lecture 5D: Graphically Comparing Groups on Time to Event Outcomes ", "Lecture 5E: Self Assessment/Active Learning Exercise ", "Supporting Documents For Homework 2A", "Homework 2A", "Homework 2B", "Supporting Information for Homework 2C", "Homework 2C", "Homework 2D", "Homework 2E", "Homework 2F", "Formula Files for Quiz 2", "Quiz 2 Solutions", "Quiz 2 (Covers Material Through Lecture 5)"]}, {"title": "Module 3A: Sampling Variability and Confidence Intervals", "description": "Understanding sampling variability is the key to defining the uncertainty in any given sample/samples based estimate from a single study.  In this module, sampling variability is explicitly defined and explored through simulations.   The resulting patterns from these simulations will give rise to a mathematical results that is the underpinning of all statistical interval estimation and inference: the central limit theorem.  This result will used to create 95% confidence intervals for population means, proportions and rates from the results of a single random sample.", "video": ["Introduction to Module 3 ", "Learning Objectives, Lecture Set 6", "Lecture 6A: Sampling Distribution Definition", "Lecture 6B: Examples: Sampling Distribution for a Single Mean ", "Lecture 6C: Examples: Sampling Distribution for a Single Proportion, Incidence Rate ", "Lecture 6D: Estimating Sampling Distribution Characteristics from Single Samples of Data", "Lecture 6E: Self Assessment/Active Learning Exercise ", "Learning Objectives, Lecture Set 7", "Lecture 7A: Confidence Intervals for Population Means", "Lecture 7B: Confidence Intervals for Sample Proportions and Rates ", "Lecture 7C: On the Interpretation of CIs ", "Lecture 7D: A Note about CIs for Smaller Samples/Exact CIs ", "Lecture 7E: Self-Assessment/Active Learning Exercise "]}, {"title": "Module 3B: Sampling Variability and Confidence Intervals", "description": "The concepts from the previous module (3A) will be extended create 95% CIs for  group comparison measures (mean differences, risk differences, etc..) based on the results from a single study.", "video": ["Learning Objectives, Lecture Set 8", "Lecture 8A: An Overview of Confidence Intervals for Population Comparison Measures", "Lecture 8B: Confidence Intervals for Differences in Population Means ", "Lecture 8C: Confidence Intervals for Binary Comparisons: Part 1, Difference in Proportions (Risk Difference)   - Subtitles Pending", "Lecture 8D: Confidence Intervals for Binary Comparisons: Part 2: Ratio of Proportions (Relative Risk), Odds Ratio   - Subtitles Pending", "Lecture 8E: Confidence Intervals for Incidence Rate Ratios ", "Lecture 8F: Revisiting Ratios and the Log Scale (with Respect to Effect Sized and CIs) \t", "Lecture 8G: Self Assessment/Active Learning Exercise ", "Supporting Information For Homework 3A", "Homework 3A", "Homework 3B", "Homework 3C", "Quiz 3 Solutions", "Quiz 3 (Covers Material Through Lecture 8)"]}, {"title": "Module 4A: Making Group Comparisons: The Hypothesis Testing Approach", "description": "Module 4A shows a complimentary approach to confidence intervals when comparing a summary measure between two populations via two samples; statistical hypothesis testing.  This module will cover some of the most used statistical tests including the t-test for means, chi-squared test for proportions and log-rank test for time-to-event outcomes.", "video": ["Introduction to Module 4 ", "Learning Objectives, Lecture Set 9", "Lecture 9A: Two-Group Hypothesis Testing: The General Concept", "Lecture 9B: Comparing Means between Two Populations: The Paired Approach ", "Lecture 9C: Comparing Means between Two Populations: The Unpaired Approach ", "Lecture 9D: Section D: Debriefing on the p-value, Part 1 ", "Lecture 9E: Self Assessment Exercise ", "Learning Objectives, Lecture Set 10", "Lecture 10A: Comparing Proportions between Two Populations: The “Z-Test” Approach", "Lecture 10B: Comparing Proportions between Two Populations: Chi-Squared and Fisher’s Exact Tests ", "Lecture 10C: Comparing Time-to-Event Between Two Populations: The Log-Rank Test ", "Lecture 10D: Debriefing on the P-Value, Part II "]}, {"title": "Module 4B: Making Group Comparisons: The Hypothesis Testing Approach", "description": "Module 4B extends the hypothesis tests for two populations comparisons to \"omnibus\" tests for comparing means, proportions or incidence rates between more than two populations with one test", "video": ["Learning Objectives, Lecture Set 11", "Lecture 11A: (Hypothesis Testing) Comparing Means Between More than Two Populations: Analysis of Variance (ANOVA)", "Lecture 11B: (Hypothesis Testing) Comparing Proportions between More than Two Populations: Chi-Square Tests ", "Lecture 11C: Hypothesis Testing) Comparing Survival Curves between More than Two Populations: Log-Rank Tests ", "Supporting Documents For Homework 4", "Homework 4A", "Homework 4B", "Homework 4C", "Homework 4D", "Homework 4E", "Quiz 4 Solutions", "Learning Objectives, Lecture Set 12", "Lecture 12A: Precision and Sample Size : An Overview", "Lecture 12B: Computing Sample Size to Achieve a Desired Level of Precision : Single Population Quantities", "Lecture 12C: Computing Sample Size to Achieve a Desired Level of Precision : Population  Comparison Quantities", "Learning Objectives, Lecture Set 13", "Lecture 13A: Power and Its Influences", "Lecture 13B: Sample Size Computations For Studies Comparing Two (or More) Means", "Lecture 13C: Sample Size Computations For Studies Comparing Two (or More) Proportions or Incidence Rates", "Lecture 13D: Sample Size and Study Design Principles:  A Brief Summary", "Lecture 13E: An Example of the Mathematics Behind Power Computations", "Quiz 4 (Covers Material Through Lecture 11)"]}]}, {"title": "An Intuitive Introduction to Probability", "course_info": "About this course: This course will provide you with an intuitive and practical introduction into Probability Theory. You will be able to learn how to apply Probability Theory in different scenarios and you will earn a \"toolbox\" of methods to deal with uncertainty in your daily life. \n\nThe course is split in 5 modules. In each module you will first have an easy introduction into the topic, which will serve as a basis to further develop your knowledge about the topic and acquire the \"tools\" to deal with uncertainty. Additionally, you will have the opportunity to complete 5 exercise sessions to reflect about the content learned in each module and start applying your earned knowledge right away. \n\nThe topics covered are: \"Probability\", \"Conditional Probability\", \"Applications\", \"Random Variables\", and \"Normal Distribution\".\n\nYou will see how the modules are taught in a lively way, focusing on having an entertaining and useful learning experience! We are looking forward to see you online!", "target_audience": "Who is this class for: This course is aimed for all those who want to learn how to improve decisions that involve some uncertainty. The course is constructed such that no previous knowledge of statistics is required to follow it. The acquired knowledge can be applied in any field that involves uncertainty.", "created_by": "University of Zurich", "teach_by": [{"name": "Karl Schmedders", "department": "Department of Business Administration"}], "package_num": null, "package_name": null, "level": "Beginner", "rating": "4.8", "week_data": [{"title": "Probability", "description": "In this module we will learn about probabilities and perform our first calculations using probability formulas. We want to get comfortable with the idea that probabilities describe the chance of uncertain events occurring.", "video": ["Welcome", "Script: Part 1", "Introduction", "Definition and Rules", "A First Look at Statistical Independence", "Subjective Probabilities", "Empirical Probabilities: Benford's Law", "Exercise", "Introductory Quiz", "1.1", "1.2", "1.3", "1.4", "1.5", "1.6"]}, {"title": "Conditional Probability", "description": "The arrival of new information may lead us to alter our probabilistic assessments of uncertain events. In this module, we will learn how the concept of \"conditional\" probabilities allows us to make these changes correctly.", "video": ["Intuition", "Script: Part 2", "Definition", "Multiplication Rules", "Probability Tables", "Bayes Rule", "Exercise", "2.1", "2.2", "2.3", "2.4", "2.5", "2.6"]}, {"title": "Application", "description": "We will discuss some fascinating every-day applications of probability. In addition to entertaining examples, we will also review very serious applications from finance and law.", "video": ["The Birthday Problem", "The Monty Hall Problem", "Structuring Risks: Part 1", "Structuring Risks: Part 2", "The Prosecutor's Fallacy", "The Sad Story of Sally Clark", "3.1", "3.2", "3.3.1", "3.3.2", "3.4", "3.5"]}, {"title": "Discrete Random Variables", "description": "In this module we move beyond probabilities and learn about important summary measures such as expected values, variances, and standard deviations. We also learn about the most popular discrete probability distribution, the binomial distribution.", "video": ["Discrete Random Variables", "Script: Part 4", "Expected Value", "Measures of Dispersion", "Application: Financial Model", "Binomial Distribution", "Application: Airline Overbooking", "Exercise 1", "Exercise 2", "4.1", "4.2", "4.3", "4.4", "4.5", "4.6", "4.7", "4.8"]}, {"title": "Normal Distribution", "description": "We want to get comfortable with the normal distribution. We will discuss what the famous bell curve really represents. And we will learn how easy it is to calculate normal probabilities.", "video": ["Continuous Random Variables", "Script: Part 5", "Normal Distribution", "Calculating Normal Probabilities", "Calculations with the Normal Distribution", "Application of the Normal Distribution", "Exercise", "5.1", "5.2", "5.3", "5.4", "5.5"]}]}, {"title": "A Crash Course in Causality:  Inferring Causal Effects from Observational Data", "course_info": "About this course: We have all heard the phrase “correlation does not equal causation.”  What, then, does equal causation?  This course aims to answer that question and more!  \n\nOver a period of 5 weeks, you will learn how causal effects are defined, what assumptions about your data and models are necessary, and how to implement and interpret some popular statistical methods.  Learners will have the opportunity to apply these methods to example data in R (free statistical software environment).\n\nAt the end of the course, learners should be able to:\n1.  Define causal effects using potential outcomes\n2.  Describe the difference between association and causation\n3.  Express assumptions with causal graphs\n4.  Implement several types of causal inference methods (e.g. matching, instrumental variables, inverse probability of treatment weighting)\n5.  Identify which causal assumptions are necessary for each type of statistical method\n\nSo join us.... and discover for yourself why modern statistical methods for estimating causal effects are indispensable in so many fields of study!", "target_audience": "Who is this class for: Familiarity with traditional statistical methods, such as regression models, and basic probability recommended. Familiarity with free statistical environment R recommended.  Learners should successfully download R before starting the course.", "created_by": "University of Pennsylvania", "teach_by": [{"name": "Jason A. Roy, Ph.D. ", "department": "Department of Biostatistics, Epidemiology, and Informatics"}], "package_num": null, "package_name": null, "level": "Intermediate", "rating": "4.9", "week_data": [{"title": "Welcome and Introduction to Causal Effects", "description": "This module focuses on defining causal effects using potential outcomes. A key distinction is made between setting/manipulating values and conditioning on variables. Key causal identifying assumptions are also introduced.", "video": ["Welcome to \"A Crash Course in Causality\"", "Confusion over causality", "Potential outcomes and counterfactuals", "Hypothetical interventions", "Causal effects", "Practice Quiz", "Causal assumptions", "Stratification", "Practice Quiz", "Incident user and active comparator designs", "Causal effects"]}, {"title": "Confounding and Directed Acyclic Graphs (DAGs)", "description": "This module introduces directed acyclic graphs. By understanding various rules about these graphs, learners can identify whether a set of variables is sufficient to control for confounding.", "video": ["Confounding", "Causal graphs", "Relationship between DAGs and probability distributions", "Paths and associations", "Conditional independence (d-separation)", "Practice Quiz", "Confounding revisited", "Backdoor path criterion", "Disjunctive cause criterion", "Identify from DAGs sufficient sets of confounders"]}, {"title": "Matching and Propensity Scores", "description": "An overview of matching methods for estimating causal effects is presented, including matching directly on confounders and matching on the propensity score. The ideas are illustrated with data analysis examples in R.", "video": ["Observational studies", "Overview of matching", "Matching directly on confounders", "Practice Quiz", "Greedy (nearest-neighbor) matching", "Optimal matching", "Assessing balance", "Analyzing data after matching", "Practice Quiz", "Sensitivity analysis", "Data example in R", "Propensity scores", "Propensity score matching", "Propensity score matching in R", "Matching", "Propensity score matching", "Data analysis project - analyze data in R using propensity score matching"]}, {"title": "Inverse Probability of Treatment Weighting (IPTW)", "description": "Inverse probability of treatment weighting, as a method to estimate causal effects, is introduced. The ideas are illustrated with an IPTW data analysis in R.", "video": ["Intuition for Inverse Probability of Treatment Weighting (IPTW)", "More intuition for IPTW estimation", "Marginal structural models", "IPTW estimation", "Assessing balance", "Practice Quiz", "Distribution of weights", "Remedies for large weights", "Doubly robust estimators", "Data example in R", "IPTW", "Data analysis project - carry out an IPTW causal analysis"]}, {"title": "Instrumental Variables Methods", "description": "This module focuses on causal effect estimation using instrumental variables in both randomized trials with non-compliance and in observational studies. The ideas are illustrated with an instrumental variables analysis in R.", "video": ["Introduction to instrumental variables", "Randomized trials with noncompliance", "Compliance classes", "Assumptions", "Practice Quiz", "Causal effect identification and estimation", "IVs in observational studies", "Two stage least squares", "Weak instruments", "Practice Quiz", "IV analysis in R", "Instrumental variables / Causal effects in randomized trials with non-compliance"]}]}, {"title": "Inferential Statistics", "course_info": "About this course: Inferential statistics are concerned with making inferences based on relations found in the sample, to relations in the population. Inferential statistics help us decide, for example, whether the differences between groups that we see in our data are strong enough to provide support for our hypothesis that group differences exist in general, in the entire population.\n\nWe will start by considering the basic principles of significance testing: the sampling and test statistic distribution, p-value, significance level, power and type I and type II errors. Then we will consider a large number of statistical tests and techniques that help us make inferences for different types of data and different types of research designs. For each individual statistical test we will consider how it works, for what data and design it is appropriate and how results should be interpreted. You will also learn how to perform these tests using freely available software. \n\nFor those who are already familiar with statistical testing: We will look at z-tests for 1 and 2 proportions,  McNemar's test for dependent proportions, t-tests for 1 mean (paired differences) and 2 means, the Chi-square test for independence, Fisher’s exact test, simple regression (linear and exponential) and multiple regression (linear and logistic), one way and factorial analysis of variance, and non-parametric tests (Wilcoxon, Kruskal-Wallis, sign test,  signed-rank test, runs test).", "target_audience": null, "created_by": "University of Amsterdam", "teach_by": [{"name": "Annemarie Zand Scholten", "department": "Economics and Business"}, {"name": "Emiel van Loon", "department": "Institute for Biodiversity and Ecosystem Dynamics"}], "package_num": "4", "package_name": "Methods and Statistics in Social Sciences Specialization ", "level": null, "rating": "4.4", "week_data": [{"title": "Before we get started...", "description": "[formatted text here]", "video": ["Hi there", "Welcome to Inferential Statistics!", "How to navigate this course", "How to contribute", "General info - What will I learn in this course?", "Course format - How is this course structured?", "Requirements - What resources do I need?", "Grading - How do I pass this course?", "Team - Who created this course?", "Honor Code - Integrity in this course", "Useful literature and documents"]}, {"title": "Comparing two groups", "description": "In this second module of week 1 we dive right in with a quick refresher on statistical hypothesis testing. Since we're assuming you just completed the course Basic Statistics, our treatment is a little more abstract and we go really fast! We provide the relevant Basic Statistics videos in case you need a gentler introduction. After the refresher we discuss methods to compare two groups on a categorical or quantitative dependent variable. We use different test for independent and dependent groups.", "video": ["Comparing two groups - Drawing inferences", "1.01 Null hypothesis testing", "1.02 P-values", "1.03 Confidence intervals and two-sided tests", "1.04 Power", "Comparing two groups - Independent groups", "1.05 Two independent proportions", "1.06 Two independent means", "Comparing two groups - Dependent groups", "1.07 Two dependent proportions", "1.08 Two dependent means", "Comparing two groups - Controlling for other variables", "1.09 Controlling for other variables", "Comparing two groups - Transcripts", "R lab - Getting started (part1)", "R lab - Getting started (part 2)", "Comparing two groups", "R lab - Comparing two groups"]}, {"title": "Categorical association", "description": "In this module we tackle categorical association. We'll mainly discuss the Chi-squared test that allows us to decide whether two categorical variables are related in the population. If two categorical variables are unrelated you would expect that categories of these variables don't 'go together'. You would expect the number of cases in each category of one variable to be proportionally similar at each level of the other variable. The Chi-squared test helps us to compare the actual number of cases for each combination of categories (the joint frequencies) to the expected number of cases if the variables are unrelated.", "video": ["Categorical association - Chi-squared test for association", "2.01 Categorical association and independence", "2.02  The Chi-squared test", "2.03 Interpreting the Chi-squared test", "Categorical association - Chi-squared test for goodness of fit", "2.04 Chi-squared as goodness-of-fit", "Categorical association - Sidenotes and an alternative to the Chi-squared test", "2.05 The Chi-squared test - sidenotes", "2.06 Fisher's exact test", "Categorical association - Transcripts", "Categorical association", "R lab - Categorical association"]}, {"title": "Simple regression", "description": "In this module we’ll see how to describe the association between two quantitative variables using simple (linear) regression analysis. Regression analysis allows us to model the relation between two quantitative variables and - based on our sample -decide whether a 'real' relation exists in the population. Regression analysis is more useful than just calculating a correlation coefficient, since it allows us assess how well our regression line fits the data, it helps us to identify outliers and to predict scores on the dependent variable for new cases.", "video": ["Simple regression - Describing quantitative association", "3.01 The regression line", "3.02 The regression equation", "3.03 The regression model", "3.04 Predictive power", "Simple regression - Drawing inferences", "3.05 Pitfalls in regression", "3.06 Testing the model", "3.07 Checking assumptions", "3.08 CI and PI for predicted values", "Simple regression - Exponential regression", "3.09 Exponential regression", "Simple regression - Transcripts", "Simple regression", "R lab - Simple regression"]}, {"title": "Multiple regression", "description": "In this module we’ll see how we can use more than one predictor to describe or predict a quantitative outcome variable. In the social sciences relations between psychological and social variables are generally not very strong, since outcomes are generally influences by complex processes involving many variables. So it really helps to be able to describe an outcome variable with several predictors, not just to increase the fit of the model, but also to assess the individual contribution of each predictor, while controlling for the others. ", "video": ["Multiple regression - Model", "4.01 Regression model", "4.02 R and R-squared", "Multiple regression - Tests", "4.03 Overall test", "4.04 Individual tests", "4.05 Checking assumptions", "Multiple regression - Categorical predictors, categorical response variable and example", "4.06 Categorical predictors", "4.07 Categorical response variable", "4.08 Interpreting results", "Multiple regression - Transcripts", "Multiple regression", "R lab - Multiple regression"]}, {"title": "Analysis of variance", "description": "In this module we'll discuss analysis of variance, a very popular technique that allows us to compare more than two groups on a quantitative dependent variable. The reason we call it analysis of variance is because we compare two estimates of the variance in the population. If the group means differ in the population then these variance estimates differ. Just like in multiple regression, factorial analysis of variance allows us to investigate the influence of several independent variables.", "video": ["Analysis of variance - Basics and one-way ANOVA", "5.01 One-way ANOVA", "5.02 One-way ANOVA - Assumptions and F-test", "5.03 One-way ANOVA - Post-hoc t-tests", "Analysis of variance - Factorial ANOVA and regression", "5.04 Factorial ANOVA", "5.05  Factorial ANOVA - Assumptions and tests", "5.06 ANOVA and regression", "Analysis of variance - Transcripts", "Analysis of variance", "R lab - Analysis of variance"]}, {"title": "Non-parametric tests", "description": "In this module we'll discuss the last topic of this course: Non-parametric tests. Until now we've mostly considered tests that require assumptions about the shape of the distribution (z-tests, t-tests and F-tests). Sometimes those assumptions don't hold. Non-parametric tests require fewer of those assumptions. There are several non-parametric tests that correspond to the parametric z-, t- and F-tests. These tests also come in handy when the response variable is an ordered categorical variable as opposed to a quantitative variable. There are also non-parametric equivalents to the correlation coefficient and some tests that have no parametric-counterparts.", "video": ["Non-parametric tests - The basics", "6.01 Non-parametric tests - Why and when", "6.02 The sign test", "Non-parametric tests - Comparing groups with respect to mean rank", "6.03 One sample - Wilcoxon signed rank test", "6.04 Two samples - Wilcoxon/Mann-Whitney test", "6.05 Several samples - Kruskal-Wallis test", "a note about test-names", "Non-parametric tests - Rank-based correlation & randomness", "6.06 Spearman correlation", "6.07 The runs test", "Non-parametric tests - Transcripts", "Non-parametric tests", "R lab - Non-parametric tests"]}, {"title": "Exam time!", "description": "In this final module there's no new material to study. We advise you to take some extra time to review the material from the previous modules and to practice for the final exam. We've provided a practice exam that you can take as many times as you like. The final exam is structured exactly like the practice exam, so you know what to expect. Please note that you can only take the final exam twice every seven days, so make sure you are fully prepared. Please follow the honor code and do not communicate or confer with others while taking this exam or after. In the open questions of the exam (i.e. those that are not multiple choice) you should report your answers to 3 decimal places, and use 5 decimal places in your calculations. Good luck!", "video": ["Practice exam", "Final exam"]}]}, {"title": "Statistical Reasoning for Public Health 2: Regression Methods", "course_info": "About this course: A practical and example filled tour of simple and multiple regression techniques (linear, logistic, and Cox PH) for estimation, adjustment and prediction.", "target_audience": null, "created_by": "Johns Hopkins University", "teach_by": [{"name": "John McGready, PhD, MS", "department": "Bloomberg School of Public Health"}], "package_num": null, "package_name": null, "level": null, "rating": "4.7", "week_data": [{"title": "Introduction and Module 1A: Simple Regression Methods", "description": "In this module, a unified structure for simple regression models will be presented, followed by detailed treatises and examples of both simple linear and logistic models.", "video": ["Welcome to Statistical Reasoning for Public Health 2", "Syllabus", "Learning Objectives, Lecture 1", "Lecture 1a: Simple Regression: An Overview", "Lecture 1b: Simple Linear Regression with a Binary (or Nominal Categorical) Predictor ", "Lecture 1c: Simple Linear Regression with a Continuous Predictor ", "Lecture 1d: Simple Linear Regression Model: Estimating the Regression Equation—Accounting for Uncertainty in the Estimates ", "Lecture 1e: Measuring the Strength of a Linear Association ", "Learning Objectives, Lecture 2", "Lecture 2 Introduction: Simple Logistic Regression", "Lecture 2a: Simple Logistic Regression with a Binary (or Categorical) Predictor ", "Lecture 2b: Simple Logistic Regression with a Continuous Predictor ", "Lecture 2c: Simple Logistic Regression: Accounting for Uncertainty in the Estimates ", "Lecture 2d: Estimating Risk and Functions of Risk from Logistic Regression Results "]}, {"title": "Module 1B: More Simple Regression Methods", "description": "In this model, more detail is given regarding Cox regression, and it's similarities and differences from the other two regression models from module 1A.  The basic structure of the model is detailed, as well as its assumptions, and multiple examples are presented.", "video": ["Learning Objectives, Lecture 3", "Lecture 3 Introduction: Simple Cox (Proportional Hazards) Regression", "Lecture 3a: Simple Cox Regression: The Concept of Proportional Hazards ", "Lecture 3b: Simple Cox Regression with Binary or Categorical Predictors ", "Lecture 3d: Accounting for Uncertainty in Slope Estimate and Translating Cox Regression Results to Predicted Survival Curves ", "Lecture 3c: Simple Cox Regression with a Continuous Predictor ", "Supporting Information for Homework 1", "Homework 1A", "Homework 1B", "Homework 1C", "Homework 1D", "Homework 1E", "Homework 1F", "Homework 1G", "Quiz 1 Solutions", "Module 1 Quiz: Covers Lectures 1-3"]}, {"title": "Module 2A: Confounding and Effect Modification (Interaction)", "description": "This module, along with module 2B introduces two key concepts in statistics/epidemiology, confounding and effect modification.  A relation between an outcome and exposure of interested can be confounded if a another variable (or variables) is associated with both the outcome and the exposure.  In such cases the crude outcome/exposure associate may over or under-estimate the association of interest. Confounding is an ever-present threat in non-randomized studies, but results of interest can be adjusted for potential confounders.  ", "video": ["Learning Objectives, Lecture 4", "Lecture 4 Introduction: Confounding", "Lecture 4a: Confounding: A Formal Definition and Some Examples ", "Lecture 4b: Adjusted Estimates: Presentation, Interpretation, and Utility for Assessing Confounding ", "Lecture 4c: Adjusted Estimates: The General Idea Behind the Computations "]}, {"title": "Module 2B: Effect Modification (Interaction", "description": "Effect modification (Interaction), unlike confounding,  is a phenomenon of \"nature\" and cannot be controlled by study design choice.  However, it can be investigated in a manner similar to that of confounding. This set of lectures will define and give examples of effect modification, and compare and contrast it with confounding.", "video": ["Learning Objectives, Lecture 5", "Lecture 5 Introduction: Effect Modification", "Lecture 5a: Effect Modification: Introduction with Some Examples ", "Lecture 5b: Effect Modification: More Examples of Investigating Effect Modification ", "Lecture 5c: Confounding versus Effect Modification: A Review ", "Supporting Information for Homework 2", "Homework 2A", "Homework 2B", "Homework 2C", "Homework 2D", "Quiz 2 Solutions", "Module 2 Quiz: Covers Lectures 1-5"]}, {"title": "Module 3A: Multiple Regression Methods", "description": "This  module extends linear and logistic methods to allow for the inclusion of multiple predictors in a single regression model.", "video": ["Learning Objectives, Lecture 6", "Lecture 6a: An Overview of Multiple Regression for Estimation, Adjustment and Basic Prediction and Multiple Linear Regression", "Lecture 6b: Multiple Linear Regression: Some Examples ", "Lecture 6c: Multiple Linear Regression: Basics of Model Selection and Estimating Outcomes ", "Lecture 6d: Multiple Linear Regression: Some Examples from the Literature ", "Learning Objectives, Lecture 7", "Lecture 7 Introduction: Multiple Logistic Regression", "Lecture 7a: Multiple Logistic Regression: Some Examples ", "Lecture 7b: Basics of Model Selection and Estimating Outcomes ", "Lecture 7c: Some Examples from the Literature "]}, {"title": "Module 3B: More Multiple Regression Methods", "description": "This  set of lectures extends the techniques debuted in lecture set 3 to allow for multiple predictors of a time-to-event outcome using a single, multivariable regression model.", "video": ["Learning Objectives, Lecture 8", "Lecture 8 Introduction: Multiple Cox Regression", "Lecture 8a: Multiple Cox PH Regression: Some Examples ", "Lecture 8b: Multiple Cox Regression: Basics of Model Selection and Estimating Outcomes ", "Lecture 8c: Multiple Cox Regression: Some Examples from the Literature ", "Learning Objectives, Lecture 9", "Lecture 9 Introduction: Investigating Effect Modification and Non-Linear Relationships with Multiple Regression", "Lecture 9a: Effect Modification and Non-Linear Associations: Regression Based Approaches ", "Lecture 9b: Examples of Interaction Terms from Published Research ", "Lecture 9c: Non-Linear Relationships with Continuous Predictors in Regression: The Spline Approach ", "Supporting Information for Homework 3", "Homework 3A", "Homework 3B", "Homework 3C", "Homework 3D", "Quiz 3 Solutions", "Module 3 Quiz: Covers Lectures 1-8"]}, {"title": "Module 4: Additional Topics in Regression", "description": "", "video": ["Lecture 10 Introduction:  Propensity Scores: Another Approach to Estimating Adjusted Associations", "Lecture 10a: Propensity Scores: Definition and Adjustment ", "Lecture 10b: More Examples of Propensity Score Adjustment ", "Lecture 10c: Propensity Score Matching ", "Supporting Information for Homework 4", "Homework 4A", "Homework 4B", "Homework 4C", "Quiz 4 Solutions", "Learning Objectives, Lecture 10", "Module 4 Quiz: Covers Lectures 1-10"]}]}, {"title": "Advanced Linear Models for Data Science 1: Least Squares", "course_info": "About this course: Welcome to the Advanced Linear Models for Data Science Class 1: Least Squares. This class is an introduction to least squares from a linear algebraic and mathematical perspective. Before beginning the class make sure that you have the following:\n\n- A basic understanding of linear algebra and multivariate calculus.\n- A basic understanding of statistics and regression models.\n- At least a little familiarity with proof based mathematics.\n- Basic knowledge of the R programming language.\n\nAfter taking this course, students will have a firm foundation in a linear algebraic treatment of regression modeling. This will greatly augment applied data scientists' general understanding of regression models.", "target_audience": "Who is this class for: This class is for students who already have had a class in regression modeling and are familiar with the area who would like to see a more advanced treatment of the topic. ", "created_by": "Johns Hopkins University", "teach_by": [{"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "package_num": null, "package_name": null, "level": "Advanced", "rating": "4.4", "week_data": [{"title": "Background", "description": "We cover some basic matrix algebra results that we will need throughout the class. This includes some basic vector derivatives. In addition, we cover some some basic uses of matrices to create summary statistics from data. This includes calculating and subtracting means from observations (centering) as well as calculating the variance.\n", "video": ["Introduction", "Welcome to the class", "Course textbook", "Grading", "In this module", "Matrix derivatives", "Coding example", "Centering by matrix multiplication", "Coding example", "Variance via matrix multiplication", "Coding example", "Background Quiz"]}, {"title": "One and two parameter regression", "description": "In this module, we cover the basics of regression through the origin and linear regression. Regression through the origin is an interesting case, as one can build up all of multivariate regression with it.", "video": ["Before you begin", "Regression through the origin", "Centering first", "Coding example", "Before you begin", "Connection with linear regression", "Coding example", "Fitted values and residuals", "One Parameter Regression Quiz"]}, {"title": "Linear regression", "description": "In this lecture, we focus on linear regression, the most standard technique for investigating unconfounded linear relationships. ", "video": ["Before you begin", "Least squares", "Coding example", "Prediction", "Coding example", "Residuals", "Coding example", "Generalizations", "Generalizations", "Generalizations example", "Linear Regression Quiz"]}, {"title": "General least squares", "description": "We now move on to general least squares where an arbitrary full rank design matrix is fit to a vector outcome.", "video": ["Before you begin", "Least squares", "Coding example", "Second derivation of least squares", "Projections", "Third derivation of least squares", "Coding example", "General Least Squares Quiz"]}, {"title": "Least squares examples", "description": "Here we give some canonical examples of linear models to relate them to techniques that you may already be using.", "video": ["Basic examples of design matrices and fits", "Group effects", "Change of parameterization", "ANCOVA", "Least Squares Examples Quiz"]}, {"title": "Bases and residuals", "description": "Here we give a very useful kind of linear model, that is decomposing a signal into a basis expansion.", "video": ["Bases, introduction", "Bases 2, Fourier", "Bases 3, SVDs", "Bases, coding example", "Introduction to residuals", "Partitioning variability", "Bases Quiz", "Residuals Quiz"]}]}, {"title": "Experimentation for Improvement", "course_info": "About this course: We are always using experiments to improve our lives, our community, and our work. Are you doing it efficiently? Or are you (incorrectly) changing one thing at a time and hoping for the best? \n\nIn this course, you will learn how to plan efficient experiments - testing with many variables. Our goal is to find the best results using only a few experiments. A key part of the course is how to optimize a system.\n\nWe use simple tools: starting with fast hand calculations, then we show how to use FREE software. \n\nThe course comes with slides, transcripts of all lectures, subtitles (English, Spanish and Portuguese; some Chinese and French), videos, audio files, source code, and a free textbook. You get to keep all of it, all freely downloadable.\n\nThis course is for anyone working in a company, or wanting to make changes to their life, their community, their neighbourhood. You don't need to be a statistician or scientist! There's something for everyone in here.\n⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯\nWhat have prior students said about this course?\n\n\"This definitely is one of the most fruitful courses I have participated at Coursera, considering the takeaways and implementations! And so far I finished 12 [courses].\"\n\n\"Excelente curso, flexible y con suficiente material didáctico fácilmente digerible y cómodo. No importa si se tiene pocas bases matemáticas o estadísticas, el curso proporciona casi toda explicación necesaria para un entendimiento alto.\"\n\n\"I wish I had enrolled in your course years ago -- it would have saved us a lot of time in optimizing experimental conditions.\" Jason Eriksen, 3 Jan 2017\n\n\"Interesting and developing both analytical and creative thinking. The lecturer took care to bring lots of real live examples which are fun to analyze.\" 20 February 2016.\n\n\"... love your style of presentation, and the examples you took from everyday life to explain things. It is very difficult to make such a mathematical course accessible and comprehensible to this wide a variety of people!\"\n⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯", "target_audience": null, "created_by": "McMaster University", "teach_by": [{"name": "Kevin Dunn", "department": "Chemical Engineering"}], "package_num": null, "package_name": null, "level": "Intermediate", "rating": "4.8", "week_data": [{"title": "Introduction", "description": "We perform experiments all the time, so let's learn some terminology that we will use throughout the course. We show plenty of examples, and see how to analyze an experiment. We end by pointing out: \"how not to run an experiment\".", "video": ["Promotional video for this course", "Materials for this section", "1A: Why experiments are so important", "1B: Some basic terminology", "1C: Analysis of your first experiment", "1D: How NOT to run an experiment", "Ungraded practice quiz 1", "Module 1 quiz"]}, {"title": "Analysis of experiments by hand", "description": "The focus is on manual calculations. Why? Because you have to understand the most basic building blocks of efficient experiments. We look at systems with 2 and 3 variables (factors). Don't worry; the computer will do the work in the next module.", "video": ["Materials for this section", "2A: Analysis of experiments in two factors by hand", "2B: Numeric predictions from two-factor experiments", "2C: Two-factor experiments with interactions", "2D: In-depth case study: analyzing a system with 3 factors by hand", "Ungraded practice quiz 2", "Enrichment: Made for you by Madeleine: an interview with Joy", "Module 2 quiz"]}, {"title": "Using computer software to analyze experiments", "description": "Now we use free software to do the work for us. You can even run the software through a website (without installing anything special). We look at systems with 2, 3 and 4 factors. Most importantly we focus on the software interpretation.", "video": ["Materials for this section", "3A: Setting up the least squares model for a 2 factor experiment", "3B: Solving the mathematical model for a 2 factor experiment using software", "3C: Using computer software for a 3 factor experiment", "3D: Case study: a 4-factor system using computer software", "Ungraded practice quiz 3", "Enrichment: Dr. Soo Chan Carusone talks about experiments in a medical context", "Module 3 quiz"]}, {"title": "Getting more information, with fewer experiments", "description": "This is where the course gets tough and rough, but real. The quiz at the end if a tough one, so take it several times to be sure you have mastered the material - that's all that matters - understanding. We want to do as few experiments as possible, while still learning the most we can. Feel free to skip to module 5, which is the crucial learning from the whole course. You can come back here later. In module 4 we show how to do *practical* experiments that practitioners use everyday. We learn about important safeguards to ensure that we are not mislead by Mother Nature.", "video": ["Materials for this section", "4A: The trade-offs when doing half-fraction factorials", "4B: The technical details behind half-fractions - math warning!", "4C: A case study with aliasing in a fractional factorial", "4D: All about disturbances, why we randomize, and what covariates are", "Ungraded practice quiz 4: [4A,B,C,D]", "4E: All about blocking", "4F: Introducing aliasing notation", "4G: Using aliasing notation to plan experiments", "4H: An example of an analyzing an experiment with aliasing", "Ungraded practice quiz [4E, 4F, 4G, 4H]", "Enrichment: My colleague, David, and his student Jeff, talk about water treatment experiments", "Module 4 quiz [4A to 4H]"]}, {"title": "Response surface methods (RSM) to optimize any system", "description": "This is the goal we've been working towards: how to optimize any system. We start gently. We optimize a system with 1 factor and we also show why optimizing one factor at a time is misleading. We spend several videos to show how to optimize a system with 2 variables.", "video": ["Materials for this section", "5A: Response surface methods (RSM): an introduction", "5B: Response surface methods (RSM): one variable", "5C: Why changing one factor at a time (OFAT) will mislead you", "5D: The concept of contour plots and which objectives should we maximize", "Ungraded practice quiz [5A, 5B, 5C, 5D]", "5E: RSM in 2 factors: introducing the case study", "5F: RSM case study continues: constraints and mistakes", "5G: RSM case study continues: approaching the optimum", "Ungraded practice quiz [5E, 5F, 5G]", "Enrichment: An interview with Dr. Joe Kim (McMaster University)", "Module 5 quiz [5A, 5B, 5C, 5D]", "Module 5 quiz [5E, 5F, 5G]"]}, {"title": "Wrap-up and future directions", "description": "We close up the course and point out the next steps you might follow to extend what you have learned here.", "video": ["Materials for this section", "6: The big picture (wrapping it up, and other topics)", "Final survey: your feedback and comments"]}]}, {"title": "Regression Modeling in Practice", "course_info": "About this course: This course focuses on one of the most important tools in your data analysis arsenal: regression analysis. Using either SAS or Python, you will begin with linear regression and then learn how to adapt when two variables do not present a clear linear relationship. You will examine multiple predictors of your outcome and be able to identify confounding variables, which can tell a more compelling story about your results. You will learn the assumptions underlying regression analysis, how to interpret regression coefficients, and how to use regression diagnostic plots and other tools to evaluate the quality of your regression model. Throughout the course, you will share with others the regression models you have developed and the stories they tell you.", "target_audience": null, "created_by": "Wesleyan University", "teach_by": [{"name": "Jen Rose", "department": "Psychology"}, {"name": "Lisa Dierker", "department": "Psychology"}], "package_num": "3", "package_name": "Data Analysis and Interpretation Specialization ", "level": null, "rating": "4.3", "week_data": [{"title": "Introduction to Regression", "description": "This session starts where the Data Analysis Tools course left off. This first set of videos provides you with some conceptual background about the major types of data you may work with, which will increase your competence in choosing the statistical analysis that’s most appropriate given the structure of your data, and in understanding the limitations of your data set. We also introduce you to the concept of confounding variables, which are variables that may be the reason for the association between your explanatory and response variable. Finally, you will gain experience in describing your data by writing about your sample, the study data collection procedures, and your measures and data management steps. ", "video": ["Some Guidance for Learners New to the Specialization", "Lesson 1: Observational Data", "Lesson 2: Experimental Data", "Lesson 3: Confounding Variables", "Lesson 4: Introduction to Multivariate Methods", "Getting Set up for Assignments", "Tumblr Instructions", "How to Write About Data", "Writing About Your Data: Example Assignment", "Writing About Your Data"]}, {"title": "Basics of Linear Regression", "description": "In this session, we discuss more about the importance of testing for confounding, and provide examples of situations in which a confounding variable can explain the association between an explanatory and response variable. In addition, now that you have statistically tested the association between an explanatory variable and your response variable, you will test and interpret this association using basic linear regression analysis for a quantitative response variable. You will also learn about how the linear regression model can be used to predict your observed response variable. Finally, we will also discuss the statistical assumptions underlying the linear regression model, and show you some best practices for coding your explanatory variables\nNote that if your research question does not include one quantitative response variable, you can use one from your data set just to get some practice with the tool. \n", "video": ["SAS or Python - Which to Choose?", "Getting Started with SAS", "Getting Started with Python", "Course Codebooks", "Course Data Sets", "Uploading Your Own Data to SAS", "SAS Program Code for Video Examples", "SAS Lesson 1: More on Confounding Variables", "SAS Lesson 2: Testing a Basic Linear Regression Mode", "SAS Lesson 3: Categorical Explanatory Variables", "Python Program Code for Video Examples", "Python Lesson 1: More on Confounding Variables", "Python Lesson 2: Testing a Basic Linear Regression Model", "Python Lesson 3: Categorical Explanatory Variables", "Lesson 4: Linear Regression Assumptions", "Outlier Decision Tree", "Lesson 5: Centering Explanatory Variables", "Test a Basic Linear Regression Model"]}, {"title": "Multiple Regression", "description": "Multiple regression analysis is tool that allows you to expand on your research question, and conduct a more rigorous test of the association between your explanatory and response variable by adding additional quantitative and/or categorical explanatory variables to your linear regression model. In this session, you will apply and interpret a multiple regression analysis for a quantitative response variable, and will learn how to use confidence intervals to take into account error in estimating a population parameter. You will also learn how to account for nonlinear associations in a linear regression model. Finally, you will develop experience using regression diagnostic techniques to evaluate how well your multiple regression model predicts your observed response variable. \nNote that if you have not yet identified additional explanatory variables, you should choose at least one additional explanatory variable from your data set. When you go back to your codebooks, ask yourself a few questions like “What other variables might explain the association between my explanatory and response variable?”; “What other variables might explain more of the variability in my response variable?”, or even “What other explanatory variables might be interesting to explore?” Additional explanatory variables can be either quantitative, categorical, or both. Although you need only two explanatory variables to test a multiple regression model, we encourage you to identify more than one additional explanatory variable. Doing so will really allow you to experience the power of multiple regression analysis, and will increase your confidence in your ability to test and interpret more complex regression models. If your research question does not include one quantitative response variable, you can use the same quantitative response variable that you used in Module 2, or you may choose another one from your data set. ", "video": ["SAS Program Code for Video Examples", "SAS Lesson 1: Multiple Regression", "SAS Lesson 2: Confidence Intervals", "SAS Lesson 3: Polynomial Regression", "SAS Lesson 4: Evaluating Model Fit, pt. 1", "SAS Lesson 5: Evaluating Model Fit, pt. 2", "Python Program Code for Video Examples", "Python Lesson 1: Multiple Regression", "Python Lesson 2: Confidence Intervals", "Python Lesson 3: Polynomial Regression", "Python Lesson 4: Evaluating Model Fit, pt. 1", "Python Lesson 5: Evaluating Model Fit, pt. 2", "Test a Multiple Regression Model "]}, {"title": "Logistic Regression", "description": "In this session, we will discuss some things that you should keep in mind as you continue to use data analysis in the future. We will also teach also you how to test a categorical explanatory variable with more than two categories in a multiple regression analysis. Finally, we introduce you to logistic regression analysis for a binary response variable with multiple explanatory variables. Logistic regression is simply another form of the linear regression model, so the basic idea is the same as a multiple regression analysis. But, unlike the multiple regression model, the logistic regression model is designed to test binary response variables. You will gain experience testing and interpreting a logistic regression model, including using odds ratios and confidence intervals to determine the magnitude of the association between your explanatory variables and response variable.   \nYou can use the same explanatory variables that you used to test your multiple regression model with a quantitative outcome, but your response variable needs to be binary (categorical with 2 categories). If you have a quantitative response variable, you will have to bin it into 2 categories. Alternatively, you can choose a different binary response variable from your data set that you can use to test a logistic regression model. If you have a categorical response variable with more than two categories, you will need to collapse it into two categories.\n", "video": ["SAS Program Code for Video Examples", "SAS Lesson 1: Categorical Explanatory Variables with More Than Two Categories", "Python Program Code for Video Examples", "Python Lesson 1: Categorical Explanatory Variables with More Than Two Categories", "Lesson 2: A Few Things to Keep in Mind", "SAS Lesson 3: Logistic Regression for a Binary Response Variable, pt 1", "SAS Lesson 4: Logistic Regression for a Binary Response Variable, pt. 2", "Python Lesson 3: Logistic Regression for a Binary Response Variable, pt. 1", "Python Lesson 4: Logistic Regression for a Binary Response Variable, pt. 2", "Week 1 Video Credits", "Week 2 Video Credits", "Week 3 Video Credits", "Week 4 Video Credits", "Test a Logistic Regression Model"]}]}, {"title": "Advanced Linear Models for Data Science 2: Statistical Linear Models", "course_info": "About this course: Welcome to the Advanced Linear Models for Data Science Class 2: Statistical Linear Models. This class is an introduction to least squares from a linear algebraic and mathematical perspective. Before beginning the class make sure that you have the following:\n\n- A basic understanding of linear algebra and multivariate calculus.\n- A basic understanding of statistics and regression models.\n- At least a little familiarity with proof based mathematics.\n- Basic knowledge of the R programming language.\n\nAfter taking this course, students will have a firm foundation in a linear algebraic treatment of regression modeling. This will greatly augment applied data scientists' general understanding of regression models.", "target_audience": "Who is this class for: This class is for students who already have had a class in regression modeling and are familiar with the area who would like to see a more advanced treatment of the topic. ", "created_by": "Johns Hopkins University", "teach_by": [{"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "package_num": null, "package_name": null, "level": "Advanced", "rating": "4.6", "week_data": [{"title": "Introduction and expected values", "description": "In this module, we cover the basics of the course as well as the prerequisites. We then cover the basics of expected values for multivariate vectors. We conclude with the moment properties of the ordinary least squares estimates. ", "video": ["Introductory video", "Welcome to the class", "Course textbook", "Introduction to expected values", "Multivariate expected values, the basics", "Expected values, matrix operations", "Multivariate variances and covariances", "Multivariate covariance and variance matrix operations", "Expected values of quadratic forms", "Expected value properties of least squares estimates", "Expected Values"]}, {"title": "The multivariate normal distribution", "description": "In this module, we build up the multivariate and singular normal distribution by starting with iid normals.", "video": ["Introduction to the multivariate normal", "Normals and multivariate normals", "The singular normal distribution", "Normal likelihoods", "Normal conditional distributions", "A note on the last quiz question.", "the multivariate normal"]}, {"title": "Distributional results", "description": "In this module, we build the basic distributional results that we see in multivariable regression.", "video": ["Distributional results", "Chi squared results for quadratic forms", "Confidence intervals for regression coefficients", "F distribution", "Coding example", "Prediction intervals", "Coding example", "Confidence ellipsoids", "Coding example", "Distributional results"]}, {"title": "Residuals", "description": "In this module we will revisit residuals and consider their distributional results. We also consider the so-called PRESS residuals and show how they can be calculated without re-fitting the model.", "video": ["Residuals", "Residuals distributional results", "Code demonstration", "Leave one out residuals", "Press residuals", "Thanks for taking the course", "Residuals"]}]}, {"title": "Statistics for Genomic Data Science", "course_info": "About this course: An introduction to the statistics behind the most popular genomic data science projects. This is the sixth course in the Genomic Big Data Science Specialization from Johns Hopkins University.", "target_audience": null, "created_by": "Johns Hopkins University", "teach_by": [{"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}], "package_num": "7", "package_name": "Genomic Data Science Specialization ", "level": null, "rating": "4.1", "week_data": [{"title": "Module 1", "description": "This course is structured to hit the key conceptual ideas of normalization, exploratory analysis, linear modeling, testing, and multiple testing that arise over and over in genomic studies. ", "video": ["Welcome to Statistics for Genomic Data Science", "Syllabus", "Pre Course Survey", "Introduction and Materials", "What is Statistics?", "Finding Statistics You Can Trust (4:44)", "Getting Help (3:44)", "What is Data? (4:28)", "Representing Data (5:23)", "Module 1 Overview (1:07)", "Reproducible Research (3:42)", "Achieving Reproducible Research (5:02)", "R Markdown (6:26)", "The Three Tables in Genomics (2:10)", "The Three Tables in Genomics (in R)\t(3:46)", "Experimental Design: Variability, Replication, and Power (14:17)", "Experimental Design: Confounding and Randomization (9:26)", "Exploratory Analysis (9:21)", "Exploratory Analysis in R Part I (7:22)", "Exploratory Analysis in R Part II (10:07)", "Exploratory Analysis in R Part III (7:26)", "Data Transforms (7:31)", "Clustering (8:43)", "Clustering in R (9:09)", "Module 1 Quiz"]}, {"title": "Module 2", "description": "This week we will cover preprocessing, linear modeling, and batch effects.", "video": ["Module 2 Overview (1:12)", "Dimension Reduction (12:13)", "Dimension Reduction (in R)\t(8:48)", "Pre-processing and Normalization (11:26)", "Quantile Normalization (in R) (4:49)", "The Linear Model (6:50)", "Linear Models with Categorical Covariates (4:08)", "Adjusting for Covariates (4:16)", "Linear Regression in R\t(13:03)", "Many Regressions at Once (3:50)", "Many Regressions in R\t(7:21)", "Batch Effects and Confounders (7:11)", "Batch Effects in R: Part A (8:18)", "Batch Effects in R: Part B (3:50)", "Module 2 Quiz"]}, {"title": "Module 3", "description": "This week we will cover modeling non-continuous outcomes (like binary or count data), hypothesis testing, and multiple hypothesis testing.", "video": ["Module 3 Overview (1:07)", "Logistic Regression (7:03)", "Regression for Counts\t(5:02)", "GLMs in R (9:28)", "Inference (4:18)", "Null and Alternative Hypotheses\t(4:45)", "Calculating Statistics (5:11)", "Comparing Models (7:08)", "Calculating Statistics in R", "Permutation (3:26)", "Permutation in R (3:33)", "P-values\t(6:04)", "Multiple Testing (8:25)", "P-values and Multiple Testing in R: Part A\t(5:58)", "P-values and Multiple Testing in R: Part B (4:23)", "Module 3 Quiz"]}, {"title": "Module 4", "description": "In this week we will cover a lot of the general pipelines people use to analyze specific data types like RNA-seq, GWAS, ChIP-Seq, and DNA Methylation studies. ", "video": ["Module 4 Overview (1:21)", "Gene Set Enrichment\t(4:19)", "More Enrichment (3:59)", "Gene Set Analysis in R\t(7:43)", "The Process for RNA-seq (3:59)", "The Process for Chip-Seq (5:25)", "The Process for DNA Methylation (5:03)", "The Process for GWAS/WGS (6:12)", "Combining Data Types (eQTL) (6:04)", "eQTL in R (10:36)", "Researcher Degrees of Freedom\t (5:49)", "Inference vs. Prediction (8:52)", "Knowing When to Get Help\t(2:31)", "Statistics for Genomic Data Science Wrap-Up (1:53)", "Post Course Survey", "Module 4 Quiz"]}]}, {"title": "Measuring Causal Effects in the Social Sciences", "course_info": "About this course: How can we know if the differences in wages between men and women are caused by discrimination or differences in background characteristics? In this PhD-level course we look at causal effects as opposed to spurious relationships. We will discuss how they can be identified in the social sciences using quantitative data, and describe how this can help us understand social mechanisms.", "target_audience": null, "created_by": "University of Copenhagen", "teach_by": [{"name": "Anders Holm", "department": "Department of Sociology"}], "package_num": null, "package_name": null, "level": null, "rating": "4.0", "week_data": [{"title": "The Nature of Causal Effects and How to Measure Them", "description": "Welcome to the first week of the course! Thıs week we are looking at the nature of causal effects and how to measure them. ", "video": ["Course information", "Lecture 1 - The Nature of Causal Effects and How to Measure Them ", "Module 1 Lecture Quiz", "Module 1 Case Quiz"]}, {"title": "The Multivariate Regression Model and Mediating Factors", "description": "This second module introduces you multivariate regression model and the concept of mediating factors.", "video": ["Lecture 2 - The Multivariate Regression Model and Mediating Factors ", "Module 2 Lecture Quiz", "Module 2 Case Quiz"]}, {"title": "Randomized Controlled Trials", "description": "In this third week of the course we are having a closer look at causality and the randomzied controlled trial.", "video": ["Lecture 3 - Randomized Controlled Trials ", "Module 3 Lecture Quiz", "Module 3 Case Quiz"]}, {"title": "Instrumental Variables", "description": "The fourth week of the course we will go through the concept of instrumental variables.", "video": ["Lecture 4 - Instrumental Variables", "Module 4 Lecture Quiz", "Module 4a Case Quiz", "Module 4b Case Quiz"]}, {"title": "Difference in Difference", "description": "The final module of the course deals with the difference in difference. We hope you enjoyed the course and have learned something that you can use in your future work and research. ", "video": ["Lecture 5 - Difference in Difference", "Module 5 Lecture Quiz", "Module 5 Case Cuiz"]}]}, {"title": "Data Science Ethics", "course_info": "About this course: What are the ethical considerations regarding the privacy and control of consumer information and big data, especially in the aftermath of recent large-scale data breaches?\n\nThis course provides a framework to analyze these concerns as you examine the ethical and privacy implications of collecting and managing big data. Explore the broader impact of the data science field on modern society and the principles of fairness, accountability and transparency as you gain a deeper understanding of the importance of a shared set of ethical values. You will examine the need for voluntary disclosure when leveraging metadata to inform basic algorithms and/or complex artificial intelligence systems while also learning best practices for responsible data management, understanding the significance of the Fair Information Practices Principles Act and the laws concerning the \"right to be forgotten.\"\n\nThis course will help you answer questions such as who owns data, how do we value privacy, how to receive informed consent and what it means to be fair.\n\nData scientists and anyone beginning to use or expand their use of data will benefit from this course. No particular previous knowledge needed.", "target_audience": null, "created_by": "University of Michigan", "teach_by": [{"name": "H.V. Jagadish", "department": "Electrical Engineering and Computer Science"}], "package_num": null, "package_name": null, "level": "Beginner", "rating": null, "week_data": [{"title": "What are Ethics?", "description": "Module 1 of this course establishes a basic foundation in the notion of simple utilitarian ethics we use for this course. The lecture material and the quiz questions are designed to get most people to come to an agreement about right and wrong, using the utilitarian framework taught here. If you bring your own moral sense to bear, or think hard about possible counter-arguments, it is likely that you can arrive at a different conclusion. But that discussion is not what this course is about. So resist that temptation, so that we can jointly lay a common foundation for the rest of this course.", "video": ["Course Syllabus", "Welcome Announcement", "Help us learn more about you!", "What are Ethics? - Introduction", "Data Science Ethics - Course Preview", "What are Ethics?", "Data Science Needs Ethics", "Case Study: Spam (not the meat)", "Module 1 Discussion", "Module 1 Quiz"]}, {"title": "History, Concept of Informed Consent", "description": "Early experiments on human subjects were by scientists intent on advancing medicine, to the benefit of all humanity, disregard for welfare of individual human subjects. Often these were performed by white scientists, on black subject. In this module we will talk about the laws that govern the Principle of Informed Consent. We will also discuss why informed consent doesn’t work well for retrospective studies, or for the customers of electronic businesses.", "video": ["Human Subjects Research and Informed Consent: Part 1", "Human Subjects Research and Informed Consent: Part 2", "Limitations of Informed Consent", "Case Study: It's Not OKCupid", "Module 2 Discussion", "Module 2 Quiz"]}, {"title": "Data Ownership", "description": "Who owns data about you? We'll explore that question in this module. A few examples of personal data include copyrights for biographies; ownership of photos posted online, Yelp, Trip Advisor, public data capture, and data sale. We'll also explore the limits on recording and use of data. ", "video": ["Data Ownership", "Limits on Recording and Use", "Data Ownership Finale", "Case Study: Rate My Professor", "Module 3 Discussion", "Module 3 Quiz"]}, {"title": "Privacy", "description": "Privacy is a basic human need. Privacy means the ability to control information about yourself, not necessarily the ability to hide things. We have seen the rise different value systems with regards to privacy. Kids today are more likely to share personal information on social media, for example. So while values are changing, this doesn’t remove the fundamental need to be able to control personal information. In this module we'll examine the relationship between the services we are provided and the data we provide in exchange: for example, the location for a cell phone. We'll also compare and contrast \"data\" against \"metadata\".", "video": ["Privacy - Introduction", "Privacy", "History of Privacy", "Degrees of Privacy", "Modern Privacy Risks", "Case Study: Targeted Ads", "Case Study: The Naked Mile", "Case Study: Sneaky Mobile Apps", "Module 4 Discussion", "Module 4 Discussion Prompt References", "Module 4 Quiz"]}, {"title": "Anonymity", "description": "Certain transactions can be performed anonymously. But many cannot, including where there is physical delivery of product. Two examples related to anonymous transactions we'll look at are \"block chains\" and \"bitcoin\". We'll also look at some of the drawbacks that come with anonymity.", "video": ["Anonymity", "De-identification Has Limited Value: Part 1", "De-identification Has Limited Value: Part 2", "Case Study: Credit Card Statements", "Module 5 Discussion", "Module 5 Quiz"]}, {"title": "Data Validity", "description": "Data validity is not a new concern. All too often, we see the inappropriate use of Data Science methods leading to erroneous conclusions. This module points out common errors, in language suited for a student with limited exposure to statistics. We'll focus on the notion of representative sample: opinionated customers, for example, are not necessarily representative of all customers.", "video": ["Data Validity - Introduction", "Validity", "Choice of Attributes and Measures", "Errors in Data Processing", "Errors in Model Design", "Managing Change", "Case Study: Three Blind Mice", "Case Study: Algorithms and Race", "Case Study: Algorithms in the Office", "Case Study: GermanWings Crash", "Case Study: Google Flu", "Module 6 Discussion", "Module 6 Quiz"]}, {"title": "Algorithmic Fairness", "description": "What could be fairer than a data-driven analysis? Surely the dumb computer cannot harbor prejudice or stereotypes. While indeed the analysis technique may be completely neutral, given the assumptions, the model, the training data, and so forth, all of these boundary conditions are set by humans, who may reflect their biases in the analysis result, possibly without even intending to do so. Only recently have people begun to think about how algorithmic decisions can be unfair. Consider this article, published in the New York Times. This module discusses this cutting edge issue.", "video": ["Algorithmic Fairness - Introduction", "Algorithmic Fairness", "Correct But Misleading Results", "P Hacking", "Case Study: High Throughput Biology", "Case Study: Geopricing", "Case Study: Your Safety Is My Lost Income", "Module 7 Discussion", "Module 7 Quiz"]}, {"title": "Societal Consequences", "description": "In Module 8, we consider societal consequences of Data Science that we should be concerned about even if there are no issues with fairness, validity, anonymity, privacy, ownership or human subjects research. These “systemic” concerns are often the hardest to address, yet just as important as other issues discussed before. For example, we consider ossification, or the tendency of algorithmic methods to learn and codify the current state of the world and thereby make it harder to change. Information asymmetry has long been exploited for the advantage of some, to the disadvantage of others. Information technology makes spread of information easier, and hence generally decreases asymmetry. However, Big Data sets and sophisticated analyses increase asymmetry in favor of those with ability to acquire/access. ", "video": ["Societal Consequences - Introduction", "Societal Impact", "Ossification", "Surveillance", "Case Study: Social Credit Scores", "Case Study: Predictive Policing", "Module 8 Discussion", "Module 8 Quiz"]}, {"title": "Code of Ethics", "description": "Finally, in Module 9, we tie all the issues we have considered together into a simple, two-point code of ethics for the practitioner.", "video": ["Code of Ethics", "Wrap Up", "Case Study: Algorithms and Facial Recognition", "Post-Course Survey", "Module 9 Quiz", "Data Ethics Case Study"]}, {"title": "Attributions", "description": "This module contains lists of attributions for the external audio-visual resources used throughout the course.", "video": ["Week 1 Attributions", "Week 2 Attributions", "Week 3 Attributions", "Week 4 Attributions"]}]}, {"title": "Mathematical Biostatistics Boot Camp 2", "course_info": "About this course: Learn fundamental concepts in data analysis and statistical inference, focusing on one and two independent samples.", "target_audience": null, "created_by": "Johns Hopkins University", "teach_by": [{"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "package_num": null, "package_name": null, "level": null, "rating": "4.1", "week_data": [{"title": "Hypothesis Testing", "description": "In this module, you'll get an introduction to hypothesis testing, a core concept in statistics. We'll cover hypothesis testing for basic one and two group settings as well as power. After you've watched the videos and tried the homework, take a stab at the quiz.", "video": ["Syllabus", "Hypothesis Testing", "More Hypothesis Testing", "General Rules of Hypothesis Testing", "Two-sided Tests", "Confidence Intervals & P Values", "Power", "Calculating Power", "T Tests & Monte Carlo", "Two Sample Tests - Matched Data I", "Two Sample Tests - Matched Data II", "Two Sample Tests - Regression to the Mean", "Two Sample Tests - Two Independent Groups", "Module 1 Homework (Not counted toward final grade)", "Module 1 Quiz"]}, {"title": "Two Binomials", "description": "In this module we'll be covering some methods for looking at two binomials. This includes the odds ratio, relative risk and risk difference. We'll discussing mostly confidence intervals in this module and will develop the delta method, the tool used to create these confidence intervals. After you've watched the videos and tried the homework, take a crack at the quiz!", "video": ["Two Sample Binomial Tests - Score Statistic", "Two Sample Binomial Tests - Exact Tests", "Two Sample Binomial Tests - Comparing 2 Binomial Proportions", "Relative Risks & Odds Ratios - Relative Measures", "Relative Risks & Odds Ratios - The Relative Risk", "Relative Risks & Odds Ratios - The Odds Ratio", "Delta Method", "Delta Method & Derivation", "Module 2 Homework", "Module 2 Quiz"]}, {"title": "Discrete Data Settings", "description": "In this module, we'll discuss testing in discrete data settings. This includes the famous Fisher's exact test, as well as the many forms of tests for contingency table data. You'll learn the famous observed minus expected squared over the expected formula, that is broadly applicable. ", "video": ["Fisher's Exact Test", "Hyper-Geometric Distribution", "Fisher's Exact Text in Practice & Monte Carlo", "Chi Squared Testing", "Testing Independence", "Generalization", "Goodness of Fit Testing", "Module 3 Homework", "Module 3 Quiz"]}, {"title": "Techniques", "description": "This module is a bit of a hodge podge of important techniques. It includes methods for discrete matched pairs data as well as some classical non-parametric methods.", "video": ["Simpson's Paradox", "Simpson's Paradox, more examples", "Weighting", "CMH test", "Case Control Sampling", "Exact inference for The Odds Ratio", "Matched 2x2 Tables", "Dependence and Marginal Homogeneity", "Estimation of the Marginal Difference in Proportions", "Odds and Ends for Matched 2x2 Tables", "the sign test", "the sign rank test", "the rank sum test", "Poisson distribution", "Poisson likelihood", "Poisson P-value calculation", "Module 4 Homework", "Module 4 Quiz"]}]}, {"title": "Exploring and Producing Data for Business Decision Making", "course_info": "About this course: This course provides an analytical framework to help you evaluate key problems in a structured fashion and will equip you with tools to better manage the uncertainties that pervade and complicate business processes. Specifically, you will be introduced to statistics and how to summarize data and learn concepts of frequency, normal distribution, statistical studies, sampling, and confidence intervals.\n\nWhile you will be introduced to some of the science of what is being taught, the focus will be on applying the methodologies. This will be accomplished through the use of Excel and data sets from many different disciplines, allowing you to see the use of statistics in very diverse settings. The course will focus not only on explaining these concepts, but also understanding the meaning of the results obtained.\n\nUpon successful completion of this course, you will be able to:\n\n•\tSummarize large data sets in graphical, tabular, and numerical forms.\n•\tUnderstand the significance of proper sampling and why you can rely on sample information.\n•\tUnderstand why normal distribution can be used in so many settings.\n•\tUse sample information to infer about the population with a certain level of confidence about the accuracy of the estimations.\n•\tUse Excel for statistical analysis.\n\nThis course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.", "target_audience": null, "created_by": "University of Illinois at Urbana-Champaign", "teach_by": [{"name": "Fataneh Taghaboni-Dutta", "department": "University of Illinois, Urbana-Champaign College of Business Department of Business Administration"}], "package_num": "5", "package_name": "Managerial Economics and Business Analysis  Specialization ", "level": "Beginner", "rating": "4.7", "week_data": [{"title": "Course Orientation", "description": "You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course.", "video": ["Welcome to Exploring and Producing Data for Business Decision Making!", "Syllabus", "About the Discussion Forums", "Glossary", "Orientation Quiz", "Using Excel for this Course", "Excel Data Analysis Toolpak", "Updating Your Profile", "Getting to Know Your Classmates", "Social Media"]}, {"title": "Module 1: Introduction and Summarizing Data", "description": "Data is all around you, but what is the data telling you? The first step in making better decisions and taking action is to get a good understanding of information you have gathered. In this module we will learn about some of the tools in statistics that help us achieve this.", "video": ["Module 1 Overview", "Module 1 Readings", "1-1.1. Basic Terminology", "Lesson 1-1 Practice Quiz", "1-2.1. Summarizing Data: Frequency Tables", "1-2.2. Frequency Tables in Excel: Quantitative Data", "1-2.3. Frequency Tables in Excel: Qualitative Data", "1-2.4. Presenting Frequency Tables as a Bar Graph in Excel", "Lesson 1-2 Practice Quiz", "1-3.1. Summarizing Data: Histograms", "1-3.2. Histograms in Excel", "Lesson 1-3 Practice Quiz", "1-4.1. Summarizing Data: Pie Charts", "1-4.2. Pie Charts in Excel", "Lesson 1-4 Practice Quiz", "1-5.1. Summarizing Data: Scatter Plots", "1-5.2. Scatter Plots in Excel", "Lesson 1-5 Practice Quiz", "Module 1 Quiz"]}, {"title": "Module 2: Descriptive Statistics and Probability Distributions", "description": "We all have heard the phrase that a \"picture is worth a thousand words,\" but you certainly don’t want one of those to be \"what exactly am I looking at?\" So, now that you know to use \"pictures\" to summarize your data, let’s make those pictures easier to understand.", "video": ["Module 2 Overview", "Module 2 Readings", "2-1.1. Measures of Central Tendencies", "2-1.2. Mean and Median in Excel", "Lesson 2-1 Practice Quiz", "2-2.1. Measures of Dispersion", "2-2.2. Standard Deviation in Excel", "Lesson 2-2 Practice Quiz", "2-3.1. Percentiles and Z-Score", "2-3.2. Z-Score in Excel", "Lesson 2-3 Practice Quiz", "2-4.1. Discrete and Continuous Random Variables", "2-4.2. Expected Value in Excel", "Lesson 2-4 Practice Quiz", "2-5.1. Normal Distribution", "2-5.2. Normal Distribution in Excel", "2-5.3. Standard Normal Distribution in Excel", "2-5.4. \"Less Than\" in Excel", "2-5.5. \"Greater Than\" in Excel", "2-5.6. \"Between Values\" in Excel", "2-5.7. Finding Z for a Given Probability (INV) in Excel", "Lesson 2-5 Practice Quiz", "2-6.1. Standard Normal Distribution Table", "Lesson 2-6 Practice Quiz", "Module 2 Quiz"]}, {"title": "Module 3: Sampling and Central Limit Theorem", "description": "You are charged with analyzing a market segment for your company. You and your team have figured out what variables you need to understand; you also have an idea what factors might be influencing these variables of interest. Now you are ready to do your analysis. But, wait! Where is the data? How do you begin to get the data? In this module we will review the means by which you can begin to produce data – the concepts of sampling and Central Limit Theorem – and will help you understand how to produce \"good\" sample data and why sample data will work.", "video": ["Module 3 Overview", "Module 3 Readings", "3-1.1. Producing Data", "Lesson 3-1 Practice Quiz", "3-2.1. Sampling", "3-2.2 Sampling Function in Excel", "Lesson 3-2 Practice Quiz", "3-3.1. Central Limit Theorem and Sampling Means", "3-3.2. Animating the Central Limit Theorem", "3-3.3. Sampling Distribution and Empirical Rule in Excel", "Lesson 3-3 Practice Quiz", "3-4.1. Central Limit Theorem – Sampling Proportion", "Lesson 3-4 Practice Quiz", "Exploring and Producing Data Peer Review Assignment", "Module 3 Quiz"]}, {"title": "Module 4: Inference", "description": "You have sample data and have done the analysis – you think you can say something about the population based on your sample study.  But, do you have a sense of what are the chances of you being right or wrong?  How can you be surer? What else should you have considered? In this module, you will learn how to find the answers to these questions.", "video": ["Module 4 Overview", "Module 4 Readings", "4-1.1. Confidence Interval Basics", "Lesson 4-1 Practice Quiz", "4-2.1. Confidence Interval for Means", "4-2.2. Confience Interval for Mean in Excel", "4-2.3. Impact of Confidence Level Illustrated in Excel", "Lesson 4-2 Practice Quiz", "4-3.1. Confidence Interval for Population Proportion", "4-3.2. Confidence Interval for Population Proportion in Excel", "4-3.3. Confidence Interval Animation in Excel", "4-3.4. Starting Salary Example in Excel", "Lesson 4-3 Practice Quiz", "4-4.1. Sample Size", "4-4.2. Sample Size Proportion in Excel", "4-4.3. Sample Size Mean in Excel", "4-4.4. Sample Size Effect in Excel", "Lesson 4-4 Practice Quiz", "Module 4 Quiz"]}]}, {"title": "Science Literacy - How Solid Science Can Help You Save the World", "course_info": "About this course: This course will help you become scientifically literate so that you can make better choices for yourself and the world. Unlike other courses on statistics and scientific methods, we explore global challenges - such as poverty or climate change - and then discuss how key approaches of statistics and scientific methods can help tackle these challenges. We present these approaches in a non-mathematical and easily accessible way. You will leave the course being able to recognize which efforts to do good in this world actually work, and you will have used your science literacy to make some personal changes in your life. \n\nMany current attempts to do good in this world are based on good intentions, but don’t work well, or are even harmful. In this course we talk to leading experts from academia, business and non-profit organizations about how we can use science to distinguish bad, good and even better ways of improving this world. We also invite you to change your own behavior to do more good. You will learn how to spot BS (bad science) in the media, how to evaluate whether a social program works or not, and how your career could have a better impact on this world. Finally, you will develop your own plan on how you are going to do good better with science. \n\nGuest speakers include Behavioral Economist Dan Ariely, Philosopher Peter Singer, and Happiness researcher Elizabeth Dunn.", "target_audience": null, "created_by": "Erasmus University Rotterdam", "teach_by": [{"name": "Vera Schölmerich", "department": "Erasmus University College, Faculty of Social and Behavioural Sciences, Erasmus University"}, {"name": "Kellie Liket", "department": "Impact Centre Erasmus (ICE) - Erasmus School of Economics"}], "package_num": null, "package_name": null, "level": "Beginner", "rating": "4.2", "week_data": [{"title": "Science literacy as a vaccine against the charlatans", "description": "We kick off this course by providing some ammunition for what the astrophysicist Neil de Grasse Tyson said: ”Science literacy is a vaccine against the charlatans of the world that would exploit your ignorance.” Here, you will discover that you should be wary of the recommendations that you get from our governments on how to save water, from the media based on new scientific findings, and even from our doctor. At the same time, you will see that a basic understanding of statistics and scientific methods - which you will acquire throughout this course - will protect you against misinformation and bad science, and help you make better choices for yourself and for this world. At the end of this week you will create and upload a short 3-minute video pitch about global challenges and what your own role is in tackling them (if any). \n\nPlease note that Erasmus University Rotterdam pursues the science of learning. Online learners are important participants in that pursuit. The information we gather from your engagement with our instructional offerings makes it possible for faculty, researchers, and designers to continuously improve their work and, in that process, build learning science. By registering as an online learner, you are also participating in research. ", "video": ["90 second trailer", "Water we doing?!", "Eating no or less animals? ", "Cost-benefit analyses to answer the question: should I floss or not?", "Make your own cost-benefit analysis", "Check your cost-benefit analysis", "Optional video: What Morillio and his team are doing to improve access to clean water", "Optional documentary about costs and benefits of eating meat", "Dear doctor", "Is your life based on evidence? ", "Why Most Research Findings Are False", "Optional reading: Lies, Damned Lies, and Medical Science", "Optional video: Full-length interview with John Ioannidis", "Please take 3 minutes to tell us how things are going", "Ted Talk \"Photos that bear witness to modern day slavery\"", "Implanting a chip in your brain"]}, {"title": "Stop guessing!", "description": "\"Whenever you leave behind failure, that means you're doing better. If you think everything you've done has been great, you're probably dumb.\" – Louis CK. \n\nMany social programs - such as efforts to reduce poverty or improve people’s health - are based on good intentions but simply don’t work, or are even harmful. This week you will discover that humans are pretty bad at guessing which programs work, and that the best way to assess whether a program works is by running randomized controlled trials. You will see that conducting multiple rigorous evaluations is how we can leave behind failure and do better in this world. And not be dumb. At the end of this week you’ll be ready to put your new skills to use by critically assessing a BS (bad science) media report. ", "video": ["Week introduction", "A big fat problem", "What lifestyle changes would you like to make, and how?", "Video testimonial: hear how your fellow learners are doing", "Optional: Full-length interview with Dan Ariely", "Poverty, Bills & Cash", "Giving Game", "Charity overview", "Initial Charity Choice", "PlayPumps", "Reflect on PlayPumps", "Doing good", "Poverty, Bills & Cash Continued", "Charity Details", "Final Charity Choice", "About the Giving Game", "Giving well", "Challenge your charity", "Optional video: full-length interview with Paul Niehaus", "2 minute excerpt from the interview with Dan Ariely", "Dan Ariely's panda express experiment", "Micro Creditis", "Ted Talk: Why medicine often has dangerous side effects for women", "Is the medicine you drop OK for you?", "Pay it forward", "Our interviewees talk about being annoyed", "Recap Q & A", "Calling Bad Science (BS) "]}, {"title": "Putting it all together", "description": "", "video": ["Week Introduction", "Buying happiness", "Giving what you can?", "Fooled by correlation", "Changed your mind?", "Optional video: full length interview Elizabeth Dunn", "Optional video: full-length interview Peter Singer", "Measuring your own happiness - question 1", "Measuring your own happiness - question 2 & 3", "Measuring your own happiness - question 4", "Measuring your own happiness - question 5 - 8", "Measuring your own happiness - question 9", "Measuring your own happiness - question 10", "Main Take-aways", "Some final comments on happiness", "Optional video: Want to find out more about measurement validity?", "Thought Experiment", "Thought Experiment Conclusion", "In the best of wealth", "How rich are you?", "Arctic expedition", "Thinking rationally about charity", "Doing Good Better", "Will you make any changes to your current career?", "Optional video: full-length interview Robert Wiblin", "Optional: Beth Barnes on how to save the world", "Optional: Want to find out more about how organizations can calculate cost-effectiveness", "9 to 5", "Share your frustration with us ", "Imagine. You are almighty - what would you do?", "Share your personal #deceptiondetox", "What are your major take-aways?", "Optional: full-length interview Marcel Fafchamps", "Graded multiple choice quiz: measurement matters", "Doing Good Done Better "]}, {"title": "Yes but wait", "description": "", "video": ["Week 4 Introduction", "Existential Risk", "Existential Risk", "Innovating to zero", "Optional: Ted Talk Bill Gates", "Optional: Video of the Ocean Cleanup Project", "Optional: Full-length interview Karen Maas", "What's your carbon footprint?", "Other existential risks", "Expert risk assessment", "Existential Risk Conclusion", "Comparing potential for impact", "Podcast: How can we prioritize global challenges?", "Fishy finance", "We haven't got an answer for this one. You?", "Optional video: Full-length interview Helen Toxopeus", "Optional videos: mini tutorials by Dirk Bezemer", "Know this tune?"]}, {"title": "Capstone", "description": "You have watched many videos that we made on tackling global challenges with science. Now it’s time to create your own video on how you are going to tackle a global challenge with science. ", "video": ["Capstone project"]}]}, {"title": "Statistics with R Capstone", "course_info": "About this course: The capstone project will be an analysis using R that answers a specific scientific/business question provided by the course team. A large and complex dataset will be provided to learners and the analysis will require the application of a variety of methods and techniques introduced in the previous courses, including exploratory data analysis through data visualization and numerical summaries, statistical inference, and modeling as well as interpretations of these results in the context of the data and the research question. The analysis will implement both frequentist and Bayesian techniques and discuss in context of the data how these two approaches are similar and different, and what these differences mean for conclusions that can be drawn from the data.\n \nA sampling of the final projects will be featured on the Duke Statistical Science department website.\n\nNote: Only learners who have passed the four previous courses in the specialization are eligible to take the Capstone.", "target_audience": null, "created_by": "Duke University", "teach_by": [{"name": "Merlise A Clyde", "department": "Department of Statistical Science"}, {"name": "Colin Rundel ", "department": "Statistical Science"}, {"name": "David Banks", "department": "Statistical Science"}, {"name": "Mine Çetinkaya-Rundel", "department": "Department of Statistical Science"}], "package_num": "5", "package_name": "Statistics with R Specialization ", "level": null, "rating": "4.7", "week_data": [{"title": "About the Capstone Project", "description": "Welcome to the capstone project! This week's content is an introduction to the project assignment and goals. The readings in this week will introduce the data set that you will be analyzing for your project and the specific questions you will answer using data analysis techniques we learned in the previous courses. It is important to understand what we will be doing in the course before jumping into the detailed analysis. So we encourage you to start with the first lecture to get the big picture, and then delve into the specifics of the analysis. Enjoy, and good luck! Remember, if you have questions, you can post them on the discussion forums.", "video": ["Welcome to the Statistics with R Capstone course", "Introduction to the Capstone Course", "Tips for Success and Suggested Work Pace", "What to Do This Week", "Feedback surveys", "Learning Objectives for Courses 1-4"]}, {"title": "Exploratory Data Analysis (EDA)", "description": "This week you will work on conducting an exploratory analysis of the housing data. Exploratory analysis is an essential first step for familiarizing yourself with and understanding the data. \n\nIn this week, you will complete a quiz which will guide you through certain important aspects of the data. The insights you gain through this assignment will help inform modeling in the future quizzes and peer assessments. \n\nFeel free to post questions about this assignment on the discussion forum. ", "video": ["What to Do This Week", "EDA Quiz - Assignment Guide", "Feedback Survey", "EDA Quiz"]}, {"title": "EDA and Basic Model Selection - Submission", "description": "This week we will dig deeper into our exploratory data analysis of the data. We now have all the information and data necessary to perform a deep dive into the EDA and it is time start your initial analysis report! We encourage you to start your analysis report (presented in peer-review format next week) early so you will have enough time to complete it. You will conduct exploratory data analysis, model selection, and model evaluation, and then complete a written report which answers several questions which will guide you through the process. This report will be your first peer-review assignment in this course. ", "video": ["What to Do This Week"]}, {"title": "EDA and Basic Model Selection - Evaluation", "description": "Great work so far! We hope you will also learn as much from evaluating your peers' work as completing your own assignment. Happy learning!", "video": ["What to Do This Week", "Feedback Survey", "EDA and Basic Model Selection"]}, {"title": "Model Selection and Diagnostics", "description": "We are half way through the course! In this week, you will continue model selection and model diagnostics, which will serve a starting point for your final project. You will be assessed on your work through a quiz. If you have any questions so far, don't hesitate to post on the forum so that others can help and discuss the question together.", "video": ["What to Do This Week", "Model Selection and Diagnostics Quiz - Assignment Guide", "Feedback Survey", "Model Selection and Diagnostics Quiz"]}, {"title": "Out of Sample Prediction", "description": "In this week, you will gain experience using your model to perform out-of-sample prediction and validation.  The skills honed this week will guide you through your final analysis in the weeks to come.  Please feel free to go back to prior weeks and review the necessary background knowledge. ", "video": ["What do Do This Week", "Out of Sample Prediction Quiz - Assignment Guide", "Feedback Survey", "Out of Sample Prediction Quiz"]}, {"title": "Final Data Analysis - Submission", "description": "In the next two weeks, you will complete your final data analysis project. You will submit your answers using the Final Data Analysis peer review assignment link in Week 8.", "video": ["What to Do This Week"]}, {"title": "Final Data Analysis - Evaluation", "description": "Congratulations on making through to the final week of the course! In this week, we will finish this data analysis project by completing the evaluation of three of your peers' assignments. ", "video": ["What to Do This Week", "Feedback Survey", "Final Data Analysis "]}]}]