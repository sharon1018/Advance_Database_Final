[{"course_info": "About this course: This course will introduce the learner to the basics of the python programming environment, including how to download and install python, expected fundamental python programming techniques, and how to find help with python programming questions. The course will also introduce data manipulation and cleaning techniques using the popular python pandas data science library and introduce the abstraction of the DataFrame as the central data structure for data analysis. The course will end with a statistics primer, showing how various statistical measures can be applied to pandas DataFrames. By the end of the course, students will be able to take tabular data, clean it,  manipulate it, and run basic inferential statistical analyses.\n\nThis course should be taken before any of the other Applied Data Science with Python courses:  Applied Plotting, Charting & Data Representation in Python, Applied Machine Learning in Python, Applied Text Mining in Python, Applied Social Network Analysis in Python.", "level": "Intermediate", "package_name": "Applied Data Science with Python Specialization ", "created_by": "University of Michigan", "package_num": "1", "teach_by": [{"name": "Christopher Brooks", "department": null}], "target_audience": "Who is this class for: This course is part of “Applied Data Science with Python“ and is intended for learners who have basic python or programming background, and want to apply statistics, machine learning, information visualization, social network analysis, and text analysis techniques to gain new insight into data.\n\nOnly minimal statistics background is expected, and the first course contains a refresh of these basic concepts. There are no geographic restrictions. Learners with a formal training in Computer Science but without formal training in data science will still find the skills they acquire in these courses valuable in their studies and careers.", "rating": "4.5", "week_data": [{"description": "In this week you'll get an introduction to the field of data science, review common Python functionality and features which data scientists use, and be introduced to the Coursera Jupyter Notebook for the lectures. All of the course information on grading, prerequisites, and expectations are on the course syllabus, and you can find more information about the Jupyter Notebooks on our Course Resources page.", "video": ["Introduction to Specialization", "Syllabus", "Help us learn more about you!", "Data Science", "50 years of Data Science, David Donoho (optional)", "The Coursera Jupyter Notebook System", "Notice for Auditing Learners: Assignment Submission", "Week 1 Lectures Jupyter Notebook", "Python Functions", "Python Types and Sequences", "Python More on Strings", "Python Demonstration: Reading and Writing CSV files", "Python Dates and Times", "Advanced Python Objects, map()", "Advanced Python Lambda and List Comprehensions", "Advanced Python Demonstration: The Numerical Python Library (NumPy)", "Week One Quiz"], "title": "Week 1"}, {"description": "In this week of the course you'll learn the fundamentals of one of the most important toolkits Python has for data cleaning and processing -- pandas. You'll learn how to read in data into DataFrame structures, how to query these structures, and the details about such structures are indexed. The module ends with a programming assignment and a discussion question.", "video": ["Introduction", "Week 2 Lectures Jupyter Notebook", "The Series Data Structure", "Querying a Series", "The DataFrame Data Structure", "DataFrame Indexing and Loading", "Querying a DataFrame", "Indexing Dataframes", "Missing Values", "Hacked Data", "Assignment 2", "Assignment 2  Submission"], "title": "Week 2"}, {"description": "In this week you'll deepen your understanding of the python pandas library by learning how to merge DataFrames, generate summary tables, group data into logical pieces, and manipulate dates. We'll also refresh your understanding of scales of data, and discuss issues with creating metrics for analysis. The week ends with a more significant programming assignment.", "video": ["Week 3 Lectures Jupyter Notebook", "Merging Dataframes", "Pandas Idioms", "Group by", "Scales", "Pivot Tables", "Date Functionality", "Goodhart's Law", "Assignment 3", "Assignment 3 Submission"], "title": "Week 3"}, {"description": "In this week of the course you'll be introduced to a variety of statistical techniques such a distributions, sampling and t-tests. The majority of the week will be dedicated to your course project, where you'll engage in a real-world data cleaning activity and provide evidence for (or against!) a given hypothesis. This project is suitable for a data science portfolio, and will test your knowledge of cleaning, merging, manipulating, and test for significance in data. The week ends with two discussions of science and the rise of the fourth paradigm -- data driven discovery.", "video": ["Week 4 Lectures Jupyter Notebook", "Introduction", "Distributions", "More Distributions", "Hypothesis Testing in Python", "End of Theory", "Science Isn't Broken: p-hacking activity", "Assignment 4 - Project", "Post-course Survey", "Assignment 4 Submission"], "title": "Week 4"}], "title": "Introduction to Data Science in Python"}, {"course_info": "About this course: You will learn how to build a successful machine learning project. If you aspire to be a technical leader in AI, and know how to set direction for your team's work, this course will show you how.\n\nMuch of this content has never been taught elsewhere, and is drawn from my experience building and shipping many deep learning products. This course also has two \"flight simulators\" that let you practice decision-making as a machine learning project leader. This provides \"industry experience\" that you might otherwise get only after years of ML work experience.\n\nAfter 2 weeks, you will: \n- Understand how to diagnose errors in a machine learning system, and \n- Be able to prioritize the most promising directions for reducing error\n- Understand complex ML settings, such as mismatched training/test sets, and comparing to and/or surpassing human-level performance\n- Know how to apply end-to-end learning, transfer learning, and multi-task learning\n\nI've seen teams waste months or years through not understanding the principles taught in this course. I hope this two week course will save you months of time.\n\nThis is a standalone course, and you can take this so long as you have basic machine learning knowledge. This is the third course in the Deep Learning Specialization.", "level": "Beginner", "package_name": "Deep Learning Specialization ", "created_by": "deeplearning.ai", "package_num": "3", "teach_by": [{"name": "Andrew Ng", "department": null}, {"name": "Head Teaching Assistant - Kian Katanforoosh", "department": null}, {"name": "Teaching Assistant - Younes Bensouda Mourri", "department": null}], "target_audience": "Who is this class for: Pre-requisites:\n- This course is aimed at individuals with basic knowledge of machine learning, who want to know how to set technical direction and prioritization for their work.\n- It is recommended that you take course one and two of this specialization (Neural Networks and Deep Learning, and Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization) prior to beginning this course.", "rating": "4.8", "week_data": [{"description": "", "video": ["Why ML Strategy", "Orthogonalization", "Single number evaluation metric", "Satisficing and Optimizing metric", "Train/dev/test distributions", "Size of the dev and test sets", "When to change dev/test sets and metrics", "Why human-level performance?", "Avoidable bias", "Understanding human-level performance", "Surpassing human-level performance", "Improving your model performance", "Machine Learning flight simulator", "Andrej Karpathy interview", "Bird recognition in the city of Peacetopia (case study)"], "title": "ML Strategy (1)"}, {"description": "", "video": ["Carrying out error analysis", "Cleaning up incorrectly labeled data", "Build your first system quickly, then iterate", "Training and testing on different distributions", "Bias and Variance with mismatched data distributions", "Addressing data mismatch", "Transfer learning", "Multi-task learning", "What is end-to-end deep learning?", "Whether to use end-to-end deep learning", "Ruslan Salakhutdinov interview", "Autonomous driving (case study)"], "title": "ML Strategy (2)"}], "title": "Structuring Machine Learning Projects"}, {"course_info": "About this course: As data collection has increased exponentially, so has the need for people skilled at using and interacting with data; to be able to think critically, and provide insights to make better decisions and optimize their businesses. This is a data scientist, “part mathematician, part computer scientist, and part trend spotter” (SAS Institute, Inc.). According to Glassdoor, being a data scientist is the best job in America; with a median base salary of $110,000 and thousands of job openings at a time. The skills necessary to be a good data scientist include being able to retrieve and work with data, and to do that you need to be well versed in SQL, the standard language for communicating with database systems.\n\nThis course is designed to give you a primer in the fundamentals of SQL and working with data so that you can begin analyzing it for data science purposes. You will begin to ask the right questions and come up with good answers to deliver valuable insights for your organization. This course starts with the basics and assumes you do not have any knowledge or skills in SQL. It will build on that foundation and gradually have you write both simple and complex queries to help you select data from tables.  You'll start to work with different types of data like strings and numbers and discuss methods to filter and pare down your results. \n\nYou will create new tables and be able to move data into them. You will learn common operators and how to combine the data. You will use case statements and concepts like data governance and profiling. You will discuss topics on data, and practice using real-world programming assignments. You will interpret the structure, meaning, and relationships in source data and use SQL as a professional to shape your data for targeted analysis purposes. \n\nAlthough we do not have any specific prerequisites or software requirements to take this course, you a simple text editor is recommended for the final project. So what are you waiting for? This is your first step in landing a job in the best occupation in the US and soon the world!", "level": "Beginner", "package_name": null, "created_by": "University of California, Davis", "package_num": null, "teach_by": [{"name": "Sadie St. Lawrence", "department": "Founder and Executive Director, Women in Data (WID)"}], "target_audience": "Who is this class for: This course is primary targeted for anyone entering the data science field. It assumes you have no prior knowledge of the SQL programming language.", "rating": "4.5", "week_data": [{"description": "In this module, you will be able to define SQL and discuss how SQL differs from other computer languages. You will be able to compare and contrast the roles of a database administrator and a data scientist, and explain the differences between one-to-one, one-to-many, and many-to-many relationships with databases. You will be able to use the SELECT statement and talk about some basic syntax rules. You will be able to add comments in your code and synthesize its importance.", "video": ["Course Introduction", "Module Introduction", "What is SQL Anyway?", "Data Models, Part 1: Thinking About Your Data", "Data Models, Part 2: The Evolution of Data Models", "Data Models, Part 3: Relational vs. Transactional Models", "Retrieving Data with a SELECT Statement", "Creating Tables", "Creating Temporary Tables", "Adding Comments to SQL", "Summary", "SQL Overview", "Data Modeling and ER Diagrams", "Comparing NoSQL and SQL", "Module 1 Quiz", "Module 1 Coding Questions"], "title": "Getting Started and Selecting & Retrieving Data with SQL"}, {"description": "In this module, you will be able to use several more new clauses and operators including WHERE, BETWEEN, IN, OR, NOT, LIKE, ORDER BY, and GROUP BY. You will be able to use the wildcard function to search for more specific or parts of records, including their advantages and disadvantages, and how best to use them. You will be able to discuss how to use basic math operators, as well as aggregate functions like AVERAGE, COUNT, MAX, MIN, and others to begin analyzing our data.", "video": ["Module Introduction", "Basics of Filtering with SQL", "Advanced Filtering: IN, OR, and NOT", "Using Wildcards in SQL", "Sorting with ORDER BY", "Math Operations", "Aggregrate Functions", "Grouping Data with SQL", "Putting it All Together", "SQL for Various Data Science Languages", "Module 2 Quiz", "Module 2 Coding Assignment"], "title": "Filtering, Sorting, and Calculating Data with SQL"}, {"description": "In this module, you will be able to discuss subqueries, including their advantages and disadvantages, and when to use them. You will be able to recall the concept of a key field and discuss how these help us link data together with JOINs. You will be able to identify and define several types of JOINs, including the Cartesian join, an inner join, left and right joins, full outer joins, and a self join. You will be able to use aliases and pre-qualifiers to make your SQL code cleaner and efficient.", "video": ["Module Introduction", "Using Subqueries", "Subquery Best Practices and Considerations", "Joining Tables: An Introduction", "Cartesian (Cross) Joins", "Inner Joins", "Aliases and Self Joins", "Advanced Joins: Left, Right, and Full Outer Joins", "Unions", "Summary", "SQL and Python", "Union and Union All", "What do you think you'll use?", "Module 3 Quiz", "Module 3 Coding Assignment"], "title": "Subqueries and Joins in SQL"}, {"description": "In this module, you will be able to discuss how to modify strings by concatenating, trimming, changing the case, and using the substring function. You will be able to discuss the date and time strings specifically. You will be able to use case statements and finish this module by discussing data governance and profiling. You will also be able to apply fundamental principles when using SQL for data science. You'll be able to use tips and tricks to apply SQL in a data science context.\n", "video": ["Module Introduction", "Working with Text Strings", "Working with Date and Time Strings", "Date and Time Strings Examples", "Case Statements", "Views", "Data Governance and Profiling", "Using SQL for Data Science, Part 1", "Using SQL for Data Science, Part 2", "Additional SQL Resources to Explore", "How do you plan on using SQL in the future?", "Yelp Dataset SQL Lookup", "Course Summary", "Module 4 Quiz", "Module 4 Coding Questions", "Data Scientist Role Play: Profiling and Analyzing the Yelp Dataset"], "title": "Modifying and Analyzing Data with SQL"}], "title": "SQL for Data Science"}, {"course_info": "About this course: In this course you will get an introduction to the main tools and ideas in the data scientist's toolbox. The course gives an overview of the data, questions, and tools that data analysts and data scientists work with. There are two components to this course. The first is a conceptual introduction to the ideas behind turning data into actionable knowledge. The second is a practical introduction to the tools that will be used in the program like version control, markdown, git, GitHub, R, and RStudio.", "level": null, "package_name": "Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "1", "teach_by": [{"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "During Week 1, you'll learn about the goals and objectives of the Data Science Specialization and each of its components. You'll also get an overview of the field as well as instructions on how to install R.", "video": ["Welcome to the Data Scientist's Toolbox", "Pre-Course Survey", "Syllabus", "Specialization Textbooks", "Specialization Motivation", "The Elements of Data Analytic Style", "The Data Scientist's Toolbox", "Getting Help", "Finding Answers", "R Programming Overview", "Getting Data Overview", "Exploratory Data Analysis Overview", "Reproducible Research Overview", "Statistical Inference Overview", "Regression Models Overview", "Practical Machine Learning Overview", "Building Data Products Overview", "Installing R on Windows  {Roger Peng}", "Install R on a Mac  {Roger Peng}", "Installing Rstudio  {Roger Peng}", "Installing Outside Software on Mac (OS X Mavericks)", "Week 1 Quiz"], "title": "Week 1"}, {"description": "This is the most lecture-intensive week of the course. The primary goal is to get you set up with R, Rstudio, Github, and the other tools we will use throughout the Data Science Specialization and your ongoing work as a data scientist. ", "video": ["Tips from Coursera Users - Optional Video ", "Command Line Interface", "Introduction to Git", "Introduction to Github", "Creating a Github Repository", "Basic Git Commands", "Basic Markdown", "Installing R Packages", "Installing Rtools", "Week 2 Quiz"], "title": "Week 2: Installing the Toolbox"}, {"description": "The Week 3 lectures focus on conceptual issues behind study design and turning data into knowledge. If you have trouble or want to explore issues in more depth, please seek out answers on the forums. They are a great resource! If you happen to be a superstar who already gets it, please take the time to help your classmates by answering their questions as well. This is one of the best ways to practice using and explaining your skills to others. These are two of the key characteristics of excellent data scientists. ", "video": ["Types of Questions", "What is Data?", "What About Big Data?", "Experimental Design", "Week 3 Quiz"], "title": "Week 3: Conceptual Issues"}, {"description": "In Week 4, we'll focus on the Course Project. This is your opportunity to install the tools and set up the accounts that you'll need for the rest of the specialization and for work in data science.", "video": ["Post-Course Survey", "Course Project"], "title": "Week 4: Course Project Submission & Evaluation"}], "title": "The Data Scientist’s Toolbox"}, {"course_info": "About this course: Learn about artificial neural networks and how they're being used for machine learning, as applied to speech and object recognition, image segmentation, modeling language and human motion, etc. We'll emphasize both the basic algorithms and the practical tricks needed to get them to work well.\n\nThis course contains the same content presented on Coursera beginning in 2013. It is not a continuation or update of the original course. It has been adapted for the new platform.\n\nPlease be advised that the course is suited for an intermediate level learner - comfortable with calculus and with experience programming (Python).", "level": null, "package_name": null, "created_by": "University of Toronto", "package_num": null, "teach_by": [{"name": "Geoffrey Hinton", "department": "Department of Computer Science"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "Introduction to the course -  machine learning and neural nets", "video": ["Syllabus and Course Logistics", "Lecture Slides (and resources)", "Why do we need machine learning? [13 min]", "What are neural networks? [8 min]", "Some simple models of neurons [8 min]", "A simple example of learning [6 min]", "Three types of learning [8 min]", "Setting Up Your Programming Assignment Environment", "Installing Octave on Windows", "Installing Octave on Mac OS X (10.10 Yosemite and 10.9 Mavericks)", "Installing Octave on Mac OS X (10.8 Mountain Lion and Earlier)", "Installing Octave on GNU/Linux", "More Octave", "Lecture 1 Quiz"], "title": "Introduction "}, {"description": "An overview of the main types of neural\nnetwork architecture ", "video": ["Lecture Slides (and resources)", "Types of neural network architectures [7 min]", "Perceptrons: The first generation of neural networks [8 min]", "A geometrical view of perceptrons [6 min]", "Why the learning works [5 min]", "What perceptrons can't do [15 min]", "Lecture 2 Quiz"], "title": "The Perceptron learning procedure"}, {"description": "Learning the weights of a linear neuron ", "video": ["Lecture Slides (and resources)", "Learning the weights of a linear neuron [12 min]", "The error surface for a linear neuron [5 min]", "Learning the weights of a logistic output neuron [4 min]", "The backpropagation algorithm [12 min]", "Using the derivatives computed by backpropagation [10 min]", "Forward Propagation in Neural Networks", "Lecture 3 Quiz", "Programming Assignment 1: The perceptron learning algorithm."], "title": "The backpropagation learning proccedure"}, {"description": "Learning to predict the next word", "video": ["Lecture Slides (and resources)", "Learning to predict the next word [13 min]", "A brief diversion into cognitive science [4 min]", "Another diversion: The softmax output function [7 min]", "Neuro-probabilistic language models [8 min]", "Ways to deal with the large number of possible outputs [15 min]", "Lecture 4 Quiz"], "title": "Learning feature vectors for words"}, {"description": "In this module we look at why object recognition is difficult. ", "video": ["Lecture Slides (and resources)", "Why object recognition is difficult [5 min]", "Achieving viewpoint invariance [6 min]", "Convolutional nets for digit recognition [16 min]", "Convolutional nets for object recognition [17min]", "Lecture 5 Quiz", "Programming Assignment 2: Learning Word Representations."], "title": "Object recognition with neural nets"}, {"description": "We delve into mini-batch gradient descent as well as discuss adaptive learning rates.", "video": ["Lecture Slides (and resources)", "Overview of mini-batch gradient descent", "A bag of tricks for mini-batch gradient descent", "The momentum method", "Adaptive learning rates for each connection", "Rmsprop: Divide the gradient by a running average of its recent magnitude", "Lecture 6 Quiz"], "title": "Optimization: How to make the learning go faster"}, {"description": "This module explores training recurrent neural networks", "video": ["Lecture Slides (and resources)", "Modeling sequences: A brief overview", "Training RNNs with back propagation", "A toy example of training an RNN", "Why it is difficult to train an RNN", "Long-term Short-term-memory", "Lecture 7 Quiz"], "title": "Recurrent neural networks"}, {"description": "We continue our look at recurrent neural networks", "video": ["Lecture Slides (and resources)", "Modeling character strings with multiplicative connections [14 mins]", "Learning to predict the next character using HF [12  mins]", "Echo State Networks [9 min]", "Lecture 8 Quiz"], "title": "More recurrent neural networks"}, {"description": "We discuss strategies to make neural networks generalize better", "video": ["Lecture Slides (and resources)", "Overview of ways to improve generalization [12 min]", "Limiting the size of the weights [6 min]", "Using noise as a regularizer [7 min]", "Introduction to the full Bayesian approach [12 min]", "The Bayesian interpretation of weight decay [11 min]", "MacKay's quick and dirty method of setting weight costs [4 min]", "Lecture 9 Quiz", "Programming assignment 3: Optimization and generalization"], "title": "Ways to make neural networks generalize better"}, {"description": "This module we look at why it helps to combine multiple neural networks to improve generalization", "video": ["Lecture Slides (and resources)", "Why it helps to combine models [13 min]", "Mixtures of Experts [13 min]", "The idea of full Bayesian learning [7 min]", "Making full Bayesian learning practical [7 min]", "Dropout [9 min]", "Lecture 10 Quiz"], "title": "Combining multiple neural networks to improve generalization"}, {"description": "", "video": ["Lecture Slides (and resources)", "Hopfield Nets [13 min]", "Dealing with spurious minima [11 min]", "Hopfield nets with hidden units [10 min]", "Using stochastic units to improv search [11 min]", "How a Boltzmann machine models data [12 min]", "Lecture 11 Quiz"], "title": "Hopfield nets and Boltzmann machines"}, {"description": "This module deals with Boltzmann machine learning ", "video": ["Lecture Slides (and resources)", "Boltzmann machine learning [12 min]", "OPTIONAL VIDEO: More efficient ways to get the statistics [15 mins]", "Restricted Boltzmann Machines [11 min]", "An example of RBM learning [7 mins]", "RBMs for collaborative filtering [8 mins]", "Lecture 12 Quiz"], "title": "Restricted Boltzmann machines (RBMs)"}, {"description": "", "video": ["Lecture Slides (and resources)", "The ups and downs of back propagation [10 min]", "Belief Nets [13 min]", "The wake-sleep algorithm [13 min]", "Programming Assignment 4: Restricted Boltzmann Machines", "Lecture 13 Quiz"], "title": "Stacking RBMs to make Deep Belief Nets"}, {"description": "", "video": ["Lecture Slides (and resources)", "Learning layers of features by stacking RBMs [17 min]", "Discriminative learning for DBNs [9 mins]", "What happens during discriminative fine-tuning? [8 mins]", "Modeling real-valued data with an RBM [10 mins]", "OPTIONAL VIDEO: RBMs are infinite sigmoid belief nets [17 mins]", "Lecture 14 Quiz"], "title": "Deep neural nets with generative pre-training"}, {"description": "", "video": ["Lecture Slides (and resources)", "From PCA to autoencoders [5 mins]", "Deep auto encoders [4 mins]", "Deep auto encoders for document retrieval [8 mins]", "Semantic Hashing [9 mins]", "Learning binary codes for image retrieval [9 mins]", "Shallow autoencoders for pre-training [7 mins]", "Lecture 15 Quiz", "Final Exam"], "title": "Modeling hierarchical structure with neural nets"}, {"description": "", "video": ["OPTIONAL: Learning a joint model of images and captions [10 min]", "OPTIONAL: Hierarchical Coordinate Frames [10 mins]", "OPTIONAL: Bayesian optimization of hyper-parameters [13 min]"], "title": "Recent applications of deep neural nets"}], "title": "Neural Networks for Machine Learning"}, {"course_info": "About this course: In this course you will learn how to program in R and how to use R for effective data analysis. You will learn how to install and configure software necessary for a statistical programming environment and describe generic programming language concepts as they are implemented in a high-level statistical language. The course covers practical issues in statistical computing which includes programming in R, reading data into R, accessing R packages, writing R functions, debugging, profiling R code, and organizing and commenting R code. Topics in statistical data analysis will provide working examples.", "level": "Intermediate", "package_name": "Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "2", "teach_by": [{"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": "Who is this class for: Some programming experience (in any language) is recommended. ", "rating": "4.5", "week_data": [{"description": "This week covers the basics to get you started up with R. The Background Materials lesson contains information about course mechanics and some videos on installing R. The Week 1 videos cover the history of R and S, go over the basic data types in R, and describe the functions for reading and writing data. I recommend that you watch the videos in the listed order, but watching the videos out of order isn't going to ruin the story. ", "video": ["Welcome to R Programming", "About the Instructor", "Pre-Course Survey", "Syllabus", "Course Textbook", "Course Supplement: The Art of Data Science", "Data Science Podcast: Not So Standard Deviations", "Installing R on a Mac", "Installing R on Windows", "Installing R Studio (Mac)", "Writing Code / Setting Your Working Directory (Windows)", "Writing Code / Setting Your Working Directory (Mac)", "Getting Started and R Nuts and Bolts", "Introduction", "Overview and History of R", "Getting Help", "R Console Input and Evaluation", "Data Types - R Objects and Attributes", "Data Types - Vectors and Lists", "Data Types - Matrices", "Data Types - Factors", "Data Types - Missing Values", "Data Types - Data Frames", "Data Types - Names Attribute", "Data Types - Summary", "Reading Tabular Data", "Reading Large Tables", "Textual Data Formats", "Connections: Interfaces to the Outside World", "Subsetting - Basics", "Subsetting - Lists", "Subsetting - Matrices", "Subsetting - Partial Matching", "Subsetting - Removing Missing Values", "Vectorized Operations", "Introduction to swirl", "Practical R Exercises in swirl Part 1", "swirl Lesson 1: Basic Building Blocks", "swirl Lesson 2: Workspace and Files", "swirl Lesson 3: Sequences of Numbers", "swirl Lesson 4: Vectors", "swirl Lesson 5: Missing Values", "swirl Lesson 6: Subsetting Vectors", "swirl Lesson 7: Matrices and Data Frames", "Week 1 Quiz"], "title": "Week 1: Background, Getting Started, and Nuts & Bolts"}, {"description": "Welcome to Week 2 of R Programming. This week, we take the gloves off, and the lectures cover key topics like control structures and functions. We also introduce the first programming assignment for the course, which is due at the end of the week.", "video": ["Week 2: Programming with R", "Control Structures - Introduction", "Control Structures - If-else", "Control Structures - For loops", "Control Structures - While loops", "Control Structures - Repeat, Next, Break", "Your First R Function", "Functions (part 1)", "Functions (part 2)", "Scoping Rules - Symbol Binding", "Scoping Rules - R Scoping Rules", "Scoping Rules - Optimization Example (OPTIONAL)", "Coding Standards", "Dates and Times", "Practical R Exercises in swirl Part 2", "swirl Lesson 1: Logic", "swirl Lesson 2: Functions", "swirl Lesson 3: Dates and Times", "Programming Assignment 1 INSTRUCTIONS: Air Pollution", "Week 2 Quiz", "Programming Assignment 1: Quiz"], "title": "Week 2: Programming with R"}, {"description": "We have now entered the third week of R Programming, which also marks the halfway point. The lectures this week cover loop functions and the debugging tools in R. These aspects of R make R useful for both interactive work and writing longer code, and so they are commonly used in practice.", "video": ["Week 3: Loop Functions and Debugging", "Loop Functions - lapply", "Loop Functions - apply", "Loop Functions - mapply", "Loop Functions - tapply", "Loop Functions - split", "Debugging Tools - Diagnosing the Problem", "Debugging Tools - Basic Tools", "Debugging Tools - Using the Tools", "Practical R Exercises in swirl Part 3", "swirl Lesson 1: lapply and sapply", "swirl Lesson 2: vapply and tapply", "Week 3 Quiz", "Programming Assignment 2: Lexical Scoping "], "title": "Week 3: Loop Functions and Debugging"}, {"description": "This week covers how to simulate data in R, which serves as the basis for doing simulation studies. We also cover the profiler in R which lets you collect detailed information on how your R functions are running and to identify bottlenecks that can be addressed. The profiler is a key tool in helping you optimize your programs. Finally, we cover the str function, which I personally believe is the most useful function in R.", "video": ["Week 4: Simulation & Profiling", "The str Function", "Simulation - Generating Random Numbers", "Simulation - Simulating a Linear Model", "Simulation - Random Sampling", "R Profiler (part 1)", "R Profiler (part 2)", "Practical R Exercises in swirl Part 4", "swirl Lesson 1: Looking at Data", "swrl Lesson 2: Simulation", "swirl Lesson 3: Base Graphics", "Programming Assignment 3 INSTRUCTIONS: Hospital Quality", "Post-Course Survey", "Week 4 Quiz", "Programming Assignment 3: Quiz"], "title": "Week 4: Simulation & Profiling"}], "title": "R Programming"}, {"course_info": "About this course: This course will introduce the core data structures of the Python programming language. We will move past the basics of procedural programming and explore how we can use the Python built-in data structures such as lists, dictionaries, and tuples to perform increasingly complex data analysis. This course will cover Chapters 6-10 of the textbook “Python for Everybody”.  This course covers Python 3.", "level": null, "package_name": "Python for Everybody Specialization ", "created_by": "University of Michigan", "package_num": "2", "teach_by": [{"name": "Charles Severance", "department": "School of Information"}], "target_audience": null, "rating": "4.8", "week_data": [{"description": "In this class, we pick up where we left off in the previous class, starting in Chapter 6 of the textbook and covering Strings and moving into data structures.   The second week of this class is dedicated to getting Python installed if you want to actually run the applications on your desktop or laptop.  If you choose not to install Python, you can just skip to the third week and get a head start.", "video": ["Video Welcome - Dr. Chuck", "Reading: Welcome to Python Data Structures", "Textbook", "Welcome to Python 3", "Submitting Assignments", "6.1 - Strings", "6.2 - Manipulating Strings", "Notice for Auditing Learners: Assignment Submission", "Worked Exercise: 6.5", "Bonus: Office Hours New York City", "Bonus: Monash Museum of Computing History", "Fun: The Textbook Authors Meet @PyCon", "Audio Versions of All Lectures", "Chapter 6 Quiz", "Assignment 6.5"], "title": "Chapter Six: Strings"}, {"description": "In this module you will set things up so you can write Python programs.  We do not require installation of Python for this class.  You can write and test Python programs in the browser using the \"Python Code Playground\" in this lesson.  Please read the \"Using Python in this Class\" material for details.", "video": ["Important Reading: Using Python in this Class", "Notes on Choice of Text Editor", "Demonstration: Using the Python Playground", "Python Code Playground", "Windows 10: Installing Python and Writing A Program", "Windows: Taking Screen Shots", "Macintosh: Using Python and Writing A Program", "Macintosh: Taking Screen Shots", "Optional- Installing Python Screen Shots"], "title": "Unit: Installing and Using Python"}, {"description": "Up to now, we have been working with data that is read from the user or data in constants.   But real programs process much larger amounts of data by reading and writing files on the secondary storage on your computer.   In this chapter we start to write our first programs that read, scan, and process real data.  ", "video": ["7.1 - Files", "7.2 - Processing Files", "Demonstration: Worked Exercise 7.1", "Where is the 7.2 worked exercise?", "Bonus: Office Hours Barcelona", "Bonus: Gordon Bell - Building Blocks of Computing", "Chapter 7 Quiz", "Assignment 7.1", "Assignment 7.2"], "title": "Chapter Seven: Files"}, {"description": "As we want to solve more complex problems in Python, we need more powerful variables.  Up to now we have been using simple variables to store numbers or strings where we have a single value in a variable.  Starting with lists we will store many values in a single variable using an indexing scheme to store, organize, and retrieve different values from within a single variable.  We call these multi-valued variables \"collections\" or \"data structures\".", "video": ["8.1 - Lists", "8.2 - Manipulating Lists", "8.3 - Lists and Strings", "Fun: Python Lists in Paris", "Worked Exercise: Lists", "Bonus: Office Hours - Chicago", "Bonus: Rasmus Lerdorf - Inventing the PHP Language", "Chapter 8 Quiz", "Assignment 8.4", "Assignment 8.5"], "title": "Chapter Eight: Lists"}, {"description": "The Python dictionary is one of its most powerful data structures.  Instead of representing values in a linear list, dictionaries store data as key / value pairs.  Using key / value pairs gives us a simple in-memory \"database\" in a single Python variable.", "video": ["9.1 - Dictionaries", "9.2 - Counting with Dictionaries", "9.3 - Dictionaries and Files", "Worked Exercise: Dictionaries", "Bonus: Office Hours - Amsterdam", "Bonus: Brendan Eich - Inventing Javascript", "Fun: Dr. Chuck Goes Motocross Racing", "Chapter 9 Quiz", "Assignment 9.4"], "title": "Chapter Nine: Dictionaries"}, {"description": "Tuples are our third and final basic Python data structure.  Tuples are a simple version of lists.  We often use tuples in conjunction with dictionaries to accomplish multi-step tasks like sorting or looping through all of the data in a dictionary.", "video": ["10 - Tuples", "Worked Exercise: Tuples and Sorting", "Bonus: Office Hours - Puebla, Mexico", "Bonus: John Resig - Inventing JQuery", "Douglas Crockford: JavaScript Object Notation (JSON)", "Fun: The Greatest Taco in the World", "Chapter 10 Quiz", "Assignment 10.2"], "title": "Chapter Ten: Tuples"}, {"description": "To celebrate your making it to the halfway point in our Python for Everybody Specialization, we welcome you to attend our online graduation ceremony.  It is not very long, and it features a Commencement speaker and very short commencement speech.", "video": ["Graduation Ceremony", "Please Rate this Course on Class-Central"], "title": "Graduation"}], "title": "Python Data Structures"}, {"course_info": "About this course: By now you have definitely heard about data science and big data. In this one-week class, we will provide a crash course in what these terms mean and how they play a role in successful organizations. This class is for anyone who wants to learn what all the data science action is about, including those who will eventually need to manage data scientists. The goal is to get you up to speed as quickly as possible on data science without all the fluff. We've designed this course to be as convenient as possible without sacrificing any of the essentials.\n\nThis is a focused course designed to rapidly get you up to speed on the field of data science. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward.\n\nAfter completing this course you will know. \n\n1. How to describe the role data science plays in various contexts\n2. How statistics, machine learning, and software engineering play a role in data science\n3. How to describe the structure of a data science project\n4. Know the key terms and tools used by data scientists\n5. How to identify a successful and an unsuccessful data science project\n3. The role of a data science manager\n\n\nCourse cover image by r2hox. Creative Commons BY-SA: https://flic.kr/p/gdMuhT", "level": null, "package_name": "Executive Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "1", "teach_by": [{"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "This one-module course constitutes the first \"week\" of the Executive Data Science Specialization. This is an intensive introduction to what you need to know about data science itself. You'll learn important terminology and how successful organizations use data science.", "video": ["About Your Instructors", "Specialization Textbook", "Grading", "Pre-Course Survey", "What is Data Science?", "Statistics by example activities", "Statistics by example activities", "Machine learning", "Machine learning, the basics", "Machine learning further reading", "What is Software Engineering for Data Science?", "The Structure of a Data Science Project", "The outputs of a data science experiment", "The outputs of a data science experiment", "The four secrets of a successful data science experiment", "The four secrets of a successful data science experiment", "Data Scientist Toolbox", "Separating Hype from Value", "Post-Course Survey", "What is data science?", "What is statistics good for?", "Machine learning", "Quiz: Software Engineering", "Structure of a Data Science Project", "The outputs of a data science experiment", "Defining Success in Data Science", "Data scientist toolbox", "Separating hype from value"], "title": "A Crash Course in Data Science"}], "title": "A Crash Course in Data Science"}, {"course_info": "About this course: Welcome to Data-driven Decision Making. In this course you'll get an introduction to Data Analytics and its role in business decisions. You'll learn why data is important and how it has evolved. You'll be introduced to “Big Data” and how it is used. You'll also be introduced to a framework for conducting Data Analysis and what tools and techniques are commonly used. Finally, you'll have a chance to put your knowledge to work in a simulated business setting.\n\nThis course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017.", "level": "Beginner", "package_name": "Data Analysis and Presentation Skills: the PwC Approach Specialization ", "created_by": "PwC", "package_num": "1", "teach_by": [{"name": "Alex Mannella", "department": null}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "In this module you'll learn the basics of data analytics and how businesses use to solve problems. You'll learn the value data analytics brings to business decision-making processes. We’ll introduce you to a framework for data analysis and tools used in data analytics. Finally, we’re going to talk about careers and roles in data analytics and data science.", "video": ["Specialization overview", "Welcome to Data Driven Decision Making", "Course overview and syllabus", "Updating your profile", "Get to know your classmates and share your goals", "What is Data Analytics?", "Solving business problems using data analytics", "The value delivered by analytics", "Making business-defining decisions using data analytics", "PwC's Global Data and Analytics Survey 2016", "A Message from our Chief People Officer at PwC", "Why do you need a data and analytics framework?", "The 4 aspects of the data and analytics framework", "The data and analytics framework", "Data and analytics framework: tools and techniques", "Types of analytics", "Make better and faster decisions with data and analytics", "Making better and faster decisions", "Data and analytics at PwC", "Careers and roles in a professional services firm", "Week 1 recap with Amity and Mike", "Learn more about PwC and our career opportunities", "Week 1 Quiz"], "title": "Introduction to Data Analytics"}, {"description": "This module is an introductory look at big data and big data analytics where you will learn the about different types of data. We’ll also introduce you to PwC's perspective on big data and explain the impact of big data on businesses.  Finally we will name some of the different types of tools and technologies used to gather data.", "video": ["The marketplace and emerging trends in big data analytics", "Predicting the implications of technology advancements", "Business impacts of technology advancements and data trends", "What technology advacements have changed your behaviors?", "What is Big Data?", "\"Structured\", \"Semi-Structured\", and \"Unstructured\" data", "Implications of unstructured data - Case studies", "PwC's perspective on big data", "Data and analytics examples at PwC", "Data in action discussion", "Identifying, organizing and processing data", "Data tools and technologies", "What is the value you can expect from emerging trends in big data?", "Week 2 recap with Amity and Mike", "Week 2 Quiz"], "title": "Technology and types of data "}, {"description": "In this module we will describe some of the tools for data analytics and some of the key technologies for data analysis. We will talk about how visualization is important to the practice of data analytics. Finally we will identify a variety of tools and languages used and consider when those tools are best used.\n", "video": ["Types of data analysis techniques", "Data analysis approaches and techniques", "The role of Excel", "The role of SAS", "The role of R", "The role of Python", "The Power of Visualization", "The role of Tableau", "The role of QlikView", "What tools have you used?", "Week 3 recap with Amity and Mike", "Week 3 quiz"], "title": "Data analysis techniques and tools"}, {"description": "The course project will give you an opportunity to practice what you have learned.  You will participate in a simulated business situation in which you will select the best course of action.  You will then prepare a final deliverable which will be evaluated by your peers.  Additionally, you will have the opportunity to provide feedback on your peer's submissions.  ", "video": ["Intro to Week 4 and Course Project", "Final course simulation", "Course recap with Amity and Mike", "Learn more about PwC and our career opportunities", "Final course simulation quiz", "Final Project and Peer Review with Feedback"], "title": "Data-driven decision making project "}], "title": "Data-driven Decision Making"}, {"course_info": "About this course: This course introduces you to sampling and exploring data, as well as basic probability theory and Bayes' rule. You will examine various types of sampling methods, and discuss how such methods can impact the scope of inference. A variety of exploratory data analysis techniques will be covered, including numeric summary statistics and basic data visualization. You will be guided through installing and using R and RStudio (free statistical software), and will use this software for lab exercises and a final project. The concepts and techniques in this course will serve as building blocks for the inference and modeling courses in the Specialization.", "level": "Beginner", "package_name": "Statistics with R Specialization ", "created_by": "Duke University", "package_num": "1", "teach_by": [{"name": "Mine Çetinkaya-Rundel", "department": "Department of Statistical Science"}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "<p>This course introduces you to sampling and exploring data, as well as basic probability theory. You will examine various types of sampling methods and discuss how such methods can impact the utility of a data analysis. The concepts in this module will serve as building blocks for our later courses.<p>Each lesson comes with a set of learning objectives that will be covered in a series of short videos. Supplementary readings and practice problems will also be suggested from <a href=\"https://leanpub.com/openintro-statistics/\" target=\"_blank\">OpenIntro Statistics, 3rd Edition</a> (a free online introductory statistics textbook, that I co-authored). There will be weekly quizzes designed to assess your learning and mastery of the material covered that week in the videos. In addition, each week will also feature a lab assignment, in which you will use R to apply what you are learning to real data. There will also be a data analysis project designed to enable you to answer research questions of your own choosing.<p>Since this is a Coursera course, you are welcome to participate as much or as little as you’d like, though I hope that you will begin by participating fully. One of the most rewarding aspects of a Coursera course is participation in forum discussions about the course materials. Please take advantage of other students' feedback and insight and contribute your own perspective where you see fit to do so. You can also check out the <a href=\"https://www.coursera.org/learn/probability-intro/resources/crMc4\" target=\"_blank\">resource page</a> listing useful resources for this course. <p>Thank you for joining the Introduction to Probability and Data community! Say hello in the Discussion Forums. We are looking forward to your participation in the course.</p>", "video": ["Introduction to Statistics with R", "More about Introduction to Probability and Data", "Feedback Surveys"], "title": "About Introduction to Probability and Data"}, {"description": "<p>Welcome to Introduction to Probability and Data! I hope you are just as excited about this course as I am! In the next five weeks, we will learn about designing studies, explore data via numerical summaries and visualizations, and learn about rules of probability and commonly used probability distributions. If you have any questions, feel free to post them on <a href=\"https://www.coursera.org/learn/probability-intro/module/rQ9Al/discussions?sort=lastActivityAtDesc&page=1\" target=\"_blank\"><b>this module's forum</b></a> and discuss with your peers! To get started, view the <a href=\"https://www.coursera.org/learn/probability-intro/supplement/rooeY/lesson-learning-objectives\" target=\"_blank\"><b>learning objectives</b></a> of Lesson 1 in this module.</p>", "video": ["Lesson Learning Objectives", "Introduction", "Data Basics", "Observational Studies & Experiments", "Sampling and sources of bias", "Experimental Design", "(Spotlight) Random Sample Assignment", "Suggested Readings and Practice", "Week 1 Practice Quiz", "About Lesson Choices (Read Before Selection)", "Week 1 Lab Instructions (RStudio)", "Feedback survey", "Week 1 Quiz", "Week 1 Lab: Introduction to R and RStudio"], "title": "Introduction to Data"}, {"description": "<p>Welcome to Week 2 of Introduction to Probability and Data! Hope you enjoyed materials from Week 1. This week we will delve into numerical and categorical data in more depth, and introduce inference. </p>", "video": ["Lesson Learning Objectives", "Visualizing Numerical Data", "Measures of Center", "Measures of Spread", "Robust Statistics", "Transforming Data", "Lesson Learning Objectives", "Exploring Categorical Variables", "Introduction to Inference", "Suggested Readings and Practice", "Week 2 Practice Quiz", "Week 2 Lab Instructions (RStudio)", "Feedback survey", "Week 2 Quiz", "Week 2 Lab: Introduction to Data"], "title": "Exploratory Data Analysis and Introduction to Inference"}, {"description": "<p>Welcome to Week 3 of Introduction to Probability and Data! Last week we explored numerical and categorical data. This week we will discuss probability, conditional probability, the Bayes’ theorem, and provide a light introduction to Bayesian inference. </p><p>Thank you for your enthusiasm and participation, and have a great week! I’m looking forward to working with you on the rest of this course. </p>", "video": ["Lesson Learning Objectives", "Introduction", "Disjoint Events + General Addition Rule", "Independence", "Probability Examples", "(Spotlight) Disjoint vs. Independent", "Lesson Learning Objectives", "Conditional Probability", "Probability Trees", "Bayesian Inference", "Examples of Bayesian Inference", "Suggested Readings and Practice", "Week 3 Practice Quiz", "Week 3 Lab Instructions (RStudio)", "Feedback survey", "Week 3 Quiz", "Week 3 Lab: Probability"], "title": "Introduction to Probability"}, {"description": "<p>Great work so far! Welcome to Week 4 -- the last content week of Introduction to Probability and Data! This week we will introduce two probability distributions: the normal and the binomial distributions in particular. As usual, you can evaluate your knowledge in this week's quiz. There will be <b>no labs</b> for this week. Please don't hesitate to post any questions, discussions and related topics on <a href=\"https://www.coursera.org/learn/probability-intro/module/VdVNg/discussions?sort=lastActivityAtDesc&page=1\" target=\"_blank\"><b>this week's forum</b></a>.</p>", "video": ["Lesson Learning Objectives", "Normal Distribution", "Evaluating the Normal Distribution", "Working with the Normal Distribution", "Lesson Learning Objectives", "Binomial Distribution", "Normal Approximation to Binomial", "Working with the Binomial Distribution", "Suggested Readings and Practice", "Week 4 Practice Quiz", "Feedback survey", "Data Analysis Project Example", "Week 4 Quiz"], "title": "Probability Distributions"}, {"description": "<p>Well done! You have reached the last week of Introduction to Probability and Data! There will not be any new videos in this week, instead, you will be asked to complete an initial data analysis project with a real-world data set. The project is designed to help you discover and explore research questions of your own, using real data and statistical methods we learn in this class. The the project will be graded via peer assessments, meaning that you will need to evaluate three peers' projects after submitting your own.</p><p>Get started with your data analysis in this week! It should be interesting and very exciting! As usual, feel free to post questions, concerns, and comments about the project on <a href=\"https://www.coursera.org/learn/probability-intro/module/BaTDb/discussions?sort=lastActivityAtDesc&page=1\" target=\"_blank\"><b>this week's forum</b></a>.</p> ", "video": ["Project Information", "Feedback survey", "Data Analysis Project"], "title": "Data Analysis Project"}], "title": "Introduction to Probability and Data"}, {"course_info": "About this course: Interested in increasing your knowledge of the Big Data landscape?  This course is for those new to data science and interested in understanding why the Big Data Era has come to be.  It is for those who want to become conversant with the terminology and the core concepts behind big data problems, applications, and systems.  It is for those who want to start thinking about how Big Data might be useful in their business or career.  It provides an introduction to one of the most common frameworks, Hadoop, that has made big data analysis easier and more accessible -- increasing the potential for data to transform our world!\n\nAt the end of this course, you will be able to:\n\n* Describe the Big Data landscape including examples of real world big data problems including the three key sources of Big Data: people, organizations, and sensors. \n\n* Explain the V’s of Big Data (volume, velocity, variety, veracity, valence, and value) and why each impacts data collection, monitoring, storage, analysis and reporting.\n\n* Get value out of Big Data by using a 5-step process to structure your analysis. \n\n* Identify what are and what are not big data problems and be able to recast big data problems as data science questions.\n\n* Provide an explanation of the architectural components and programming models used for scalable big data analysis.\n\n* Summarize the features and value of core Hadoop stack components including the YARN resource and job management system, the HDFS file system and the MapReduce programming model.\n\n* Install and run a program using Hadoop!\n\nThis course is for those new to data science.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  \n\nHardware Requirements:\n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size.  \n\nSoftware Requirements:\nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge. Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+.", "level": null, "package_name": "Big Data Specialization ", "created_by": "University of California, San Diego", "package_num": "1", "teach_by": [{"name": "Ilkay Altintas", "department": "San Diego Supercomputer Center"}, {"name": "Amarnath Gupta", "department": "San Diego Supercomputer Center (SDSC)"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "Welcome to the Big Data Specialization!  We're excited for you to get to know us and we're looking forward to learning about you! ", "video": ["Welcome to the Big Data Specialization", "By the end of this course you will be able to...", "Optional: Watch this fun video about the San Diego Supercomputer Center!", "Tell us about yourself and learn about your classmates", "Let's Discuss: Why are you taking this class?"], "title": "Welcome "}, {"description": "Data -- it's been around (even digitally) for a while.  What makes data \"big\" and where does this big data come from?", "video": ["What launched the Big Data era?", "Applications: What makes big data valuable", "Let's Discuss: What application area interests you?", "Example: Saving lives with Big Data", "Example: Using Big Data to Help Patients", "A Sentiment Analysis Success Story: Meltwater helping Danone", "Did you know?: 25 facts about big data", "Slides: What Launched the Big Data Era?", "Slides: Applications: What Makes Big Data Valuable?", "Slides: Saving Lives With Big Data", "Slides: Using Big Data to Help Patients", "Getting Started: Where Does Big Data Come From?", "Machine-Generated Data: It's Everywhere and There's a Lot!", "Machine-Generated Data: Advantages", "Big Data Generated By People: The Unstructured Challenge", "Big Data Generated By People: How Is It Being Used?", "Organization-Generated Data: Structured but often siloed", "Organization-Generated Data: Benefits Come From Combining With Other Data Types", "The Key: Integrating Diverse Data", "Let's discuss:  Who are you providing data to?", "Extra Resources", "Slides: Machine-Generated Data: It's Everywhere and There's a Lot!", "Slides: Machine-Generated Data: Advantages", "Slides: Big Data Generated By People: The Unstructured Challenge", "Slides: Big Data Generated By People: How is it Being Used?", "Slides: Organization-Generated Big Data: Structured But Often Siloed", "Slides: Organizaton-Generated Big Data: Benefits", "Slides: The Key - Integrating Diverse Data", "Why Big Data and Where Did it Come From?"], "title": "Big Data: Why and Where"}, {"description": "You may have heard of the \"Big Vs\".  We'll give examples and descriptions of the commonly discussed 5.  But, we want to propose a 6th V and we'll ask you to practice writing Big Data questions targeting this V -- value.", "video": ["Getting Started: Characteristics Of Big Data", "Characteristics of Big Data - Volume", "What does astronomical scale mean?", "Characteristics of Big Data - Variety", "Characteristics of Big Data - Velocity", "Characteristics of Big Data - Veracity", "Characteristics of Big Data - Valence", "The Sixth V: Value", "A Small Definition of Big Data", "Practice: Writing Big Data questions", "Let's Discuss: Improving the Flamingo Game", "Slides: Getting Started - Characteristics of Big Data", "Slides: Characteristics of Big Data - Volume", "Slides: Characteristics of Big Data - Variety", "Slides: Characteristics of Big Data - Velocity", "Slides: Characteristics of Big Data - Veracity", "Slides: Characteristics of Big Data - Value", "Slides: Characteristics of Big Data - Valence", "V for the V's of Big Data"], "title": "Characteristics of Big Data and Dimensions of Scalability"}, {"description": "We love science and we love computing, don't get us wrong.  But the reality is we care about Big Data because it can bring value to our companies, our lives, and the world.  In this module we'll introduce a 5 step process for approaching data science problems.", "video": ["Data Science: Getting Value out of Big Data", "Building a Big Data Strategy", "How does big data science happen?:  Five Components of Data Science", "Five P's of Data Science", "Let's Discuss: Thinking more deeply about the Ps", "Asking the Right Questions", "Steps in the Data Science Process", "Step 1: Acquiring Data", "Step 2-A: Exploring Data", "Step 2-B: Pre-Processing Data", "Step 3: Analyzing Data", "Step 4: Communicating Results", "Step 5: Turning Insights into Action", "Let's Discuss: Building a Team", "Slides: Getting Value Out of Big Data", "Slides: Building a Big Data Strategy", "Slides: The Five P's of Data Science", "Slides: Asking the Right Questions", "Slides: Steps in the Data Science Process", "Slides: Step 1 - Acquiring Data", "Slides: Step 2A-Exploring Data", "Slides: Step 2B-Preprocessing Data", "Slides: Step 3-Data Analysis", "Slides: Step 4-Communicating Results", "Slides: Step 5-Turning Insights Into Action", "Data Science 101"], "title": "Data Science: Getting Value out of Big Data"}, {"description": "Big Data requires new programming frameworks and systems.  For this course, we don't  programming knowledge or experience -- but we do want to give you a grounding in some of the key concepts.", "video": ["Getting Started: Why worry about foundations?", "What is a Distributed File System?", "Scalable Computing over the Internet", "Programming Models for Big Data", "Slides: Getting Started-Why Worry About Foundations?", "Slides: What is a Distributed File System?", "Slides: Scalable Computing Over the Internet", "Slides: Programming Models for Big Data", "Foundations for Big Data"], "title": "Foundations for Big Data Systems and Programming"}, {"description": "Let's look at some details of Hadoop and MapReduce.  Then we'll go \"hands on\" and actually perform a simple MapReduce task in the Cloudera VM.  Pay attention - as we'll guide you in \"learning by doing\" in diagramming a MapReduce task as a Peer Review.", "video": ["Hadoop: Why, Where and Who?", "The Hadoop Ecosystem: Welcome to the zoo!", "The Hadoop Distributed File System: A Storage System for Big Data", "YARN: A Resource Manager for Hadoop", "MapReduce: Simple Programming for Big Results", "MapReduce in the Pasta Sauce Example", "When to Reconsider Hadoop?", "Cloud Computing: An Important Big Data Enabler", "Cloud Service Models: An Exploration of Choices", "Value From Hadoop and Pre-built Hadoop Images", "Slides for Getting Started With Hadoop", "Downloading and Installing the Cloudera VM Instructions (Mac)", "Downloading and Installing the Cloudera VM Instructions (Windows)", "FAQ", "Copy your data into the Hadoop Distributed File System (HDFS) Instructions", "Copy your data into the Hadoop Distributed File System (HDFS)", "Run the WordCount program Instructions", "Run the WordCount program", "Let's Discuss: Map Reduce in your life", "How do I figure out how to run Hadoop MapReduce programs?", "Intro to Hadoop", "Understand by Doing: MapReduce", "Running Hadoop MapReduce Programs Quiz"], "title": "Systems: Getting Started with Hadoop"}], "title": "Introduction to Big Data"}, {"course_info": "About this course: Data science courses contain math—no avoiding that! This course is designed to teach learners the basic math you will need in order to be successful in almost any data science math course and was created for learners who have basic math skills but may not have taken algebra or pre-calculus. Data Science Math Skills introduces the core math that data science is built upon, with no extra complexity, introducing unfamiliar ideas and math symbols one-at-a-time. \n\nLearners who complete this course will master the vocabulary, notation, concepts, and algebra rules that all data scientists must know before moving on to more advanced material.\n\nTopics include:\n~Set theory, including Venn diagrams\n~Properties of the real number line\n~Interval notation and algebra with inequalities\n~Uses for summation and Sigma notation\n~Math on the Cartesian (x,y) plane, slope and distance formulas\n~Graphing and describing functions and their inverses on the x-y plane,\n~The concept of instantaneous rate of change and tangent lines to a curve\n~Exponents, logarithms, and the natural log function.\n~Probability theory, including Bayes’ theorem.\n\nWhile this course is intended as a general introduction to the math skills needed for data science, it can be considered a prerequisite for learners interested in the course, \"Mastering Data Analysis in Excel,\" which is part of the Excel to MySQL Data Science Specialization.  Learners who master Data Science Math Skills will be fully prepared for success with the more advanced math concepts introduced in \"Mastering Data Analysis in Excel.\" \n\nGood luck and we hope you enjoy the course!", "level": "Beginner", "package_name": null, "created_by": "Duke University", "package_num": null, "teach_by": [{"name": "Daniel Egger", "department": "Pratt School of Engineering, Duke University"}, {"name": "Paul Bendich", "department": "Mathematics "}], "target_audience": "Who is this class for: This course is for anyone who has basic math skills, but is interested in learning or relearning algebra or pre-calculus so they can be successful in other data science math courses.   ", "rating": "4.4", "week_data": [{"description": "This short module includes an overview of the course's structure, working process, and information about course certificates, quizzes, video lectures, and other important course details. Make sure to read it right away and refer back to it whenever needed", "video": ["Welcome to Data Science Math Skills", "Course Information", "Weekly feedback surveys"], "title": "Welcome to Data Science Math Skills"}, {"description": "This module contains three lessons that are build to basic math vocabulary. The first lesson, \"Sets and What They’re Good For,\" walks you through the basic notions of set theory, including unions, intersections, and cardinality. It also gives a real-world application to medical testing. The second lesson, \"The Infinite World of Real Numbers,\" explains notation we use to discuss intervals on the real number line. The module concludes with the third lesson, \"That Jagged S Symbol,\" where you will learn how to compactly express a long series of additions and use this skill to define statistical quantities like mean and variance.", "video": ["A note about the video lectures in this lesson", "Sets - Basics and Vocabulary", "Sets - Medical Testing Example", "Sets - Venn Diagrams", "Practice quiz on Sets", "A note about the video lectures in this lesson", "Numbers - The Real Number Line", "Numbers - Less-than and Greater-than", "Numbers - Algebra With Inequalities", "Numbers - Intervals and Interval Notation", "Practice quiz on the Number Line, including Inequalities", "A note about the video lectures in this lesson", "Sigma Notation - Introduction to Summation", "Sigma Notation - Simplification Rules", "Sigma Notation - Mean and Variance", "Practice quiz on Simplification Rules and Sigma Notation", "Feedback", "Graded quiz on Sets, Number Line, Inequalities, Simplification, and Sigma Notation"], "title": "Building Blocks for Problem Solving"}, {"description": "This module builds vocabulary for graphing functions in the plane. In the first lesson, \"Descartes Was Really Smart,\" you will get to know the Cartesian Plane, measure distance in it, and find the equations of lines. The second lesson introduces the idea of a function as an input-output machine, shows you how to graph functions in the Cartesian Plane, and goes over important vocabulary.", "video": ["A note about the video lectures in this lesson", "Cartesian Plane - Plotting Points", "Cartesian Plane - Distance Formula", "Cartesian Plane - Point-Slope Formula for Lines", "Cartesian Plane: Slope-Intercept Formula for Lines", "Practice quiz on the Cartesian Plane", "A note about the video lectures in this lesson", "Functions - Mapping from Sets to Sets", "Functions - Graphing in the Cartesian Plane", "Functions - Increasing and Decreasing Functions", "Functions - Composition and Inverse", "Practice quiz on Types of Functions", "Feedback", "Graded quiz on Cartesian Plane and Types of Function"], "title": "Functions and Graphs"}, {"description": "This module begins a very gentle introduction to the calculus concept of the derivative.  The first lesson, \"This is About the Derivative Stuff,\" will give basic definitions, work a few examples, and show you how to apply these concepts to the real-world problem of optimization. We then turn to exponents and logarithms, and explain the rules and notation for these math tools. Finally we learn about the rate of change of continuous growth, and the special constant known as “e” that captures this concept in a single number—near 2.718.", "video": ["A note about the video lectures in this lesson", "Tangent Lines - Slope of a Graph at a Point", "Tangent Lines - The Derivative Function", "Practice quiz onTangent Lines to Functions", "A note about the video lectures in this lesson", "Using Integer Exponents", "Simplification Rules for Algebra using Exponents", "How Logarithms and Exponents are Related", "The Change of Base Formula", "The Rate of Growth of Continuous Processes", "Practice quiz on Exponents and Logarithms", "Feedback", "Graded quiz on Tangent Lines to Functions, Exponents and Logarithms"], "title": "Measuring Rates of Change"}, {"description": "This module introduces the vocabulary and notation of probability theory – mathematics for the study of outcomes that are uncertain but have predictable rates of occurrence. \n\nWe start with the basic definitions and rules of probability, including the probability of two or more events both occurring, the sum rule and the product rule, and then proceed to Bayes’ Theorem and how it is used in practical problems.", "video": ["A note about the video lectures in this lesson", "Probability Definitions and Notation", "Joint Probabilities", "Practice quiz on Probability Concepts", "A note about the video lectures in this lesson", "Permutations and Combinations", "Using Factorial and “M choose N”", "The Sum Rule, Conditional Probability, and the Product Rule", "Practice quiz on Problem Solving", "A note about the video lectures in this lesson", "Bayes’ Theorem (Part 1)", "Bayes’ Theorem (Part 2)", "The Binomial Theorem and Bayes Theorem", "Practice quiz on Bayes Theorem and the Binomial Theorem", "Feedback", "Probability (basic and Intermediate) Graded Quiz"], "title": "Introduction to Probability Theory"}], "title": "Data Science Math Skills"}, {"course_info": "About this course: This course will introduce the learner to applied machine learning, focusing more on the techniques and methods than on the statistics behind these methods. The course will start with a discussion of how machine learning is different than descriptive statistics, and introduce the scikit learn toolkit. The issue of dimensionality of data will be discussed, and the task of clustering data, as well as evaluating those clusters, will be tackled. Supervised approaches for creating predictive models will be described, and learners will be able to apply the scikit learn predictive modelling methods while understanding process issues related to data generalizability (e.g. cross validation, overfitting). The course will end with a look at more advanced techniques, such as building ensembles, and practical limitations of predictive models. By the end of this course, students will be able to identify the difference between a supervised (classification) and unsupervised (clustering) technique, identify which technique they need to apply for a particular dataset and need, engineer features to meet that need, and write python code to carry out an analysis.\n\nThis course should be taken after Introduction to Data Science in Python and Applied Plotting, Charting & Data Representation in Python and before Applied Text Mining in Python and Applied Social Analysis in Python.", "level": "Intermediate", "package_name": "Applied Data Science with Python Specialization ", "created_by": "University of Michigan", "package_num": "3", "teach_by": [{"name": "Kevyn Collins-Thompson", "department": "School of Information"}], "target_audience": "Who is this class for: This course is part of “Applied Data Science with Python“ and is intended for learners who have basic python or programming background, and want to apply statistics, machine learning, information visualization, social network analysis, and text analysis techniques to gain new insight into data.\n\nOnly minimal statistics background is expected, and the first course contains a refresh of these basic concepts. There are no geographic restrictions. Learners with a formal training in Computer Science but without formal training in data science will still find the skills they acquire in these courses valuable in their studies and careers.", "rating": "4.6", "week_data": [{"description": "This module introduces basic machine learning concepts, tasks, and workflow using an example classification problem based on the K-nearest neighbors method, and implemented using the scikit-learn library.", "video": ["Course Syllabus", "Introduction", "Help us learn more about you!", "Key Concepts in Machine Learning", "Python Tools for Machine Learning", "Notice for Auditing Learners: Assignment Submission", "Module 1 Notebook", "An Example Machine Learning Problem", "Examining the Data", "K-Nearest Neighbors Classification", "Zachary Lipton: The Foundations of Algorithmic Bias (optional)", "Assignment 1", "Module 1 Quiz", "Assignment 1 Submission"], "title": "Module 1: Fundamentals of Machine Learning - Intro to SciKit Learn"}, {"description": "This module delves into a wider variety of supervised learning methods for both classification and regression, learning about the connection between model complexity and generalization performance, the importance of proper feature scaling, and how to control model complexity by applying techniques like regularization to avoid overfitting.  In addition to k-nearest neighbors, this week covers linear regression (least-squares, ridge, lasso, and polynomial regression), logistic regression, support vector machines, the use of cross-validation for model evaluation, and decision trees. ", "video": ["Module 2 Notebook", "Introduction to Supervised Machine Learning", "Overfitting and Underfitting", "Supervised Learning: Datasets", "K-Nearest Neighbors: Classification and Regression", "Linear Regression: Least-Squares", "Linear Regression: Ridge, Lasso, and Polynomial Regression", "Logistic Regression", "Linear Classifiers: Support Vector Machines", "Multi-Class Classification", "Kernelized Support Vector Machines", "Cross-Validation", "Decision Trees", "A Few Useful Things to Know about Machine Learning", "Ed Yong: Genetic Test for Autism Refuted (optional)", "Classifier Visualization Playspace", "Assignment 2", "Module 2 Quiz", "Assignment 2 Submission"], "title": "Module 2: Supervised Machine Learning - Part 1"}, {"description": "This module covers evaluation and model selection methods that you can use to help understand and optimize the performance of your machine learning models. ", "video": ["Module 3 Notebook", "Model Evaluation & Selection", "Confusion Matrices & Basic Evaluation Metrics", "Classifier Decision Functions", "Precision-recall and ROC curves", "Multi-Class Evaluation", "Regression Evaluation", "Practical Guide to Controlled Experiments on the Web (optional)", "Model Selection: Optimizing Classifiers for Different Evaluation Metrics", "Assignment 3", "Module 3 Quiz", "Assignment 3 Submission"], "title": "Module 3: Evaluation"}, {"description": "This module covers more advanced supervised learning methods that include ensembles of trees (random forests, gradient boosted trees), and neural networks (with an optional summary on deep learning).  You will also learn about the critical problem of data leakage in machine learning and how to detect and avoid it.", "video": ["Module 4 Notebook", "Naive Bayes Classifiers", "Random Forests", "Gradient Boosted Decision Trees", "Neural Networks", "Neural Networks Made Easy (optional)", "Play with Neural Networks: TensorFlow Playground (optional)", "Deep Learning (Optional)", "Deep Learning in a Nutshell: Core Concepts (optional)", "Assisting Pathologists in Detecting Cancer with Deep Learning (optional)", "Data Leakage", "The Treachery of Leakage (optional)", "Leakage in Data Mining: Formulation, Detection, and Avoidance (optional)", "Data Leakage Example: The ICML 2013 Whale Challenge (optional)", "Rules of Machine Learning: Best Practices for ML Engineering (optional)", "Assignment 4", "Unsupervised Learning Notebook", "Introduction", "Dimensionality Reduction and Manifold Learning", "Clustering", "How to Use t-SNE Effectively", "How Machines Make Sense of Big Data: an Introduction to Clustering Algorithms", "Conclusion", "Post-course Survey", "Module 4 Quiz", "Assignment 4 Submission"], "title": "Module 4: Supervised Machine Learning - Part 2"}], "title": "Applied Machine Learning in Python"}, {"course_info": "About this course: If you want to break into competitive data science, then this course is for you! Participating in predictive modelling competitions can help you gain practical experience, improve and harness your data modelling skills in various domains such as credit, insurance, marketing, natural language processing, sales’ forecasting and computer vision to name a few. At the same time you get to do it in a competitive context against thousands of participants where each one tries to build the most predictive algorithm. Pushing each other to the limit can result in better performance and smaller prediction errors. Being able to achieve high ranks consistently can help you accelerate your career in data science.\n\nIn this course, you will learn to analyse and solve competitively such predictive modelling tasks. \n\nWhen you finish this class, you will:\n\n- Understand how to solve predictive modelling competitions efficiently and learn which of the skills obtained can be applicable to real-world tasks.\n- Learn how to preprocess the data and generate new features from various sources such as text and images.\n- Be taught advanced feature engineering techniques like generating mean-encodings, using aggregated statistical measures or finding nearest neighbors as a means to improve your predictions.\n- Be able to form reliable cross validation methodologies that help you benchmark your solutions and avoid overfitting or underfitting when tested with unobserved (test) data. \n- Gain experience of analysing and interpreting the data. You will become aware of inconsistencies, high noise levels, errors and other data-related issues such as leakages and you will learn how to overcome them. \n- Acquire knowledge of different algorithms and learn how to efficiently tune their hyperparameters and achieve top performance. \n- Master the art of combining different machine learning models and learn how to ensemble. \n- Get exposed to past (winning) solutions and codes and learn how to read them.\n\nDisclaimer : This is not a machine learning course in the general sense. This course will teach you how to get high-rank solutions against thousands of competitors with focus on practical usage of machine learning methods rather than the theoretical underpinnings behind them.\n\nPrerequisites: \n- Python: work with DataFrames in pandas, plot figures in matplotlib, import and train models from scikit-learn, XGBoost, LightGBM.\n- Machine Learning: basic understanding of linear models, K-NN, random forest, gradient boosting and neural networks.", "level": "Advanced", "package_name": "Advanced Machine Learning Specialization ", "created_by": "National Research University Higher School of Economics", "package_num": "2", "teach_by": [{"name": "Dmitry Ulyanov", "department": "HSE Faculty of Computer Science"}, {"name": "Alexander Guschin", "department": "HSE Faculty of Computer Science"}, {"name": "Mikhail Trofimov", "department": "HSE Faculty of Computer Science"}, {"name": "Dmitry Altukhov", "department": "HSE Faculty of Computer Science"}, {"name": "Marios Michailidis", "department": "H2O.ai"}], "target_audience": null, "rating": "4.8", "week_data": [{"description": "This week we will introduce you to competitive data science. You will learn about competitions' mechanics, the difference between competitions and a real life data science,  hardware and software that people usually use in competitions. We will also briefly recap major ML models frequently used in competitions.", "video": ["Introduction", "Welcome!", "Meet your lecturers", "Course overview", "Week 1 overview", "Competition Mechanics", "Kaggle Overview [screencast]", "Real World Application vs Competitions", "Practice Quiz", "Recap of main ML algorithms", "Disclaimer", "Recap", "Explanation for quiz questions", "Will performance of GBDT model drop dramatically if we remove the first tree?", "Additional Materials and Links", "Software/Hardware Requirements", "Software/Hardware", "Pandas basics", "Explanation for quiz questions", "Additional Material and Links", "Recap", "Pandas basics", "Graded Soft/Hard Quiz"], "title": "Introduction & Recap"}, {"description": "In this module we will summarize approaches to work with features: preprocessing, generation and extraction. We will see, that the choice of the machine learning model impacts both preprocessing we apply to the features and our approach to generation of new ones. We will also discuss feature extraction from text with Bag Of Words and Word2vec, and feature extraction from images with Convolution Neural Networks.", "video": ["Overview", "Numeric features", "Categorical and ordinal features", "Datetime and coordinates", "Handling missing values", "Feature preprocessing and generation with respect to models", "Explanation for quiz questions", "Additional Material and Links", "Bag of words", "Word2vec, CNN", "Feature extraction from text and images", "Explanation for quiz questions", "Additional Material and Links", "Feature preprocessing and generation with respect to models", "Feature extraction from text and images"], "title": "Feature Preprocessing and Generation with Respect to Models"}, {"description": "This is just a reminder, that the final project in this course is better to start soon! The final project is in fact a competition, in this module you can find an information about it.", "video": ["Final project", "Final project overview", "Meet and Greet ", "Final project advice #1"], "title": "Final Project Description"}, {"description": "We will start this week with Exploratory Data Analysis (EDA). It is a very broad and exciting topic and an essential component of solving process. Besides regular videos you will find a walk through EDA process for Springleaf competition data and an example of prolific EDA for NumerAI competition with extraordinary findings.", "video": ["Week 2 overview", "Exploratory data analysis", "Building intuition about the data", "Reading material for video 2", "Exploring anonymized data", "Notebook for video 3 screencast ", "Visualizations", "Dataset cleaning and other things to check", "Additional material and links", "Notebook for the screencast", "Springleaf competition EDA I", "Springleaf competition EDA II", "Numerai competition EDA", "Exploratory data analysis"], "title": "Exploratory Data Analysis"}, {"description": "In this module we will discuss various validation strategies. We will see that the strategy we choose depends on the competition setup and that correct validation scheme is one of the bricks for any winning solution.   ", "video": ["Validation and overfitting", "Validation strategies", "Validation strategies", "Data splitting strategies", "Problems occurring during validation", "Validation", "Comments on quiz", "Additional material and links", "Validation"], "title": "Validation"}, {"description": "Finally, in this module we will cover something very unique to data science competitions. That is, we will see examples how it is sometimes possible to get a top position in a competition with a very little machine learning, just by exploiting a data leakage.   ", "video": ["Basic data leaks", "Leaderboard probing and examples of rare data leaks", "Expedia challenge", "Comments on quiz", "Data leakages", "Additional material and links", "Final project advice #2", "Looking for a team", "Data leakages", "Data leakages", "Data leakages"], "title": "Data Leakages"}, {"description": "This week we will first study another component of the competitions: the evaluation metrics. We will recap the most prominent ones and then see, how we can efficiently optimize a metric given in a competition.", "video": ["Week 3 overview", "Motivation", "Regression metrics review I", "Constants for MSE and MAE", "Regression metrics review II", "A note about weighted median", "Classification metrics review", "General approaches for metrics optimization", "Regression metrics optimization", "Classification metrics optimization I", "Classification metrics optimization II", "\"Soft kappa\" loss implementation", "Metrics", "Comments on quiz", "Additional material and links", "Metrics"], "title": "Metrics Optimization"}, {"description": "In this module we will study a very powerful technique for feature generation. It has a lot of names, but here we call it \"mean encodings\". We will see the intuition behind them, how to construct them, regularize and extend them.    ", "video": ["Concept of mean encoding", "Regularization", "Extensions and generalizations", "Comments on quiz", "Mean encoding implementations notebook", "Final project advice #3", "Mean encodings", "Mean encoding implementation"], "title": "Advanced Feature Engineering I"}, {"description": "In this module we will talk about hyperparameter optimization process. We will also have a special video with practical tips and tricks, recorded by four instructors.", "video": ["Week 4 overview", "Hyperparameter tuning I", "Hyperparameter tuning II", "How to find sufficient `n_estimators` in Random Forest", "Hyperparameter tuning III", "Practice quiz", "Comments on quiz", "Additional material and links", "Practical guide", "How to use macros in Jupyter", "Additional materials and links", "Graded quiz"], "title": "Hyperparameter Optimization"}, {"description": "In this module we will learn about a few more advanced feature engineering techniques.", "video": ["Statistics and distance based features", "Matrix factorizations", "Feature Interactions", "t-SNE", "Comments on quiz", "Additional Materials and Links", "KNN features implementation", "Graded Advanced Features II Quiz", "KNN features implementation"], "title": "Advanced feature engineering II"}, {"description": "Nowadays it is hard to find a competition won by a single model! Every winning solution incorporates ensembles of models. In this module we will talk about the main ensembling techniques in general, and, of course, how it is better to ensemble the models in practice. ", "video": ["Introduction into ensemble methods", "Bagging", "Boosting", "Stacking", "StackNet", "Ensembling Tips and Tricks", "Validation schemes for 2-nd level models", "Ensembling implementation notebook", "Ensembling", "Comments on quiz", "Additional materials and links", "Final project advice #4", "Ensembling implementation", "Ensembling"], "title": "Ensembling"}, {"description": "For the 5th week we've prepared for you several \"walk-through\" videos. In these videos we discuss solutions to competitions we took prizes at. The video content is quite short this week to let you spend more time on the final project. Good luck!", "video": ["Week 5 overview", "Crowdflower Competition", "Springleaf Marketing Response", "Microsoft Malware Classification Challenge", "Walmart: Trip Type Classification", "Additional material and links"], "title": "Competitions go through"}, {"description": "Final project for the course.", "video": ["Final project", "Final project"], "title": "Final Project"}], "title": "How to Win a Data Science Competition: Learn from Top Kagglers"}, {"course_info": "About this course: In this first course of the specialization Excel Skills for Business you will learn the Essentials of Microsoft Excel. Within six weeks, you will learn to expertly navigate the Excel user interface, perform basic calculations with formulas and functions, professionally format spreadsheets, and create visualizations of data through charts and graphs.\n\nWhether you are self-taught and want to fill in the gaps for better efficiency and productivity, or whether you have never used Excel before, this course will set you up with a solid foundation to become a confident user and develop more advanced skills in later courses. \n\nWe have brought together a great teaching team that will be with you every step of the way. A broad range of practice quizzes and challenges will provide great opportunities to build up your skillset. Work through each new challenge with our team and in no time you will surprise yourself with how far you have come. \n\nSpreadsheet software is one of the most ubiquitous pieces of software used in workplaces across the world. Learning to confidently operate this software means adding a highly valuable asset to your employability portfolio. At a time when digital skills jobs are growing much faster than non-digital jobs, make sure to position yourself ahead of the rest by adding Excel skills to your employment portfolio.", "level": "Beginner", "package_name": "Excel Skills for Business Specialization ", "created_by": "Macquarie University", "package_num": "1", "teach_by": [{"name": "Dr Yvonne Breyer", "department": "Faculty of Business and Economics"}], "target_audience": "Who is this class for: Completing the course Excel Skills for Business: Essentials will be a highly valuable asset for anyone wanting to improve their employment portfolio. This course is for newcomers to Excel, for self-taught learners who want to fill gaps in their knowledge and for anyone wanting to consolidate their foundational knowledge of Excel in order to progress to a more advanced level of Excel. ", "rating": "4.9", "week_data": [{"description": "", "video": ["Welcome to Excel Skills for Business: Essentials", "Course goals and weekly learning objectives", "Important information about versions and regions", "Are essential skills too basic for me?", "Set your goals and be successful", "Week 1 Introduction", "Week 1 Discussion", "Read me before you start: Quizzes and Navigation", "Download the Week 1 workbooks", "Practice Video: Taking Charge of Excel", "Taking Charge of Excel", "Practice Video: Navigating and Selecting", "Navigating and Selecting", "Practice Video: View Options", "View Options", "Practice Video: Data Entry, Data Types, Editing and Deleting", "Data Entry", "Practice Video: Fill Handle", "Fill handle", "Week 1  Wrap-up", "Week 1: Practice Challenge", "Keyboard Shortcuts, Terminology, and Ninja Tips", "Excellent Tips and Resources", "Taking Charge of Excel: Test your skills, Part 1", "Taking Charge of Excel: Test your skills, Part 2"], "title": "Critical Core of Excel"}, {"description": "In this module, you will get introduced to formulas and functions in Excel.", "video": ["Week 2 Introduction", "Week 2 Discussion", "Download the Week 2 workbooks", "Practice Video: Formulas", "Formulas", "Practice Video: Formulas in Context", "Formulas in Context", "Practice Video: Functions I: SUM and AUTOSUM", "Functions I", "Practice Video: Functions II: AVERAGE, MIN and MAX", "Functions II", "Discussion: Absolute Cell References", "Practice Video: Absolute Cell References", "Absolute Cell References", "Practice Video: Calculations across sheets", "Calculations across Sheets", "Week 2  Wrap-up", "Week 2: Practice Challenges", "Word Search Puzzle: Master ranges", "Keyboard Shortcuts, Terminology, and Ninja Tips", "Excellent Tips and Resources", "Performing Calculations: Test your skills"], "title": "Performing calculations"}, {"description": "", "video": ["Week 3 Introduction", "Week 3 Discussion", "Download the Week 3 workbooks", "Practice Video: Formatting", "Formatting", "Practice Video: Borders", "Borders", "Practice Video: Alignment Tools", "Alignment Tools", "Practice Video: Format Painter", "Format Painter", "Practice Video: Number Formats", "Number Formats", "Practice Video: Styles and Themes", "Styles and Themes", "Week 3 Wrap-up", "Week 3: Practice Challenge", "Keyboard Shortcuts and Ninja Tips", "Excellent Tips and Resources", "Formatting: Test your skills"], "title": "Formatting"}, {"description": "", "video": ["Week 4 Introduction", "Week 4 Discussion", "Download the Week 4 workbooks", "Practice Video: Managing Rows and Columns", "Managing Rows and Columns", "Practice Video: Find and Replace", "Find and Replace", "Practice Video: Filtering", "Filtering", "Practice Video: Sorting", "Sorting", "Practice Video: Conditional Formatting", "Conditional Formatting", "Week 4 Wrap-up", "Week 4: Practice Challenge", "Keyboard Shortcuts, Terminology, and Ninja Tips", "Working with Data: Test your skills"], "title": "Working with Data"}, {"description": "", "video": ["Week 5 Introduction", "Week 5 Discussion", "Print to PDF - Setup Tips", "Download the Week 5 workbooks", "Practice Video: Print Preview", "Print Preview", "Practice Video: Orientation, Margins and Scale", "Orientation, Margins and Scale", "Practice Video: Page Breaks", "Page Breaks", "Practice Video: Print Titles", "Print Titles", "Practice Video: Headers and Footers", "Headers and Footers", "Week 5 Wrap-up", "Week 5: Practice Challenge", "Keyboard Shortcuts, Terminology, and Ninja Tips", "Excellent Tips and Resources", "Printing: Test your skills"], "title": "Printing"}, {"description": "", "video": ["Week 6 Introduction", "Week 6 Discussion", "Download the Week 6 workbooks", "Practice Video: Basic Chart Types: Pie, Column and Line Charts", "Basic Chart Types", "Practice Video: Move and Resize Charts", "Move and Resize Charts", "Practice Video: Change Chart Style & Type", "Change Chart Style and Type", "Practice Video: Modify Chart Elements", "Modify Chart Elements", "Week 6 Wrap-up", "Week 6: Practice Challenge", "Keyboard Shortcuts, Terminology, and Ninja Tips", "Excellent Tips and Resources", "Charts: Test your skills"], "title": "Charts"}, {"description": "", "video": ["Course 1 Final Assessment"], "title": "Final Assessment"}], "title": "Excel Skills for Business: Essentials"}, {"course_info": "About this course: This course will introduce students to the basics of the Structured Query Language (SQL) as well as basic database design for storing data as part of a multi-step data gathering, analysis, and processing effort.  The course will use SQLite3 as its database.  We will also build web crawlers and multi-step data gathering and visualization processes.  We will use the D3.js library to do basic data visualization.  This course will cover Chapters 14-15 of the book “Python for Everybody”. To succeed in this course, you should be familiar with the material covered in Chapters 1-13 of the textbook and the first three courses in this specialization. This course covers Python 3.", "level": null, "package_name": "Python for Everybody Specialization ", "created_by": "University of Michigan", "package_num": "4", "teach_by": [{"name": "Charles Severance", "department": "School of Information"}], "target_audience": null, "rating": "4.8", "week_data": [{"description": "To start this class out we cover the basics of Object Oriented Python. We won't be writing our own objects, but since many of the things we use like BeautifulSoup, strings, dictionaries, database connections all use Object Oriented (OO) patterns we should at least understand some of its patterns and terminology.", "video": ["Welcome to Using Databases with Python", "Help us learn more about you!", "Python Textbook", "Coming from Python 2 - Encoding Data in Python 3", "Unicode Characters and Strings", "Notice for Auditing Learners: Assignment Submission", "14.1 - Object Oriented Definitions and Terminology", "14.2 - Our First Class and Object", "14.3 - Object Life Cycle", "14.4 - Object Inheritance", "Bonus: Interview - Software Engineering - Bertrand Meyer", "Bonus: Office Hours - London", "Using Encoded Data in Python 3", "Object Oriented Programming"], "title": "Object Oriented Python"}, {"description": "We learn the four core CRUD operations (Create, Read, Update, and Delete) to manage data stored in a database.", "video": ["15.1 Relational Databases", "15.2 - Using Databases", "15.3 - Single Table CRUD", "Worked Example: Counting Email in a Database", "Bonus: Office Hours Zagreb, Croatia", "Interview: Elizabeth Fong - The Early Years of SQL", "Worked Example: Twspider.py (Chapter 15)", "Single-Table SQL", "Our First Database", "Counting Email in a Database"], "title": "Basic Structured Query Language"}, {"description": "In this section we learn about how data is stored across multiple tables in a database and how rows are linked (i.e., we establish relationships) in the database.", "video": ["15.4 - Designing a Data Model", "15.5 - Representing a Data Model in Tables", "15.6 - Inserting Relational Data", "15.7 - Reconstructing Data with JOIN", "Worked Example: Tracks.py (Chapter 15)", "Bonus: Office Hours Perth, Australia", "Bonus Interview: Niklaus Wirth", "Bonus: Office Hours Barcelona", "Multi-Table Relational SQL", "Multi-Table Database - Tracks"], "title": "Data Models and Relational SQL"}, {"description": "In this section we explore how to model situations like students enrolling in courses where each course has many students and each student is enrolled in many courses.", "video": ["15.8 - Many-to-Many Relationships", "Worked Example: roster.py (Chapter 15)", "Bonus: Office Hours Mexico, City", "Bonus Interview: Andrew Tannenbaum - Minix", "Worked Example: Twfriends.py (Chapter 15)", "Many-to-Many Relationships and Python", "Many Students in Many Courses"], "title": "Many-to-Many Relationships in SQL"}, {"description": "In this section, we put it all together, retrieve and process some data and then use the Google Maps API to visualize our data.", "video": ["16.1 - Geocoding", "16.2 - Geocoding Visualization", "Worked Example: Geodata (Chapter 16)", "Bonus: Office Hours - Amsterdam", "Bonus Interview: Richard Stallman - Free Software Foundation", "Bonus Interview: Brian Behlendorf - Apache Foundation", "Please Rate this Course on Class-Central", "Post-Course Survey", "Databases and Visualization (peer-graded)"], "title": "Databases and Visualization"}], "title": "Using Databases with Python"}, {"course_info": "About this course: This course will introduce the learner to information visualization basics, with a focus on reporting and charting using the matplotlib library. The course will start with a design and information literacy perspective, touching on what makes a good and bad visualization, and what statistical measures translate into in terms of visualizations. The second week will focus on the technology used to make visualizations in python, matplotlib, and introduce users to best practices when creating basic charts and how to realize design decisions in the framework. The third week will describe the gamut of functionality available in matplotlib, and demonstrate a variety of basic statistical charts helping learners to identify when a particular method is good for a particular problem. The course will end with a discussion of other forms of structuring and visualizing data. \n\nThis course should be taken after Introduction to Data Science in Python and before the remainder of the Applied Data Science with Python courses: Applied Machine Learning in Python, Applied Text Mining in Python, and Applied Social Network Analysis in Python.", "level": "Intermediate", "package_name": "Applied Data Science with Python Specialization ", "created_by": "University of Michigan", "package_num": "2", "teach_by": [{"name": "Christopher Brooks", "department": null}], "target_audience": "Who is this class for: This course is part of “Applied Data Science with Python“ and is intended for learners who have basic python or programming background, and want to apply statistics, machine learning, information visualization, social network analysis, and text analysis techniques to gain new insight into data.\n\nOnly minimal statistics background is expected, and the first course contains a refresh of these basic concepts. There are no geographic restrictions. Learners with a formal training in Computer Science but without formal training in data science will still find the skills they acquire in these courses valuable in their studies and careers.", "rating": "4.4", "week_data": [{"description": "In this module, you will get an introduction to principles of information visualization. We will be introduced to tools for thinking about design and graphical heuristics for thinking about creating effective visualizations. All of the course information on grading, prerequisites, and expectations are on the course syllabus, which is included in this module. ", "video": ["Introduction", "Syllabus", "Help us learn more about you!", "About the Professor: Christopher Brooks", "Tools for Thinking about Design (Alberto Cairo)", "Notice for Coursera Learners: Assignment Submission", "Hands-on Visualization Wheel", "Graphical heuristics: Data-ink ratio (Edward Tufte)", "Dark Horse Analytics (Optional)", "Graphical heuristics: Chart junk (Edward Tufte)", "Useful Junk?: The Effects of Visual Embellishment on Comprehension and Memorability of Charts", "Graphical heuristics: Lie Factor and Spark Lines (Edward Tufte)", "The Truthful Art (Alberto Cairo)", "Must a visual be enlightening?", "Graphics Lies, Misleading Visuals", "Graphics Lies, Misleading Visuals "], "title": "Module 1: Principles of Information Visualization"}, {"description": "In this module, you will delve into basic charting. For this week’s assignment, you will work with real world CSV weather data. You will manipulate the data to display the minimum and maximum temperature for a range of dates and demonstrate that you know how to create a line graph using matplotlib. Additionally, you will demonstrate the procedure of composite charts, by overlaying a scatter plot of record breaking data for a given year.", "video": ["Module 2 Jupyter Notebook", "Introduction", "Matplotlib Architecture", "Matplotlib", "Ten Simple Rules for Better Figures", "Basic Plotting with Matplotlib", "Scatterplots", "Line Plots", "Bar Charts", "Dejunkifying a Plot", "Plotting Weather Patterns", "Plotting Weather Patterns"], "title": "Module 2: Basic Charting"}, {"description": "In this module you will explore charting fundamentals. For this week’s assignment you will work to implement a new visualization technique based on academic research. This assignment is flexible and you can address it using a variety of difficulties - from an easy static image to an interactive chart where users can set ranges of values to be used.", "video": ["Module 3 Jupyter Notebook", "Subplots", "Histograms", "Selecting the Number of Bins in a Histogram: A Decision Theoretic Approach (Optional)", "Box Plots", "Heatmaps", "Animation", "Interactivity", "Practice Assignment: Understanding Distributions Through Sampling", "Practice Assignment: Understanding Distributions Through Sampling", "Building a Custom Visualization", "Assignment Reading", "Building a Custom Visualization "], "title": "Module 3: Charting Fundamentals"}, {"description": "In this module, then everything starts to come together. Your final assignment is entitled “Becoming a Data Scientist.” This assignment requires that you identify at least two publicly accessible datasets from the same region that are consistent across a meaningful dimension. You will state a research question that can be answered using these data sets and then create a visual using matplotlib that addresses your stated research question. You will then be asked to justify how your visual addresses your research question.", "video": ["Module 4 Jupyter Notebook", "Plotting with Pandas", "Seaborn", "Spurious Correlations", "Becoming an Independent Data Scientist", "Project Description", "Post-course Survey", "Becoming an Independent Data Scientist"], "title": "Module 4: Applied Visualizations"}], "title": "Applied Plotting, Charting & Data Representation in Python"}, {"course_info": "About this course: This course explores Excel as a tool for solving business problems. In this course you will learn the basic functions of excel through guided demonstration. Each week you will build on your excel skills and be provided an opportunity to practice what you’ve learned. Finally, you will have a chance to put your knowledge to work in a final project.  Please note, the content in this course was developed using a Windows version of Excel 2013.  \n\nThis course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017.", "level": "Beginner", "package_name": "Data Analysis and Presentation Skills: the PwC Approach Specialization ", "created_by": "PwC", "package_num": "2", "teach_by": [{"name": "Alex Mannella", "department": null}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "In this module you will learn the basics of Excel navigation and Excel basic functionality. You will learn how to navigate the basic Excel screen including using formulas, subtotals and text formatting. We will provide you an opportunity to perform a problem solving exercise using the basic Excel skills.", "video": ["Welcome to Problem Solving with Excel", "Course Overview and Syllabus", "Meet the PwC Content Developers", "Meet Your Classmates ", "Tao of Excel", "Guiding Principles for Using Excel", "Week 1 Welcome", "Excel Basics Workbook", "Excel Basics and Navigation Part 1", "Excel Basics and Navigation Part 2", "Excel Basics and Navigation Handout", "A Message from our Chief People Officer at PwC", "Basic Functionality Workbook", "Paste Special - Part 1", "Paste Special - Part 2", "Cell Locking - Part 1", "Cell Locking - Part 2", "Named Ranges", "Basic Functionality Handout", "Subtotal Excel Workbook", "Subtotal - Part 1", "Subtotal - Part 2", "Subtotal Handout", "Week 1 Project", "Excel Basics and Navigation Quiz", "Basic Functionality Quiz", "Subtotal Quiz", "Week 1 project quiz"], "title": "Overview of Excel"}, {"description": "In this module you will learn about VLookup, value cleansing and text functions. We will also introduce you to PwC's perspective on the value in cleansing data and using the appropriate functions.  Finally, we will provide you an opportunity to perform a problem solving exercise using VLookup, value cleansing and text function. ", "video": ["Welcome to Week 2", "VLookup Workbook", "VLookup", "Use VLOOKUP, data validation and multiple VLOOKUPs together", "HLOOKUP", "Nesting", "VLookup Handout", "Data Cleansing: Values Workbook", "Data Cleansing: Values - CLEAN- 1a", "Values - CLEAN -1b", "Values - TRIM", "Values - SUBSTITUTE", "Values - VALUE", "Values - TEXT", "Values - Stored as Text", "Data Cleansing: Values Handout", "Data Cleansing: Text Workbook", "Data Cleansing: Text Functions", "Text - LEN", "Text - CONCATENATE", "Text - Data Validation", "Data Cleansing:  Text Handout", "Week 2 Project", "Hints for Week 2 Project", "VLookup", "Data Cleanse - Values Quiz", "Data Cleanse - Text Quiz", "Week 2 Quiz"], "title": "vLookups and Data Cleansing"}, {"description": "In this module you will learn about logical functions and pivot tables. We will show you how to create and use pivot tables to solve business problems. We will give you an opportunity to practice creating and using a pivot table to solve a business problem. Finally, we will share some insight on PwC’s perspectives on the impact of Excel on your career and work. ", "video": ["Welcome to Week 3", "Logical Functions Workbook", "Logical Functions: Introduction", "Logical Functions - COUNTIFS", "Logical Functions - COUNTIFS Part 2", "Logical Functions - SUMIFS", "Logical Functions - SUMIFS Part 2", "Logical Functions - IF + THEN", "Logical Functions - IF + AND", "Logical Functions - IF + OR", "Logical Functions - Nesting", "Logical Functions Handout", "Bringing Value with Excel", "How will you bring value by using Excel?", "Pivot Tables Workbook", "Pivot Tables", "Navigate a Pivot Table", "Sort a Pivot Table", "Advanced Filtering", "Calculated Fields", "Slicers", "Pivot Tables Handout", "Week 3 Project", "Logical Functions Quiz", "Pivot Tables Quiz", "Week 3 Project Quiz"], "title": "Logical Functions & Pivot Tables"}, {"description": "In this module you will learn more advanced Excel formulas. We will show you how to create statistical formulas, perform an index match, and lastly, build financial formulas. We will provide you with an opportunity to problem solve using statistical formulas. Finally, we will give you an opportunity to practice what you have learned through a final project.", "video": ["Welcome to Week 4", "Statistical Forecasting Workbook", "Intro to Statistical Forcasting", "Statistical Forecasting - Part 1", "Statistical Forecasting - Part 2", "Statistical Forecasting Handout", "Index Match Workbook", "Index Match", "Index Match: MATCH Formula", "x and y Variables", "2 Variable Lookup", "Index Match Handout", "Financial Function Workbook", "Financial Function - NPV and IRR", "Financial Function - Calculating Payment Schedule", "Financial Function - Calculating Payment Period", "Financial Function Handout", "Week 4 Project", "Course Wrap-up", "Learn more about PwC and our career opportunities", "Statistical Forecasting Quiz", "Index Match Quiz", "Financial Functions", "Week 4 Project Quiz"], "title": "More Advanced Formulas"}], "title": "Problem Solving with Excel "}, {"course_info": "About this course: This course provides a rigorous introduction to the R programming language, with a  particular focus on using R for software development in a data science setting. Whether you are part of a data science team or working individually within a community of developers, this course will give you the knowledge of R needed to make useful contributions in those settings. As the first course in the Specialization, the course provides the essential foundation of R needed for the following courses. We cover basic R concepts and language fundamentals, key concepts like tidy data and related \"tidyverse\" tools, processing and manipulation of complex and large datasets, handling textual data, and basic data science tasks. Upon completing this course, learners will have fluency at the R console and will be able to create tidy datasets from a wide range of possible data sources.", "level": "Intermediate", "package_name": "Mastering Software Development in R Specialization ", "created_by": "Johns Hopkins University", "package_num": "1", "teach_by": [{"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Brooke Anderson", "department": "Colorado State University"}], "target_audience": "Who is this class for: This course is aimed at learners who have some experience programming computers but who are not familiar with the R environment. ", "rating": "4.4", "week_data": [{"description": "In this module, you'll learn the basics of R, including syntax, some tidy data principles and processes, and how to read data into R.  ", "video": ["Welcome to the R Programming Environment", "Course Textbook: Mastering Software Development in R", "Syllabus", "Swirl Assignments", "Datasets", "Lesson Introduction", "Evaluation", "Objects", "Numbers", "Creating Vectors", "Mixing Objects", "Explicit Coercion", "Matrices", "Lists", "Factors", "Missing Values", "Data Frames", "Names", "Attributes", "Summary", "The Importance of Tidy Data", "The “Tidyverse”", "Reading Tabular Data with the readr Package", "Reading Web-Based Data", "Flat files online", "Requesting data through a web API", "Scraping web data", "Parsing JSON, XML, or HTML data", "Swirl Lessons"], "title": "Basic R Language"}, {"description": "During this module, you'll learn to summarize, filter, merge, and otherwise manipulate data in R, including working through the challenges of dates and times. ", "video": ["Basic Data Manipulation", "Piping", "Summarizing data", "Selecting and filtering data", "Adding, changing, or renaming columns", "Spreading and gathering data", "Merging datasets", "Working with Dates, Times, Time Zones", "Converting to a date or date-time class", "Pulling out date and time elements", "Working with time zones", "Swirl Lessons"], "title": "Data Manipulation"}, {"description": "During this module, you'll learn to use R tools and packages to deal with text and regular expressions. You'll also learn how to manage and get the most from your computer's physical memory when working in R. ", "video": ["Text Processing and Regular Expressions", "Text Manipulation Functions in R", "Regular Expressions", "RegEx Functions in R", "The stringr Package", "Summary", "The Role of Physical Memory", "Back of the Envelope Calculations", "Internal Memory Management in R", "Swirl Lessons"], "title": "Text Processing, Regular Expression, & Physical Memory"}, {"description": "In this final module, you'll learn how to overcome the challenges of working with large datasets both in memory and out as well as how to diagnose problems and find help.", "video": ["Working with Large Datasets", "In-memory strategies", "Out-of-memory strategies", "Diagnosing Problems", "How to Google Your Way Out of a Jam", "Asking for Help", "Quiz Instructions", "Reading and Summarizing Data"], "title": "Large Datasets"}], "title": "The R Programming Environment"}, {"course_info": "About this course: One of the skills that characterizes great business data analysts is the ability to communicate practical implications of quantitative analyses to any kind of audience member.  Even the most sophisticated statistical analyses are not useful to a business if they do not lead to actionable advice, or if the answers to those business questions are not conveyed in a way that non-technical people can understand.  \n\nIn this course you will learn how to become a master at communicating business-relevant implications of data analyses.  By the end, you will know how to structure your data analysis projects to ensure the fruits of your hard labor yield results for your stakeholders.  You will also know how to streamline your analyses and highlight their implications efficiently using visualizations in Tableau, the most popular visualization program in the business world.  Using other Tableau features, you will be able to make effective visualizations that harness the human brain’s innate perceptual and cognitive tendencies to convey conclusions directly and clearly.  Finally, you will be practiced in designing and persuasively presenting business “data stories” that use these visualizations, capitalizing on business-tested methods and design principles.", "level": null, "package_name": "Excel to MySQL: Analytic Techniques for Business Specialization ", "created_by": "Duke University", "package_num": "3", "teach_by": [{"name": "Daniel Egger", "department": "Pratt School of Engineering, Duke University"}, {"name": "Jana Schaich Borg", "department": "Social Science Research Institute"}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "<p>The Coursera Specialization: <a href='https://www.coursera.org/specializations/excel-mysql' target='_blank'>Excel to MySQL: Analytic Techniques for Business</a>, is about how 'Big Data' interacts with business, and how to use data analytics to create value for businesses. This specialization consists of four courses and a final Capstone Project, where you will apply your skills to a real-world business process. You will learn to perform sophisticated data-analysis functions using powerful software tools such as Microsoft Excel, Tableau, and MySQL. To learn more, watch the video and review the specialization overview document we provided.<p>In the third course of the specialization: <b>Data Visualization and Communication with Tableau</b>, you will learn how to communicate business-relevant implications of data analyses.  <p>Specifically, you will:<ul><li>craft the right questions to ensure your analysis projects succeed;</li> <li>leverage questions to design logical and structured analysis plans;</li> <li>create the most important graphs used in business analysis and transform data in Tableau;</li><li>design business dashboards with Tableau;</li> <li>tell stories with data;</li><li>design effective slide presentations to showcase your data story; and </li><li>deliver compelling business presentations.</li></ul> <p>By the end of this course, you will know how to structure your data analysis projects to ensure the fruits of your hard labor yield results for your stakeholders.  You will also know how to streamline your analyses and highlight their implications efficiently using visualizations in Tableau, the most popular visualization program in the business world.  Using other Tableau features, you will be able to make effective visualizations that harness the human brain’s innate perceptual and cognitive tendencies to convey conclusions directly and clearly.  Finally, you will be practiced in designing and persuasively presenting business “data stories” that use these visualizations, capitalizing on business-tested methods and design principles by completing a final peer assessed project recommending a business process change. <P>To get started, please begin with the video 'About This Specialization.'<P>I hope you enjoy this week's materials!</P>", "video": ["About this Specialization", "Specialization Overview", "Welcome to the Course!", "Course Overview", "FAQ (Frequently Asked Questions)", "Special Thanks!", "About the Course Team", "Feedback Survey Information"], "title": "About this Specialization and Course"}, {"description": "Welcome! This week, you will learn how data analysts ask the right questions to ensure project success. By the end of this week, you will be able to: <p><ul><li>Craft the right questions to ensure your analysis projects succeed</li><li> Leverage questions to design logical and structured analysis plans</ul></li> <p> Remember to refer back to the Additional Resources reading: Identifying and Eliciting Information from Stakeholders). In addition, you will complete a  graded quiz. <p> As always, if you have any questions, post them to the <b>Discussions.</b> <p>To get started, please begin with the video “Tips for Becoming a Data Analyst.” <p>I hope you enjoy this week's materials!</p>", "video": ["Tips for Becoming a Data Analyst", "Asking the Right Questions", "Rock Projects", "S.M.A.R.T. Objectives", "Listening to Stakeholders During Elicitation", "Stakeholder Expectations Matter", "Week 1 Additional Resources", "SPAP Graphic", "Using SPAPs to Structure Your Thinking, Part 1", "Using SPAPs to Structure Your Thinking, Part 2", "Feedback Survey", "Week 1 Quiz"], "title": "Asking The \"Right Questions\""}, {"description": "Welcome to week 2! This week you'll install Tableau Desktop to learn how visualizing data helps you figure out what your data mean efficiently, and in the process of doing so, helps you narrow in on what factors you should take into consideration in your statistical models or predictive algorithms.  Over the next two weeks, we’re going to learn how to use Tableau to implement this type of visualization and to help you find, and communicate, answers to business questions, as well as work with the Tableau functions that all data analysts should be familiar with.  You will learn to install Tableau Desktop and learn to use the program by working with two data sets. In addition, through a series of practice exercises, you will use a data set to do example analyses and to answer specific sample questions about salaries for certain data-related jobs across the United State. Then for graded exercises, you will use a different data set to work out analyses and questions that will require you to directly apply the Tableau skills you have acquired through practice. <p>By the end of this week, you will be able to: <ul><li>Create the most important graphs used in business analysis and transform data in Tableau </li></ul><p>Once you have watched the \"Why Tableau\" video, review the \"Written Instructions to install Tableau Desktop\" and install the software. Remember to refer back to the Salary Data Set and to the Dognition Data Set resources posted on the course site this week. You will also complete a graded quiz at the end of the week. <p>As always, if you have any questions, post them to the <b>Discussions</b>.<p>To get started, please begin with the video “Use Data Visualization to Drive Your Analysis\" and then review the \"Written Instructions to install Tableau Desktop.<p>I hope you enjoy this week's materials! ", "video": ["Use Data Visualization to Drive Your Analysis", "Why Tableau?", "Written Instructions to Install Tableau Desktop", "Tableau Desktop Key", "Meet Your Salary Data", "Meet Your Dognition Data", "Our Analysis Plan", "Salary Data Set, Description, and Analysis Plan", "Dognition Data Set, Description, and Analysis Plan", "Salaries of Data-Related Jobs: Your First Graph", "Formatting and Exporting Your First Graph", "Digging Deeper Using the Rows and Columns Shelves", "The Effects of Outliers Video", "Understanding the Marks Card", "Removing Outliers Using Scatterplot and Filtering and Groups", "Analyzing Data-Related Salaries in Different States Using Filtering and Groups", "When to Use Line Graphs", "Dates as Hierarchical Dimensions or Measures", "Analyzing Data-Related Salaries Over Time Using Date Hierarchies", "Analyzing Data-Related Salaries Over Time Using Trend Lines", "Analysing Data-Related Salaries Over Time Using Box Plots", "Introduction to Linear Regression", "Week 2 Practice Exercises", "Feedback Survey", "Week 2 Quiz"], "title": "Data Visualization with Tableau"}, {"description": "Welcome to week 3! This week you'll continue learning how to use Tableau to answer data analysis questions. You will learn how to use Tableau to both find, and eventually communicate answers to business questions. You'll learn about the process of elicitation, and learn how to ensure your data story is not undermined by overgeneralization or bias and how to format your data charts to begin creating a compelling data story.  By the end of this week, you will be able to: <ul><li>Write calculations and equations in Tableau</li> <li>Publish online business dashboards with Tableau.</li></ul> <p>Remember to refer  to the additional resources for this week: “Examples of Tableau Dashboards and Stories” and \"Using Tableau Dashboards When You Don't Have To.\" <p>You will also complete a graded quiz. <p>As always, if you have any questions, post them to the Discussions. <p>To get started, please begin with the video “Customizing and Sharing New Data in Tableau.” <p>I hope you enjoy this week's materials!", "video": ["Customizing and Sharing New Data in Tableau", "Data Sets Needed in Week 3", "Tableau Calculation Types", "How to Write Calculations", "Calculations that Make Filtering More Efficient", "Identifying Companies that Pay Less than the Prevailing Wage", "Blending Price Parity Data with Our Salary Data", "Adjusting Data-related Salaries for Cost of Living", "Calculating Which States Have the Top Adjusted Salaries within Job Subcategories", "Using Parameters to Define Top States", "Calculating Which Companies Have the Top Adjusted Salaries within Job Subcategories", "Designing a Dashboard to Determine Where You should Apply for Data-related Job", "Visual Story Points in Tableau", "Week 3 Additional Resources: Examples of Tableau Dashboards and Stories", "Week 3 Practice Exercises", "Feedback Survey", "Week 3 Quiz"], "title": "Dynamic Data Manipulation and Presentation in Tableau"}, {"description": "Welcome to week 4! This week you will become a master at getting people to agree with your data-driven business recommendations as you learn to deliver a compelling business presentation.  You’ll learn about the insight from the intersection of visualization science and decision science, and what this means for you as a  data analyst, who seeks to design a compelling and effective business presentations. If you intend to affect people’s decisions, you need to influence where they look. This week we will review  a set of tools and concepts you can use to optimize your visualizations and your presentation style. You will soon  be a master at getting people to agree with your data-driven business recommendations! <p>By the end of this week, you will be able to:<p><ul><li>Tell stories with data</li> <li>Design effective slide presentations to showcase your data story, and</li> <li> Deliver compelling business presentations</ul></li><p> Remember to refer back to the Study Guide: Designing and Delivering Effective Presentations. You will also complete a graded quiz. <p>As always, if you have any questions, post them to the <b>Discussions</b>.<p> To get started, please begin with the video “Using Visualization to Influence Business Decisions.”<p> I hope you enjoy this week's materials!", "video": ["Using Visualization Science to Influence Business Decisions", "The Storyboarding Hourglass", "Making Your Data Story Come Alive", "Storyboarding Your Presentation", "The Best Stress-Testers are Teams", "Overgeneralization and Sample Bias", "Misinterpretations Due to Lack of Controls", "Correlation Does Not Equal Causation", "How Correlations Impact Business Decisions", "Choosing Visualizations for Story Points", "The Neuroscience of Visual Perception Can Make or Break Your Visualization", "Misinterpretations Caused by Colorbars", "Visual Contrast Directs Where Your Audience Looks", "Week 4 Additional Resources: Designing and Delivering an Effective Business Presentation", "Formatting Slides to Communicate Data Stories", "Formatting Presentations to Communicate Data Stories", "Delivering Your Data Story", "Feedback Survey", "Week 4 Quiz"], "title": "Your Communication Toolbox: Visualizations, Logic, and Stories"}, {"description": "Welcome to week 5! This week you will complete your final project. This assignment requires you to submit a recording of yourself giving a 4-5 minute presentation in which you present a data-driven business process change proposal to Dognition company management about how to increase the numbers of tests users complete. Students will give a short, peer-reviewed business presentation that uses a specified chart in Tableau. The final project will assess your mastery of the following: <p> <ul><li>Demonstrated understanding the Tableau functions discussed in this course</li><li>Adapting visualizations to make them maximally communicative</li><li>Storyboarding skills</li> <li>Translating your story into a presentation ready for the boardroom</li><li>Effective presentation delivery</li><li>Evaluating business presentations</ul></li><p> Remember to refer to the Background Information for Peer Review Assignment  on the course web site before you begin. This final course project is a comprehensive assessment covering all of the course material and will take approximately 6-8 hours to complete.<p>As always, if you have any questions, post them to the Discussions. Thank you for your contributions to this  final project!<p> ", "video": ["Background Information for Peer Review Assignment", "Feedback Survey", "Recommendations for Dognition Business Process Change"], "title": "Final Project"}], "title": "Data Visualization and Communication with Tableau"}, {"course_info": "About this course: Before you can work with data you have to get some. This course will cover the basic ways that data can be obtained. The course will cover obtaining data from the web, from APIs, from databases and from colleagues in various formats. It will also cover the basics of data cleaning and how to make data “tidy”. Tidy data dramatically speed downstream data analysis tasks. The course will also cover the components of a complete data set including raw data, processing instructions, codebooks, and processed data. The course will cover the basics needed for collecting, cleaning, and sharing data.", "level": null, "package_name": "Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "3", "teach_by": [{"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "In this first week of the course, we look at finding data and reading different file types.", "video": ["Welcome to Week 1", "Syllabus", "Pre-Course Survey", "Obtaining Data Motivation", "Raw and Processed Data", "Components of Tidy Data", "Downloading Files", "Reading Local Files", "Reading Excel Files", "Reading XML", "Reading JSON", "The data.table Package", "Practical R Exercises in swirl Part 1", "Week 1 Quiz"], "title": "Week 1"}, {"description": "Welcome to Week 2 of Getting and Cleaning Data! The primary goal is to introduce you to the most common data storage systems and the appropriate tools to extract data from web or from databases like MySQL. ", "video": ["Reading from MySQL", "Reading from HDF5", "Reading from The Web", "Reading From APIs", "Reading From Other Sources", "Week 2 Quiz"], "title": "Week 2"}, {"description": "Welcome to Week 3 of Getting and Cleaning Data! This week the lectures will focus on organizing, merging and managing the data you have collected using the lectures from Weeks 1 and 2. ", "video": ["Subsetting and Sorting", "Summarizing Data", "Creating New Variables", "Reshaping Data", "Managing Data Frames with dplyr - Introduction", "Managing Data Frames with dplyr - Basic Tools", "Merging Data", "Practical R Exercises in swirl Part 2", "swirl Lesson 1: Manipulating Data with dplyr", "swirl Lesson 2: Grouping and Chaining with dplyr", "swirl Lesson 3: Tidying Data with tidyr", "Week 3 Quiz"], "title": "Week 3"}, {"description": "Welcome to Week 4 of Getting and Cleaning Data! This week we finish up with lectures on text and date manipulation in R. In this final week we will also focus on peer grading of Course Projects. \n", "video": ["Editing Text Variables", "Regular Expressions I", "Regular Expressions II", "Working with Dates", "Data Resources", "Practical R Exercises in swirl Part 4", "swirl Lesson 1: Dates and Times with lubridate", "Post-Course Survey", "Week 4 Quiz", "Getting and Cleaning Data Course Project"], "title": "Week 4"}], "title": "Getting and Cleaning Data"}, {"course_info": "About this course: In this course, you will learn best practices for how to use data analytics to make any company more competitive and more profitable. You will be able to recognize the most critical business metrics and distinguish them from mere data.\n \nYou’ll get a clear picture of the vital but different roles business analysts, business data analysts, and data scientists each play in various types of companies. And you’ll know exactly what skills are required to be hired for, and succeed at, these high-demand jobs.\n \nFinally, you will be able to use a checklist provided in the course to score any company on how effectively it is embracing big data culture. Digital companies like Amazon, Uber and Airbnb are transforming entire industries through their creative use of big data. You’ll understand why these companies are so disruptive and how they use data-analytics techniques to out-compete traditional companies.", "level": null, "package_name": "Excel to MySQL: Analytic Techniques for Business Specialization ", "created_by": "Duke University", "package_num": "1", "teach_by": [{"name": "Daniel Egger", "department": "Pratt School of Engineering, Duke University"}, {"name": "Jana Schaich Borg", "department": "Social Science Research Institute"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "This Coursera Specialization: Excel to MySQL: Analytic Techniques for Business, is about how 'Big Data' interacts with business, and how to use data analytics to create value for businesses. This specialization consists of four courses and a final Capstone Project, where you will apply your skills to real-world business process. You will learn to perform sophisticated data-analysis functions using powerful software tools such as Microsoft Excel, Tableau, and MySQL. To learn more, watch the video and review the specialization overview document we provided. In the first course of the specialization: Business Metrics for Data-Driven Companies, you will be able to learn best practices for using data analytics to make any company more competitive and more profitable; learn to recognize the most critical business metrics and distinguish those from mere data; understand the vital but different roles business analysts, business data analysts, and data scientists each play in various types of companies; and know exactly the skills required to be hired for, and succeed at, these high-demand jobs. Finally, using a 20-item checklist for evaluating a business, you'll be able to score any company on how effectively it is embracing big data culture. Digital companies like Amazon, Uber and Airbnb are transforming entire industries through their creative use of big data. You’ll understand why these companies are so disruptive, and how they use data-analytics techniques to out-compete traditional companies.To get started, please begin with the video 'About This Specialization.'I hope you enjoy this week's materials!", "video": ["About This Specialization", "Specialization Overview", "Introduction", "Course Overview", "Feedback Survey Information"], "title": "About This Specialization and Course "}, {"description": "Welcome! This week we will explore business metrics - the critical numbers that help companies figure out how to survive and thrive. Inside every pile of data is a vital metric trying to get out! By the end of this week, you will be able to: distinguish business metrics from mere business data; identify critical business metrics such as cash flow, profitability, and online retail marketing metrics;  distinguish revenue, profitability and risk metrics; and distinguish traditional from dynamic metrics. Included in this week’s course materials is a Cash Flow and P&L statement for Egger’s Roast Coffee, as a supplemental document, so be sure to review it carefully and refer to the glossary for key information. ", "video": ["Metrics Help Us Ask the Right Questions", "Distinguishing Revenue, Profitability, and Risk Metrics", "Distinguishing Traditional and Dynamic Metrics", "Egger's Roast Coffee Cash Flow and P&L Statements", "Egger’s Roast Coffee Case Study Part 1 – Definitions", "Egger’s Roast Coffee Case Study Part 2 – How a Profitable, Growing Company can go Bankrupt", "Revenue Metrics – Traditional Enterprise Sales Funnel", "Revenue Metrics - Amazon.com as a Leading Example of Use of Dynamic Metrics - Part 1", "Revenue Metrics - Amazon.com as a Leading Example of Use of Dynamic Metrics - Part 2", "Profitability/Efficiency Metrics: Inventory Management", "Profitability/Efficiency Metrics: Hotel Room Occupancy Optimization", "Risk Metrics: Leverage and Reputational Risk", "Feedback Survey", "Business Metrics"], "title": "Introducing Business Metrics"}, {"description": "Welcome!  This week, we will meet some great people - all former students of mine - now working at super-interesting and exciting jobs as business analysts, business data analysts, or data scientists. We’ll explore what they do, how their role relates to big data, and the skills they needed to get hired! Our hope is this information will give you a better understanding of the type of data-related job you might apply for once you've completed this specialization, and a sense of the type of company you would find most appealing to work for. By the end of this week, you will be able to:  differentiate among different job roles within a company that work with data; identify how each role works with data; and describe the skills required to perform each job role. You will differentiate how different types of companies relate to big data culture, and rank any company according to a 20-item checklist. You will also learn to differentiate how different types of companies relate to big data culture. Included in this week’s materials is a 20-item checklist to rank companies. This week also includes in-video polls so you can see how others are ranking their businesses.", "video": ["Roles and Companies as They Relate to Big Data", "The Business Analyst", "An Interview with Business Analyst Shambhavi Vashishtha", "Distinguishing the Business Data Analyst and Business Analyst Roles", "An Interview with Business Data Analyst Tiffany Yu", "Summary of Job Requirements for Data-Centric Roles", "The Data Scientist", "An Interview with Data Scientist Dai Li", "The Senior Software Engineer", "Overview of 5 Types of Companies as They Relate to Big Data", "Traditional Strategic Business Consulting", "Bricks-and-Mortar Companies", "Barnes and Noble Case Study", "20-Item Checklist for Evaluating A Business", "Strategic Business Consulting - Focus on Software/IT Systems Integration", "Hardware and Software Companies", "Digital Companies", "Feedback Survey", "Working in the Business Data Analytics Marketplace"], "title": "Working in the Business Data Analytics Marketplace"}, {"description": "Welcome! This week we’re going to go deeper into the critically-important metrics for web marketing - metrics every type of business needs to understand in order to survive. We’ll dive into the 'vertical' market of financial services - where digital companies are threatening to take away the market from traditional 'brick-and- mortar' companies.By the end of this week, you will be able to: Identify critical business metrics for all companies engaged in web-based marketing; and identify critical business metrics for financial services companies. You’ll find additional website links that expand some of the course materials covered in this week’s video lectures. ", "video": ["Web Marketing - Metrics", "Web Marketing - AdWord Metrics", "Web Marketing - Segmentation", "Links Cited In AdWords Metric Video", "Financial Services Metrics - Money Management and Investing", "The Equivalence of Different Returns –The Sharpe Ratio", "Four Types of Money Managers and Their Performance Metrics", "Venture Capital and Private Equity Investors", "Feedback Survey", "Going Deeper into Business Metrics"], "title": "Going Deeper into Business Metrics "}, {"description": "This week contains the final course assignment, a peer assessment in which you will identify business metrics of interest in a case example, describe those metrics, and propose a business process change that could be supported by the metric chosen.", "video": ["A Look Ahead", "Feedback survey", "Articulating Business Metrics in a Business Case Study"], "title": "Applying Business Metrics to a Business Case Study"}], "title": "Business Metrics for Data-Driven Companies"}, {"course_info": "About this course: This course will introduce the learner to text mining and text manipulation basics. The course begins with an understanding of how text is handled by python, the structure of text both to the machine and to humans, and an overview of the nltk framework for manipulating text. The second week focuses on common manipulation needs, including regular expressions (searching for text), cleaning text, and preparing text for use by machine learning processes. The third week will apply basic natural language processing methods to text, and demonstrate how text classification is accomplished. The final week will explore more advanced methods for detecting the topics in documents and grouping them by similarity (topic modelling). \n\nThis course should be taken after: Introduction to Data Science in Python, Applied Plotting, Charting & Data Representation in Python, and Applied Machine Learning in Python.", "level": "Intermediate", "package_name": "Applied Data Science with Python Specialization ", "created_by": "University of Michigan", "package_num": "4", "teach_by": [{"name": "V. G. Vinod Vydiswaran", "department": "School of Information"}], "target_audience": "Who is this class for: This course is part of “Applied Data Science with Python“ and is intended for learners who have basic python or programming background, and want to apply statistics, machine learning, information visualization, social network analysis, and text analysis techniques to gain new insight into data.\n\nOnly minimal statistics background is expected, and the first course contains a refresh of these basic concepts. There are no geographic restrictions. Learners with a formal training in Computer Science but without formal training in data science will still find the skills they acquire in these courses valuable in their studies and careers.", "rating": "4.0", "week_data": [{"description": "", "video": ["Course Syllabus", "Help us learn more about you!", "Introduction to Text Mining", "Handling Text in Python", "Notice for Auditing Learners: Assignment Submission", "Working with Text", "Regular Expressions", "Regex with Pandas and Named Groups", "Demonstration: Regex with Pandas and Named Groups", "Practice Quiz", "Internationalization and Issues with Non-ASCII Characters", "Introduce Yourself", "Resources: Common issues with free text", "Assignment 1", "Module 1 Quiz", "Assignment 1 Submission"], "title": "Module 1: Working with Text in Python"}, {"description": "", "video": ["Basic Natural Language Processing", "Module 2 (Python 3)", "Basic NLP tasks with NLTK", "Advanced NLP tasks with NLTK", "Practice Quiz", "Finding your own prepositional phrase attachment", "Assignment 2", "Module 2 Quiz", "Assignment 2 Submission"], "title": "Module 2: Basic Natural Language Processing"}, {"description": "", "video": ["Text Classification", "Identifying Features from Text", "Naive Bayes Classifiers", "Naive Bayes Variations", "Support Vector Machines", "Learning Text Classifiers in Python", "Case Study - Sentiment Analysis", "Demonstration: Case Study - Sentiment Analysis", "Assignment 3", "Module 3 Quiz", "Assignment 3 Submission"], "title": "Module 3: Classification of Text"}, {"description": "", "video": ["Semantic Text Similarity", "Topic Modeling", "Generative Models and LDA", "Practice Quiz", "Information Extraction", "Additional Resources & Readings", "Assignment 4", "Post-Course Survey", "Module 4 Quiz", "Assignment 4 Submission"], "title": "Module 4: Topic Modeling"}], "title": "Applied Text Mining in Python"}, {"course_info": "About this course: This course will introduce you to the wonderful world of Python programming!  We'll learn about the essential elements of programming and how to construct basic Python programs. We will cover expressions, variables, functions, logic, and conditionals, which are foundational concepts in computer programming. We will also teach you how to use Python modules, which enable you to benefit from the vast array of functionality that is already a part of the Python language. These concepts and skills will help you to begin to think like a computer programmer and to understand how to go about writing Python programs.\n\nBy the end of the course, you will be able to write short Python programs that are able to accomplish real, practical tasks. This course is the foundation for building expertise in Python programming. As the first course in a specialization, it provides the necessary building blocks for you to succeed at learning to write more complex Python programs.\n\nThis course uses Python 3.  While many Python programs continue to use Python 2, Python 3 is the future of the Python programming language. This first course will use a Python 3 version of the CodeSkulptor development environment, which is specifically designed to help beginning programmers learn quickly.  CodeSkulptor runs within any modern web browser and does not require you to install any software, allowing you to start writing and running small programs immediately.  In the later courses in this specialization,  we will help you to move to more sophisticated desktop development environments.", "level": "Beginner", "package_name": "Introduction to Scripting in Python Specialization ", "created_by": "Rice University", "package_num": "1", "teach_by": [{"name": "Scott Rixner", "department": "Computer Science"}, {"name": "Joe Warren", "department": "Computer Science"}], "target_audience": "Who is this class for: This class is primarily for people with no prior programming experience who would like to learn Python.", "rating": "4.7", "week_data": [{"description": "This module will expose you to Python so that you can run your first simple programs.  You will use Python to compute the results of arithmetic expressions, as you would when using a calculator.", "video": ["Welcome!", "Course Overview", "Hello, world!", "Python Development Environments", "Using CodeSkulptor3", "Numbers", "Simple Expressions", "Compound Expressions", "Variables and Assignment", "Handling Errors in Python", "A Short Guide to Common Errors in Python", "Practice Exercise for Expressions", "Practice Exercises for Variables and Assignments", "Basic Python Syntax"], "title": "Python as a Calculator"}, {"description": "This module will teach you how to define and call functions. Functions allow you to write code once that you can execute repeatedly with different inputs.", "video": ["What is a Function?", "Calling Functions", "Defining Functions", "Local Variables", "Functions", "Understanding Function Evaluation", "Using Print and Return in Functions", "Practice Exercises for Functions", "Functions"], "title": "Functions"}, {"description": "This module will teach you how to use logic and conditionals to change the behavior of the program based upon values within the program.", "video": ["Boolean Logic", "Logical Expressions", "Comparisons", "Conditionals", "More Conditionals", "Conditionals in Python", "Using Python Documentation", "Following Coding Standards", "Coding Standards and Style", "Practice Exercises for Logic and Conditionals", "Logic and Conditionals"], "title": "Logic and Conditionals"}, {"description": "This module will introduce you to the concept of modules. Python modules allow code to be divided up into different files and reused in different programs.  Python provides many modules that you can use within your programs.", "video": ["Python Modules", "Python Modules", "The Datetime Module", "Datetime Module Quick Reference", "Coding the Practice Project - Part 1", "Coding the Practice Project - Part 2", "Practice Project: Rock-Paper-Scissors-Lizard-Spock", "RPSLS Video", "Tips for RPSLS", "Project Video", "Project Description: Working with Dates", "OwlTest: Automated Feedback and Assessment", "Project Submission History", "Project: Working with Dates"], "title": "Python Modules"}], "title": "Python Programming Essentials"}, {"course_info": "About this course: The use of Excel is widespread in the industry. It is a very powerful data analysis tool and almost all big and small businesses use Excel in their day to day functioning. This is an introductory course in the use of Excel and is designed to give you a working knowledge of Excel with the aim of getting to use it for more advance topics in Business Statistics later. The course is designed keeping in mind two kinds of learners -  those who have very little functional knowledge of Excel and those who use Excel regularly but at a peripheral level and wish to enhance their skills. The course takes you from basic operations such as reading data into excel using various data formats, organizing and manipulating data, to some of the more advanced functionality of Excel. All along, Excel functionality is introduced using easy to understand examples which are demonstrated in a way that learners can become comfortable in understanding and applying them.\n\nTo successfully complete course assignments, students must have access to a Windows version of Microsoft Excel 2010 or later. \n________________________________________\nWEEK 1\nModule 1: Introduction to Spreadsheets\nIn this module, you will be introduced to the use of Excel spreadsheets and various basic data functions of Excel.\n\nTopics covered include:\n•\tReading data into Excel using various formats\n•\tBasic functions in Excel, arithmetic as well as various logical functions\n•\tFormatting rows and columns\n•\tUsing formulas in Excel and their copy and paste using absolute and relative referencing\n________________________________________\nWEEK 2\nModule 2: Spreadsheet Functions to Organize Data\nThis module introduces various Excel functions to organize and query data. Learners are introduced to the IF, nested IF, VLOOKUP and the HLOOKUP functions of Excel. \n\nTopics covered include:\n•\tIF and the nested IF functions\n•\tVLOOKUP and HLOOKUP\n•\tThe RANDBETWEEN function\n________________________________________\nWEEK 3\nModule 3: Introduction to Filtering, Pivot Tables, and Charts\nThis module introduces various data filtering capabilities of Excel. You’ll learn how to set filters in data to selectively access data. A very powerful data summarizing tool, the Pivot Table, is also explained and we begin to introduce the charting feature of Excel.\n\nTopics covered include:\n•\tVLOOKUP across worksheets\n•\tData filtering in Excel\n•\tUse of Pivot tables with categorical as well as numerical data\n•\tIntroduction to the charting capability of Excel\n________________________________________\nWEEK 4\nModule 4: Advanced Graphing and Charting\nThis module explores various advanced graphing and charting techniques available in Excel. Starting with various line, bar and pie charts we introduce pivot charts, scatter plots and histograms. You will get to understand these various charts and get to build them on your own.\n\nTopics covered include\n•\tLine, Bar and Pie charts\n•\tPivot charts\n•\tScatter plots\n•\tHistograms", "level": null, "package_name": "Business Statistics and Analysis Specialization ", "created_by": "Rice University", "package_num": "1", "teach_by": [{"name": "Sharad Borle", "department": "Jones Graduate School of Business"}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "Introduction to spreadsheets, reading data, manipulating data. Basic spreadsheet operations and functions.", "video": ["Meet the Professor", "Course FAQs", "Pre-Course Survey", "Reading Data into Excel", "Store Sales (Comma delimited).txt", "Store Sales (Fixed width).txt", "Stores Sales (Tab delimited Text).txt", "Reading Data into Excel", "Basic Data Manipulation in Excel", "Basic Data Manipulation in Excel", "Arithmetic Manipulation in Excel", "Arithmetic Manipulation in Excel", "Basic Functions in Excel", "Basic Functions in Excel", "Functions Using Absolute and Relative References", "Functions Using Absolute and Relative References", "Functions Explained", "Functions Explained", "Week 1 Recap", "Introduction to Spreadsheets"], "title": "Introduction to Spreadsheets"}, {"description": "Introduction to some more useful functions such as the IF, nested IF, VLOOKUP and HLOOKUP functions in Excel.", "video": ["The “IF” Command in Excel", "The \"IF\" Command in Excel", "The “IF” Command in Excel Using Numerical Data", "The “IF” Command in Excel Using Numerical Data", "The \"Nested IF\" Command in Excel", "The \"Nested IF\" Command in Excel", "The \"VLOOKUP\" Function in Excel", "The \"VLOOKUP\" Function in Excel", "Another \"VLOOKUP\" Example", "Another \"VLOOKUP\" Example", "The \"HLOOKUP\" Function in Excel", "The \"HLOOKUP\" Function in Excel", "Professor 'Know-it-all' Needs Help!", "Professor 'Know-it-all' Needs Help!", "Week 2 Recap", "Spreadsheet Functions to Organize Data"], "title": "Spreadsheet Functions to Organize Data"}, {"description": "Introduction to the Data filtering capabilities of Excel, the construction of Pivot Tables to organize data and introduction to charts in Excel.", "video": ["Using the “VLOOKUP” Function Across Worksheets", "Using the “VLOOKUP” Function Across Worksheets", "Data Filtering in Excel", "Data Filtering in Excel", "Use of Pivot Tables in Excel", "Use of Pivot Tables in Excel", "More Pivot Table Options", "More Pivot Table Options", "Application of Pivot Tables to Numeric Data", "Application of Pivot Tables to Numeric Data", "Introduction to Charts in Excel", "Introduction to Charts in Excel", "Week 3 Recap", "Introduction to Filtering, Pivot Tables, and Charts"], "title": "Introduction to Filtering, Pivot Tables, and Charts"}, {"description": "Constructing various Line, Bar and Pie charts. Using the Pivot chart features of Excel. Understanding and constructing Histograms and Scatterplots.", "video": ["Line Graphs", "Line Graphs", "Bar Graphs and Pie Charts", "Bar Graphs and Pie Charts", "Pivot Charts", "Pivot Charts", "Scatter Plots", "Scatter Plots", "Histograms Part 1", "Histograms Part 1", "Histograms Part 2", "Histograms Part 2", "Week 4 Recap", "End-of-Course Survey", "Advanced Graphing and Charting"], "title": "Advanced Graphing and Charting"}], "title": "Introduction to Data Analysis Using Excel"}, {"course_info": "About this course: How can you put data to work for you? Specifically, how can numbers in a spreadsheet tell us about present and past business activities, and how can we use them to forecast the future? The answer is in building quantitative models, and this course is designed to help you understand the fundamentals of this critical, foundational, business skill. Through a series of short lectures, demonstrations, and assignments, you’ll learn the key ideas and process of quantitative modeling so that you can begin to create your own models for your own business or enterprise. By the end of this course, you will have seen a variety of practical commonly used quantitative models as well as the building blocks that will allow you to start structuring your own models. These building blocks will be put to use in the other courses in this Specialization.", "level": null, "package_name": "Business and Financial Modeling Specialization ", "created_by": "University of Pennsylvania", "package_num": "1", "teach_by": [{"name": "Richard Waterman", "department": "Statistics-Wharton School"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "In this module, you will learn how to define a model, and how models are commonly used. You’ll examine the central steps in the modeling process, the four key mathematical functions used in models, and the essential vocabulary used to describe models. By the end of this module, you’ll be able to identify the four most common types of models, and how and when they should be used. You’ll also be able to define and correctly use the key terms of modeling, giving you not only a foundation for further study, but also the ability to ask questions and participate in conversations about quantitative models.", "video": ["1.1 Course Introduction", "1.2 Definition and Uses of Models, Common Functions", "1.3 How Models Are Used in Practice", "1.4 Key Steps in the Modeling Process", "1.5 A Vocabulary for Modeling", "1.6 Mathematical Functions", "1.7 Summary", "PDF of Lecture Slides", "Module 1: Introduction to Models Quiz"], "title": "Module 1: Introduction to Models "}, {"description": "This module introduces linear models, the building block for almost all modeling. Through close examination of the common uses together with examples of linear models, you’ll learn how to apply linear models, including cost functions and production functions to your business. The module also includes a presentation of growth and decay processes in discrete time, growth and decay in continuous time, together with their associated present and future value calculations. Classical optimization techniques are discussed. By the end of this module, you’ll be able to identify and understand the key structure of linear models, and suggest when and how to use them to improve outcomes for your business. You’ll also be able to perform present value calculations that are foundational to valuation metrics. In addition, you will understand how you can leverage models for your business, through the use of optimization to really fine tune and optimize your business functions.\n\n", "video": ["2.1 Introduction to Linear Models and Optimization", "2.2 Growth in Discrete Time", "2.3 Constant Proportionate Growth", "2.4 Present and Future Value", "2.5 Optimization", "2.6 Summary", "PDF of Lecture Slides", "Module 2: Linear Models and Optimization Quiz"], "title": "Module 2: Linear Models and Optimization"}, {"description": "This module explains probabilistic models, which are ways of capturing risk in process. You’ll need to use probabilistic models when you don’t know all of your inputs. You’ll examine how probabilistic models incorporate uncertainty, and how that uncertainty continues through to the outputs of the model. You’ll also discover how propagating uncertainty allows you to determine a range of values for forecasting. You’ll learn the most-widely used models for risk, including regression models, tree-based models, Monte Carlo simulations, and Markov chains, as well as the building blocks of these probabilistic models, such as random variables, probability distributions, Bernoulli random variables, binomial random variables, the empirical rule, and perhaps the most important of all of the statistical distributions, the normal distribution, characterized by mean and standard deviation. By the end of this module, you’ll be able to define a probabilistic model, identify and understand the most commonly used probabilistic models, know the components of those models, and determine the most useful probabilistic models for capturing and exploring risk in your own business.", "video": ["3.1 Introduction to Probabilistic Models", "3.2 Examples of Probabilistic Models", "3.3 Regression Models", "3.4 Probability Trees", "3.5 Monte Carlo Simulations", "3.6 Markov Chain Models", "3.7 Building Blocks of Probability Models", "3.8 The Bernoulli Distribution", "3.9 The Binomial Distribution", "3.10 The Normal Distribution", "3.11 The Empirical Rule", "3.12 Summary", "PDF of Lecture Slides", "Module 3: Probabilistic Models Quiz"], "title": "Module 3: Probabilistic Models"}, {"description": "This module explores regression models, which allow you to start with data and discover an underlying process. Regression models are the key tools in predictive analytics, and are also used when you have to incorporate uncertainty explicitly in the underlying data.  You’ll learn more about what regression models are, what they can and cannot do, and the questions regression models can answer. You’ll examine correlation and linear association, methodology to fit the best line to the data, interpretation of regression coefficients, multiple regression, and logistic regression. You’ll also see how logistic regression will allow you to estimate probabilities of success. By the end of this module, you’ll be able to identify regression models and their key components, understand when they are used, and be able to interpret them so that you can discuss your model and convince others that your model makes sense, with the ultimate goal of implementation.", "video": ["4.1 Introduction to Regression Model", "4.2 Use of Regression Models", "4.3 Interpretion of Regression Coefficients", "4.4 R-squared and Root Mean Squared Error (RMSE)", "4.5 Fitting Curves to Data", "4.6 Multiple Regression", "4.7 Logistic Regression", "4.8 Summary of Regression Models", "PDF of Lecture Slides", "Module 4: Regression Models Quiz"], "title": "Module 4: Regression Models"}], "title": "Fundamentals of Quantitative Modeling"}, {"course_info": "About this course: Data about our browsing and buying patterns are everywhere.  From credit card transactions and online shopping carts, to customer loyalty programs and user-generated ratings/reviews, there is a staggering amount of data that can be used to describe our past buying behaviors, predict future ones, and prescribe new ways to influence future purchasing decisions. In this course, four of Wharton’s top marketing professors will provide an overview of key areas of customer analytics: descriptive analytics, predictive analytics, prescriptive analytics, and their application to real-world business practices including Amazon, Google, and Starbucks to name a few. This course provides an overview of the field of analytics so that you can make informed business decisions. It is an introduction to the theory of customer analytics, and is not intended to prepare learners to perform customer analytics. \n\nCourse Learning Outcomes: \n\nAfter completing the course learners will be able to...\n\nDescribe the major methods of customer data collection used by companies and understand how this data can inform business decisions\n\nDescribe the main tools used to predict customer behavior and identify the appropriate uses for each tool \n\nCommunicate key ideas about customer analytics and how the field informs business decisions\n\nCommunicate the history of customer analytics and latest best practices at top firms", "level": null, "package_name": "Business Analytics Specialization ", "created_by": "University of Pennsylvania", "package_num": "1", "teach_by": [{"name": "Eric Bradlow", "department": "The Wharton School"}, {"name": "Peter Fader", "department": "The Wharton School"}, {"name": "Raghu Iyengar", "department": "The Wharton School"}, {"name": "Ron Berman", "department": "The Wharton School"}], "target_audience": null, "rating": null, "week_data": [{"description": "What is Customer Analytics? How is this course structured? What will I learn in this course? What will I learn in the Business Analytics Specialization? These short videos will give you an overview of this course and the specialization; the substantive lectures begin in Week 2.", "video": ["Course Introduction and Overview", "Overview of the Business Analytics Specialization"], "title": "Introduction to Customer Analytics"}, {"description": "\nIn this module, you’ll learn what data can and can’t describe about customer behavior as well as the most effective methods for collecting data and deciding what it means.  You’ll understand the critical difference between data which describes a causal relationship and data which describes a correlative one as you explore the synergy between data and decisions, including the principles for systematically collecting and interpreting data to make better business decisions. You’ll also learn how data is used to explore a problem or question, and how to use that data to create products, marketing campaigns, and other strategies. By the end of this module, you’ll have a solid understanding of effective data collection and interpretation so that you can use the right data to make the right decision for your company or business.\n", "video": ["What is Descriptive Analytics?", "Descriptive Data Collection: Survey Overview", "Descriptive Data Collection: Net Promoter Score and Self-Reports", "Descriptive Data Collection: Survey Design", "Passive Data Collection", "Media Planning", "Causal Data Collection and Summary", "Descriptive Analytics Slides", "Additional Readings: Descriptive Analytics", "Descriptive Analytics Quiz"], "title": "Descriptive Analytics"}, {"description": "Once you’ve collected and interpreted data, what do you do with it? In this module, you’ll learn how to take the next step: how to use data about actions in the past to make to make predictions about actions in the future. You’ll examine the main tools used to predict behavior, and learn how to determine which tool is right for which decision purposes. Additionally, you’ll learn the language and the frameworks for making predictions of future behavior. At the end of this module, you’ll be able to determine what kinds of predictions you can make to create future strategies, understand the most powerful techniques for predictive models including regression analysis, and be prepared to take  full advantage of analytics to create effective data-driven business decisions.", "video": ["Introduction to Predictive Analytics", "Asking Predictive Questions", "Regression Analysis, Part 1: The Demand Curve", "Regression Analysis, Part 2: Making Predictions", "Beyond Period 2", "Making Predictions Using a Data Set", "Data Set Predictions: Mary, Sharmila, or Chris?", "Probability Models", "Implementation of the Model", "Results and Predictions", "Reading: Customer Lifetime Value", "Predictive Analytics and Regression Analysis Slides", "Implementation of the Model Spreadsheet", "Additional Readings: Predictive Analytics", "Predictive Analytics Quiz"], "title": "Predictive Analytics"}, {"description": "How do you turn data into action? In this module, you’ll learn how prescriptive analytics provide recommendations for actions you can take to achieve your business goals. First, you’ll explore how to ask the right questions, how to define your objectives, and how to optimize for success. You’ll also examine critical examples of prescriptive models, including how quantity is impacted by price, how to maximize revenue, how to maximize profits, and how to best use online advertising. By the end of this module, you’ll be able to define a problem, define a good objective, and explore models for optimization which take competition into account, so that you can write prescriptions for data-driven actions that create success for your company or business.", "video": ["Introduction", "What is Prescriptive Analytics?", "Using the Data to Maximize Revenue", "Parameters of the Model", "Market Structure", "Competition and Online Advertising Models", "Conclusion(s)", "Prescriptive Analytics Slides", "Prescriptive Analytics Quiz"], "title": "Prescriptive Analytics"}, {"description": "How do top firms put data to work? In this module, you’ll learn how successful businesses use data to create cutting-edge, customer-focused marketing practices. You’ll explore real-world examples of the five-pronged attack to apply customer analytics to marketing, starting with data collection and data exploration, moving toward building predictive models and optimization, and continuing all the way to data-driven decisions. At the end of this module, you’ll know the best way to put data to work in your own company or business, based on the most innovative and effective data-driven practices of today’s top firms.", "video": ["Introduction to Application to Analytics", "The Future of Marketing is Business Analytics", "The Golden Age of Marketing", "Applications: ROI", "Radically New Data Sets in Marketing", "The Perils of Efficiency", "Analytics Applied: Kohl's, NetFlix, AmEx and more", "Conclusion", "Application/Case Study Slides", "Additional Reading: the Power of Data", "Applications"], "title": "Application/Case Studies"}], "title": "Customer Analytics"}, {"course_info": "About this course: Have you ever heard about such technologies as HDFS, MapReduce, Spark? Always wanted to learn these new tools but missed concise starting material? Don’t miss this course either!\n \nIn this 6-week course you will:\n- learn some basic technologies of the modern Big Data landscape, namely: HDFS, MapReduce and Spark;\n- be guided both through systems internals and their applications;\n- learn about distributed file systems, why they exist and what function they serve;\n- grasp the MapReduce framework, a workhorse for many modern Big Data applications;\n- apply the framework to process texts and solve sample business cases;\n- learn about Spark, the next-generation computational framework;\n- build a strong understanding of Spark basic concepts;\n- develop skills to apply these tools to creating solutions in finance, social networks, telecommunications and many other fields.\n\nYour learning experience will be as close to real life as possible with the chance to evaluate your practical assignments on a real cluster. No mocking, a friendly considerate atmosphere to make the process of your learning smooth and enjoyable.\n \nGet ready to work with real datasets alongside with real masters!\n\nSpecial thanks to:\n- Prof. Mikhail Roytberg, APT dept., MIPT, who was the initial reviewer of the project, the supervisor and mentor of half of the BigData team. He was the one, who helped to get this show on the road.\n- Oleg Sukhoroslov (PhD, Senior Researcher at IITP RAS), who has been teaching  MapReduce, Hadoop  and friends since 2008. Now he is leading the infrastructure team.\n- Oleg Ivchenko (PhD student APT dept., MIPT), Pavel Akhtyamov (MSc. student at APT dept., MIPT) and Vladimir Kuznetsov (Assistant at P.G. Demidov Yaroslavl State University), superbrains who have developed and now maintain the infrastructure used for practical assignments in this course.\n- Asya Roitberg, Eugene Baulin, Marina Sudarikova. These people never sleep to babysit this course day and night, to make your learning experience productive, smooth and exciting.", "level": "Intermediate", "package_name": "Big Data for Data Engineers Specialization ", "created_by": "Yandex", "package_num": "1", "teach_by": [{"name": "Ivan Puzyrevskiy", "department": null}, {"name": "Alexey A. Dral", "department": "Algorithms and Programming Technologies dept. MIPT"}, {"name": "Emeli Dral ", "department": null}, {"name": "Evgeniy Ryabenko", "department": null}], "target_audience": "Who is this class for: This course is aimed to everybody, who feel interest in Big Data. As the technologies covered throughout the course operate in Unix environment, we expect you to have basic understanding of the subject. Things like processes and files assumed to be familiar for the learner. Python is required to complete programming assignments.", "rating": "4.1", "week_data": [{"description": "", "video": ["Why BigData?", "Issues BigData can solve", "BigData Applications", "What is BigData Essentials?", "Course Structure", "Meet Emeli", "Meet Alexey", "Meet Ivan"], "title": "Welcome"}, {"description": "", "video": ["File system exploration", "File system managing", "File content exploration 1", "File content exploration 2", "Processes", "Basic Bash Commands", "HDFS Lesson Introduction", "Scaling Distributed File System", "Block and Replica States, Recovery Process 1", "Block and Replica States, Recovery Process 2", "HDFS Client", "Gentle Introduction into \"curl\"", "Web UI, REST API", "Namenode Architecture", "Distributed File Systems", "Introduction", "Text formats", "Binary formats 1", "Binary formats 2", "File formats extra (optional)", "Compression", "Demo Assignment", "How to submit your first assignment", "HDFS CLI Playground", "Rate this week", "Big Data and Distributed File Systems", "Demo Assignment", "Distributed File Systems"], "title": "What are BigData and distributed file systems (e.g. HDFS)?"}, {"description": "", "video": ["Unreliable Components 1", "Unreliable Components 2", "MapReduce", "Distributed Shell", "Fault Tolerance", "Fault Tolerance. Live Demo", "Hadoop MapReduce Intro", "Streaming", "Streaming in Python", "WordCount in Python", "Distributed Cache", "Environment, Counters", "Testing", "MapReduce Streaming", "MapReduce Features", "Combiner", "Partitioner", "Comparator", "Speculative Execution / Backup Tasks", "Compression", "Hadoop Streaming assignments", "Rate this week", "Hadoop Streaming Final"], "title": "Solving Problems with MapReduce"}, {"description": "", "video": ["Hadoop Streaming assignments", "Rate this week", "Hadoop Streaming assignment 0: Word Count", "Hadoop Streaming assignment 1: Words Rating", "Hadoop Streaming assignment 2: Stop Words", "Hadoop Streaming assignment 3: Name Count", "Hadoop Streaming assignment 4: Word Groups"], "title": "Solving Problems with MapReduce (practice week)"}, {"description": "", "video": ["Welcome", "RDDs", "Transformations 1", "Transformations 2", "Actions", "Resiliency", "Execution & Scheduling", "Caching & Persistence", "Broadcast variables", "Accumulator variables", "Getting started with Spark & Python", "Working with text files", "Joins", "Broadcast & Accumulator variables", "Spark UI", "Cluster mode", "Spark Assignments Intro", "Rate this week", "Lesson 1 Quiz", "Lesson 2 Quiz"], "title": " Introduction to Apache Spark"}, {"description": "", "video": ["Spark assignments Intro", "Rate this week", "Building an intuition behind the PMI definition", "Spark assignment 1: Pairs", "Spark assignment 2: Collocations"], "title": "Introduction to Apache Spark (practice week)"}, {"description": "", "video": ["Sampling", "Estimating proportions", "Means", "Medians", "Data and code", "Map and Reduce Side Joins", "Tabular Data, KeyFieldSelection", "Data Skew, Salting", "telecom-sms-call-internet-mi Dataset", "Advanced MapReduce Techniques", "Twitter graph case study", "Shortest path", "Jupyter Notebook Sandbox", "Rate this week", "Sample estimates", "Reconstructing the path", "Real-World Applications", "Real-World Applications: TF-IDF"], "title": "Real-World Applications"}], "title": "Big Data Essentials: HDFS, MapReduce and Spark RDD"}, {"course_info": "About this course: Important: The focus of this course is on math - specifically, data-analysis concepts and methods - not on Excel for its own sake. We use Excel to do our calculations, and all math formulas are given as Excel Spreadsheets, but we do not attempt to cover Excel Macros, Visual Basic, Pivot Tables, or other intermediate-to-advanced Excel functionality.\n\nThis course will prepare you to design and implement realistic predictive models based on data. In the Final Project (module 6) you will assume the role of a business data analyst for a bank, and develop two different predictive models to determine which applicants for credit cards should be accepted and which rejected. Your first model will focus on minimizing default risk, and your second on maximizing bank profits. The two models should demonstrate to you in a practical, hands-on way the idea that your choice of business metric drives your choice of an optimal model.\n\nThe second big idea this course seeks to demonstrate is that your data-analysis results cannot and should not aim to eliminate all uncertainty. Your role as a data-analyst is to reduce uncertainty for decision-makers by a financially valuable increment, while quantifying how much uncertainty remains. You will learn to calculate and apply to real-world examples the most important uncertainty measures used in business, including classification error rates, entropy of information, and confidence intervals for linear regression.\n\nAll the data you need is provided within the course, all assignments are designed to be done in MS Excel, and you will learn enough Excel to complete all assignments. The course will give you enough practice with Excel to become fluent in its most commonly used business functions, and you’ll be ready to learn any other Excel functionality you might need in the future (module 1).\n\nThe course does not cover Visual Basic or Pivot Tables and you will not need them to complete the assignments. All advanced concepts are demonstrated in individual Excel spreadsheet templates that you can use to answer relevant questions. You will emerge with substantial vocabulary and practical knowledge of how to apply business data analysis methods based on binary classification (module 2), information theory and entropy measures (module 3), and linear regression (module 4 and 5), all using no software tools more complex than Excel.", "level": null, "package_name": "Excel to MySQL: Analytic Techniques for Business Specialization ", "created_by": "Duke University", "package_num": "2", "teach_by": [{"name": "Jana Schaich Borg", "department": "Social Science Research Institute"}, {"name": "Daniel Egger", "department": "Pratt School of Engineering, Duke University"}], "target_audience": null, "rating": "4.2", "week_data": [{"description": "This course will prepare you to design and implement realistic predictive models based on data. In the Final Project (module 6) you will assume the role of a business data analyst for a bank, and develop two different predictive models to determine which applicants for credit cards should be accepted and which rejected.  Your first model will focus on minimizing default risk, and your second on maximizing bank profits. The two models should demonstrate to you in a practical, hands-on way the idea that your choice of business metric drives your choice of an optimal model.The second big idea this course seeks to demonstrate is that your data-analysis results cannot and should not aim to eliminate all uncertainty.  Your role as a data-analyst is to reduce uncertainty for decision-makers by a financially valuable increment, while quantifying how much uncertainty remains. You will learn to calculate and apply to real-world examples the most important uncertainty measures used in business, including classification error rates, entropy of information, and confidence intervals for linear regression. All the data you need is provided within the course, and all assignments are designed to be done in MS Excel. The course will give you enough practice with Excel to become fluent in its most commonly used business functions, and you’ll be ready to learn any other Excel functionality you might need in future (module 1). The course does not cover Visual Basic or Pivot Tables and you will not need them to complete the assignments. All advanced concepts are demonstrated in individual Excel spreadsheet templates that you can use to answer relevant questions. You will emerge with substantial vocabulary and practical knowledge of how to apply business data analysis methods based on binary classification (module 2), information theory and entropy measures (module 3), and linear regression (module 4 and 5), all using no software tools more  complex than Excel. ", "video": ["About This Specialization", "Specialization Overview", "Course Overview", "Introduction to Mastering Data Analysis in Excel", "Feedback Survey Information"], "title": "About This Course"}, {"description": "In this module, will explore the essential Excel skills to address typical business situations you may encounter in the future. The Excel vocabulary and functions taught throughout this module make it possible for you to understand the additional explanatory Excel spreadsheets that accompany later videos in this course. ", "video": ["Tips for Success", "Introduction to Using Excel in this Course", "Basic Excel Vocabulary; Intro to Charting", "Arithmetic in Excel", "Functions on Individual Cells", "Functions on a Set of Numbers", "Functions on Ordered Pairs of Data", "Sorting Data in Excel", "Introduction to the Solver Plug-in", "Excel Essentials Practice", "Feedback Survey", "Excel Essentials"], "title": "Excel Essentials for Beginners"}, {"description": "Separating collections into two categories, such as “buy this stock, don’t but that stock” or “target this customer with a special offer, but not that one” is the ultimate goal of most business data-analysis projects. There is a specialized vocabulary of measures for comparing and optimizing the performance of the algorithms used to classify collections into two groups. You will learn how and why to apply these different metrics, including how to calculate the all-important AUC: the area under the Receiver Operating Characteristic (ROC) Curve. ", "video": ["Tips for Success", "Introduction to Binary Classification", "Bombers and Seagulls:  Confusion Matrix", "Costs Determine Optimal Threshold", "Calculating Positive and Negative Predictive Values", "How to Calculate the Area Under the ROC Curve", "Binary Classification with More than One Input Variable", "Binary Classification (practice)", "Feedback Survey", "Binary Classification (graded)"], "title": "Binary Classification"}, {"description": "In this module, you will learn how to calculate and apply the vitally useful uncertainty metric known as “entropy.” In contrast to the more familiar “probability” that represents the uncertainty that a single outcome will occur, “entropy” quantifies the aggregate uncertainty of all possible outcomes.\nThe entropy measure provides the framework for accountability in data-analytic work. Entropy gives you the power to quantify the uncertainty of future outcomes relevant to your business twice: using the best-available estimates before you begin a project, and then again after you have built a predictive model.\nThe difference between the two measures is the Information Gain contributed by your work.", "video": ["Tips for Success", "Quantifying the Informational Edge", "Probability and Entropy", "Entropy of a Guessing Game", "Dependence and Mutual Information", "Using the Information Gain Calculator Spreadsheet (practice)", "The Monty Hall Problem", "Learning from One Coin Toss, Part 1", "Learning From One Coin Toss, Part 2", "Feedback Survey", "Information Measures (graded)"], "title": "Information Measures"}, {"description": "The Linear Correlation measure is a much richer metric for evaluating associations than is commonly realized. You can use it to quantify how much a linear model reduces uncertainty.  When used to forecast future outcomes, it can be converted into a “point estimate” plus a “confidence interval,” or converted into an information gain measure. You will develop a fluent knowledge of these concepts and the many valuable uses to which linear regression is put in business data analysis. This module also teaches how to use the Central Limit Theorem (CLT) to solve practical problems. The two topics are closely related because regression and the CLT both make use of a special family of probability distributions called “Gaussians.” You will learn everything you need to know to work with Gaussians in these and other contexts. \n", "video": ["Tips for Success", "Introducing the Gaussian", "Introduction to Standardization", "Standard Normal Probability Distribution in Excel", "Calculating Probabilities from Z-scores", "Central Limit Theorem", "Algebra with Gaussians", "Markowitz Portfolio Optimization", "The Gaussian (practice)", "Standardizing x and y Coordinates for Linear Regression", "Standardization Simplifies Linear Regression", "Modeling Error in Linear Regression", "Information Gain from Linear Regression", "Regression Models and PIG (practice)", "Feedback Survey", "Parametric Models for Regression (graded)"], "title": "Linear Regression"}, {"description": "This module gives you additional valuable concepts and skills related to building high-quality models. \nAs you know, a “model” is a description of a process applied to available data (inputs) that produces an estimate of a future and as yet unknown outcome as output. \nVery often, models for outputs take the form of a probability distribution. This module covers how to estimate probability distributions from data (a “probability histogram”), and how to describe and generate the most useful probability distributions used by data scientists. It also covers in detail how to develop a binary classification model with parameters optimized to maximize the AUC, and how to apply linear regression models when your input consists of multiple types of data for each event. \nThe module concludes with an explanation of “over-fitting” which is the main reason that apparently good predictive models often fail in real life business settings. We conclude with some tips for how you can avoid over-fitting in you own predictive model for the final project – and in real life.\n", "video": ["Describing Histograms and Probability Distributions Functions", "Some Important and Frequently Encountered PDFs", "AUC Calculator Explanation and Spreadsheet", "Linear Regression with More than One Input Variable", "Understanding Why Over-fitting Happens", "Feedback Survey", "Probability, AUC, and Excel Linest Function"], "title": "Additional Skills for Model Building"}, {"description": "The final course project is a comprehensive assessment covering all of the course material, and consists of four quizzes and a peer review assignment.  For quiz one and quiz two, there are learning points that explain components of the quiz.  These learning points will unlock only after you complete the quiz with a passing grade. Before you start, please read through the final project instructions.  From past student experience, the final project which includes all the quizzes and peer assessment, takes anywhere from 10-12 hours.", "video": ["Final Project Information", "Final Project Information:  Part 1", "Summary of Learning Points for Final Project: Quiz 1", "Final Project Information:  Part 2", "Summary of Learning Points for Final Project: Quiz 2", "Feedback Survey", "Part 1:  Building your Own Binary Classification Model", "Part 2:  Should the Bank Buy Third-Party Credit Information?", "Part 3: Comparing the Information Gain of Alternative Data and Models", "Part 4: Modeling Profitability Instead of Default", "Part 5: Modeling Credit Card Default Risk and Customer Profitability"], "title": "Final Course Project"}], "title": "Mastering Data Analysis in Excel"}, {"course_info": "About this course: In the capstone, students will build a series of applications to retrieve, process and visualize data using Python.   The projects will involve all the elements of the specialization.  In the first part of the capstone, students will do some visualizations to become familiar with the technologies in use and then will pursue their own project to visualize some other data that they have or can find.  Chapters 15 and 16 from the book “Python for Everybody” will serve as the backbone for the capstone. This course covers Python 3.", "level": null, "package_name": "Python for Everybody Specialization ", "created_by": "University of Michigan", "package_num": "5", "teach_by": [{"name": "Charles Severance", "department": "School of Information"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "Congratulations to everyone for making it this far. Before you begin, please view the Introduction video and read the Capstone Overview. The Course Resources section contains additional course-wide material that you may want to refer to in future weeks.", "video": ["Introduction: Welcome to the Class", "Capstone Overview", "Help Us Learn More About You!", "Python Textbook", "Coming from Python 2 - Encoding Data in Python 3", "Unicode Characters and Strings", "Notice for Auditing Learners: Assignment Submission", "Office Hours in Den Haag, Netherlands", "Interview: John Resig and Pam Fox - Khan Academy", "Using Encoded Data in Python 3"], "title": "Welcome to the Capstone"}, {"description": "This week we will download and run a simple version of the Google PageRank Algorithm and practice spidering some content. The assignment is peer-graded, and the first of three required assignments in the course. This a continuation of the material covered in Course 4 of the specialization, and is based on Chapter 16 of the textbook. ", "video": ["Building a Search Engine - Introduction", "Page Rank Overview", "Worked Example: Page Rank - Spidering (Chapter 16)", "Worked Example: Page Rank - Computation (Chapter 16)", "Worked Example: Page Rank - Visualization (Chapter 16)", "Office Hours Detroit, Michigan", "Interview: Anil Jain - Image Processing", "Peer Grade: Page Rank"], "title": "Building a Search Engine"}, {"description": "The optional Capstone project is your opportunity to select, process, and visualize the data of your choice, and receive feedback from your peers.  The project is not graded, and can be as simple or complex as you like. This week's assignment is to identify a data source and make a short discussion forum post describing the data source and outlining some possible analysis that could be done with it. You will not be required to use the data source presented here for your actual analysis.", "video": ["Identifying Your Data Source - Introduction", "List of Data Sources (Instructional Staff Curated)", "Identifying a Data Source", "Dr. Chuck's New Kitten - Sakaiger", "Interview: Bruce Schneier - The Security Mindset"], "title": "Exploring Data Sources (Project)"}, {"description": "In our second required assignment, we will retrieve and process email data from the Sakai open source project. Video lectures will walk you through the process of retrieving, cleaning up, and modeling the data.", "video": ["Spidering and Modeling Email Data - Introduction", "Gmane Introduction", "Worked Example: Gmane / Mail - Retrieval (Chapter 16)", "Worked Example: Gmane / Mail - Model (Chapter 16)", "Office Hours Baltimore, MD", "Interview: Bruce Schneier - Building Cryptographic Systems", "Loading and Modeling Mail Data"], "title": "Spidering and Modeling Email Data"}, {"description": "The task for this week is to make a discussion thread post that reflects the progress you have made to date in retrieving and cleaning up your data source so can perform your analysis.  Feedback from other students is encouraged to help you refine the process.", "video": ["Accessing New Data Sources - Introduction", "Analyzing a Data Source", "Office Hours: Dr. Chuck Pretends to be Anthony Bourdain"], "title": "Accessing New Data Sources (Project)"}, {"description": "In the final required assignment, we will do two visualizations of the email data you have retrieved and processed: a word cloud to visualize the frequency distribution and a timeline to show how the data is changing over time.", "video": ["Visualizing Email Data", "Worked Example: Gmane / Mail - Visualization (Chapter 16)", "Office Hours, Montreal, Canada", "Interview: Nathaniel Borenstein - The Father of MIME", "Visualizing Email Data"], "title": "Visualizing Email Data"}, {"description": "This week you will discuss the analysis of your data to the class. While many of the projects will result in a visualization of the data, any other results of analyzing the data are equally valued, so use whatever form of analysis and display is most appropriate to the data set you have selected.", "video": ["Visualizing new Data Sources - Introduction", "Data Analysis and Visualization", "Office Hours - Dr. Chuck's Office - Ann Arbor, Michigan", "Video: Steve Jobs, NeXT and the Internet", "Post-Course Survey"], "title": "Visualizing new Data Sources (Project)"}], "title": "Capstone: Retrieving, Processing, and Visualizing Data with Python"}, {"course_info": "About this course: One of the most common tasks performed by data scientists and data analysts are prediction and machine learning. This course will cover the basic components of building and applying prediction functions with an emphasis on practical applications. The course will provide basic grounding in concepts such as training and tests sets, overfitting, and error rates. The course will also introduce a range of model based and algorithmic machine learning methods including regression, classification trees, Naive Bayes, and random forests. The course will cover the complete process of building prediction functions including data collection, feature creation, algorithms, and evaluation.", "level": null, "package_name": "Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "8", "teach_by": [{"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "This week will cover prediction, relative importance of steps, errors, and cross validation.", "video": ["Welcome to Practical Machine Learning", "Syllabus", "Pre-Course Survey", "Prediction motivation", "What is prediction?", "Relative importance of steps", "In and out of sample errors", "Prediction study design", "Types of errors", "Receiver Operating Characteristic", "Cross validation", "What data should you use?", "Quiz 1"], "title": "Week 1: Prediction, Errors, and Cross Validation"}, {"description": "This week will introduce the caret package, tools for creating features and preprocessing.", "video": ["Caret package", "Data slicing", "Training options", "Plotting predictors", "Basic preprocessing", "Covariate creation", "Preprocessing with principal components analysis", "Predicting with Regression", "Predicting with Regression Multiple Covariates", "Quiz 2"], "title": "Week 2: The Caret Package"}, {"description": "This week we introduce a number of machine learning algorithms you can use to complete your course project.", "video": ["Predicting with trees", "Bagging", "Random Forests", "Boosting", "Model Based Prediction", "Quiz 3"], "title": "Week 3: Predicting with trees, Random Forests, & Model Based Predictions"}, {"description": "This week, we will cover regularized regression and combining predictors.  ", "video": ["Regularized regression", "Combining predictors", "Forecasting", "Unsupervised Prediction", "Course Project Instructions (READ FIRST)", "Post-Course Survey", "Quiz 4", "Prediction Assignment Writeup", "Course Project Prediction Quiz"], "title": "Week 4: Regularized Regression and Combining Predictors"}], "title": "Practical Machine Learning"}, {"course_info": "About this course: This course is an introduction to how to use relational databases in business analysis.  You will learn how relational databases work, and how to use entity-relationship diagrams to display the structure of the data held within them.  This knowledge will help you understand how data needs to be collected in business contexts, and help you identify features you want to consider if you are involved in implementing new data collection efforts.  You will also learn how to execute the most useful query and table aggregation statements for business analysts, and practice using them with real databases. No more waiting 48 hours for someone else in the company to provide data to you – you will be able to get the data by yourself!\n\nBy the end of this course, you will have a clear understanding of how relational databases work, and have a portfolio of queries you can show potential employers. Businesses are collecting increasing amounts of information with the hope that data will yield novel insights into how to improve businesses. Analysts that understand how to access this data – this means you! – will have a strong competitive advantage in this data-smitten business world.", "level": null, "package_name": "Excel to MySQL: Analytic Techniques for Business Specialization ", "created_by": "Duke University", "package_num": "4", "teach_by": [{"name": "Daniel Egger", "department": "Pratt School of Engineering, Duke University"}, {"name": "Jana Schaich Borg", "department": "Social Science Research Institute"}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "The Coursera Specialization, \"Managing Big Data with MySQL\" is about how 'Big Data' interacts with business, and how to use data analytics to create value for businesses. This specialization consists of four courses and a final Capstone Project, where you will apply your skills to a real-world business process. You will learn to perform sophisticated data-analysis functions using powerful software tools such as Microsoft Excel, Tableau, and MySQL. To learn more about the specialization, please review the first lesson below, \"Specialization Introduction: Excel to MySQL: Analytic Techniques for Business.\"  In this fourth course of this specialization, \"Managing Big Data with MySQL” you will learn how relational databases  work and how they are used in business analysis. Specifically, you will: (1) Describe the structure of relational databases; (2) Interpret and create entity-relationship diagrams and relational schemas that describe the contents of specific databases; (3) Write queries that retrieve and sort data that meet specific criteria, and retrieve such data from real MySQL and Teradata business databases that contain over 1 million rows of data; (4) Execute practices that limit the impact of your queries on other coworkers; (5) Summarize rows of data using aggregate functions, and segment aggregations according to specified variables; (6) Combine and manipulate data from multiple tables across a database; (7) Retrieve records and compute calculations that are dependent on dynamic data features; (8) Translate data analysis questions into SQL queries that accommodate the types of anomalies found in real data sets. By the end of this course, you will have a clear understanding of how relational databases work and have a portfolio of queries you can show potential employers. Businesses are collecting increasing amounts of information with the hope that data will yield novel insights into how to improve businesses.  Analysts that understand how to access this data – this means you! – will have a strong competitive advantage in this data-smitten business world.  To get started with this course, you can begin with, \"Introduction to Managing Big Data with MySQL.\"  Please take some time to not only watch the videos, but also read through the course overview as there is extremely important course information in the overview.  ", "video": ["About this Specialization", "Specialization Overview", "Welcome to Managing Big Data with MySQL", "What You Will Learn in This Course", "Course Overview", "IMPORTANT FOR EVERY LEARNER", "Special Thanks!", "Feedback Survey Information"], "title": "About this Specialization and Course "}, {"description": "Welcome to week 1! This week  you will learn how relational databases are organized, and practice making and interpreting Entity Relationship (ER) diagrams and relational schemas that describe the structure of data stored in a database. <p>By the end of the week, you will be able to:<ul><li>Describe the fundamental principles of relational database design <li>Interpret Entity Relationship (ER) diagrams and Entity Relationship (ER) schemas, and</li><li>Create your own ER diagrams and relational schemas using a software tool called ERDPlus that you will use to aid your query-writing later in the course.</li></ul><p>This week’s exercises are donated from a well-known Database Systems textbook, and will help you deepen and strengthen your understanding of how relational databases are organized.  This deeper understanding will help you navigate complicated business databases, and allow you to write more efficient queries.  At the conclusion of the week, you will test your understanding of database design principles by completing the Week 1 graded quiz.</p> <p>To get started, please begin with the video “Problems with Having a Lot of Data Used by a Lot of People.” <p>As always, if you have any questions, post them to the Discussions. <p>I hope you enjoy this week's materials!", "video": ["Problems with Having a Lot of Data Used by a Lot of People", "How Relational Databases Help Solve Those Problems", "Database Design Tools That Will Help You Learn SQL Faster", "How Entity-Relationship Diagrams Work", "Database Structures Illustrated by Entity-Relationship Diagrams", "Relational Schemas", "How to Make Entity-Relationship Diagrams using ERDPlus", "Entity-Relationship Written Exercises", "Entity-Relationship Written Exercises: Answer Key", "How to Make Relational Schemas using ERDPlus", "Relational Schemas Written Exercises", "Relational Schemas Written Exercises: Answer Key", "Dognition Relational Schema Practice Exercise", "Dillard's Relational Schema Practice Questions", "Week One Quiz:  Answers and Feedback", "Wrapping up Week 1", "Week 1 Feedback Survey", "Week 1 Graded Quiz"], "title": "Understanding Relational Databases"}, {"description": "Welcome to week 2! This week, you will start interacting with business databases. You will write SQL queries that query data from two real companies. One data set, donated from a local start-up in Durham, North Carolina called Dognition, is a MySQL database containing tables of over 1 million rows. The other data set, donated from a national US department store chain called Dillard’s, is a Teradata database containing tables with over a hundred million rows. By the end of the week, you will be able to:1.  Use two different database user interfaces2.  Write queries to verify and describe all the contents of the Dognition MySQL database and the Dillard’s Teradata database3.  Retrieve data that meet specific criteria in a socially-responsible using SELECT, FROM, WHERE, LIMIT, and TOP clauses, and4.  Format the data you retrieve using aliases, DISTINCT clauses, and ORDER BY clauses.Make sure to watch the instructional videos about how to use the database interfaces we have established for this course, and complete both the MySQL and the Teradata exercises. At the end of the week, you will test your understanding of the SQL syntax introduced this week by completing the Week 2 graded quiz.To get started, please begin with the video “Introduction to Week 2.”  As always, if you have any questions, post them to the Discussions. Enjoy this week's materials!\n", "video": ["Introduction to Week 2", "Meet Your Dognition Data", "Dognition Database Information", "Meet Your Dillard's Data", "Dillard's Database Information", "Introduction to Query Syntax", "How to Use Jupyter Notebooks", "How to Use Your Jupyter Account", "How to Use Jupyter (Written Instructions)", "Link to Your Jupyter Home Page", "MySQL Exercise 1: Looking at Your Data", "MySQL Exercise 1: Answer Key", "MySQL Exercise 2: Selecting Data Subsets using WHERE", "MySQL Exercise 2: Answer Key", "MySQL Exercise 3: Formatting Selected Data", "MySQL Exercise 3: Answer Key", "How to Use Teradata Viewpoint and SQL Scratchpad", "How to Login to and Use Teradata Viewpoint (Written Instructions)", "Week 2 Teradata Practice Exercises Guide", "Introduction to Teradata", "Read Before Starting Quiz 2", "Week 2 Quiz: Answers and Feedback", "You Have Already Become a Different Level of Business Analyst", "Week 2 Feedback Survey", "Week 2 Graded Quiz using Teradata"], "title": " Queries to Extract Data from Single Tables "}, {"description": "<p>Welcome to week 3! This week, we are going to learn the SQL syntax that allows you to segment your data into separate categories and segment.  We are also going to learn how to combine data stored in separate tables.</p><p>By the end of the week, you will be able to:</p><ul><li>Summarize values across entire columns, and break those summaries up according to specific variables or values in others columns using GROUP BY and HAVING clauses</li><li>Combine information from multiple tables using inner and outer joins</li><li>Use strategies to manage joins between tables with duplicate rows, many-to-many relationships, and atypical configurations</li><li>Practice one of the slightly more challenging use cases of aggregation functions, and</li><li>Work with the Dognition database to learn more about how MySQL handles mismatched aggregation levels.</li></ul><p>Make sure to watch the videos about joins, and complete both the MySQL and the Teradata exercises. At the end of the week, you will test your understanding of the SQL syntax introduced this week by completing the Week 3 graded quiz.</p><p>We strongly encourage you to use the course Discussions to help each other with questions. </p> <p>To get started, please begin with the video 'Welcome to Week 3.’</p><p>I hope you enjoy this week’s materials!</p>", "video": ["Welcome to Week 3", "MySQL Exercise 4: Summarizing Your Data", "MySQL Exercise 4: Answer Key", "Habits that Help You Avoid Mistakes", "MySQL Exercise 5: Breaking Your Summaries into Groups", "MySQL Exercise 5: Answer Key", "MySQL Exercise 6: Common Pitfalls of GROUP BY", "There is NO Answer Key for MySQL Exercise 6", "What are Joins?", "Joins with Many to Many Relationships and Duplicates", "A Note about Our Join Examples", "MySQL Exercise 7: Inner Joins", "MySQL Exercise 7: Answer Key", "MySQL Exercise 8: Joining Tables with Outer Joins", "MySQL Exercise 8: Answer Key", "Week 3 Teradata Practice Exercises Guide", "Week 3 Quiz:  Answers and Feedback", "No More Waiting to Retrieve Your Data", "Week 3 Feedback Survey", "Week 3 Graded Quiz using Teradata"], "title": "Queries to Summarize Groups of Data from Multiple Tables "}, {"description": "<p>Welcome to week 4, the final week of Managing Big Data with MySQL!  This week you will practice integrating the SQL syntax you’ve learn so far into queries that address analysis questions typical of those you will complete as a business data analyst.</p>  <p>By the end of the week, you will be able to:</p><ul><li>Design and execute subqueries</li><li>Introduce logical conditions into your queries using IF and CASE statements</li><li>Implement analyses that accommodate missing data or data mistakes, and</li><li>Write complex queries that incorporate many tables and clauses.</li></ul><p>By the end of this week you will feel confident claiming that you know how to write SQL queries to create business value. Due to the extensive nature of the queries we will practice this week, we have put the graded quiz that tests your understanding of the SQL strategies you will practice in its own week rather than including it in this week’s materials. </p> <p>Make sure to complete both the MySQL exercises and the Teradata exercises, and we strongly encourage you to use the course Discussions to help each other with questions.  </p><p>To get started, please begin with the video 'Welcome to Week 4.’</p><p>I hope you enjoy this week’s materials!</p>", "video": ["Welcome to Week 4", "MySQL Exercise 9: Subqueries and Derived Tables", "MySQL Exercise 9: Answer Key", "MySQL Exercise 10: Useful Logical Functions", "MySQL Exercise 10: Answer Key", "Start with an Analysis Plan", "Dognition Structured Pyramid Analysis Plan (SPAP)", "MySQL Exercise 11: Queries that Test Relationships Between Test Completion and Dog Characteristics", "MySQL Exercise 11: Answer Key", "MySQL Exercise 12: Queries that Test Relationships Between Test Completion and Testing Circumstances", "MySQL Exercise 12: Answer Key", "No Week Four Quiz", "Week 4 Feedback Survey"], "title": " Queries to Address More Detailed Business Questions"}, {"description": "This week contains the final ungraded Teradata exercises, and the final graded quiz for the course. The exercises are intended to hone and build your understanding of the last important concepts in the course, and lead directly to the quiz so be sure to do both!", "video": ["Week 5 Teradata Practice Exercises Guide", "Special Note about the Week 5 Graded Quiz - Please Read", "Quiz 5:  Answers and Feedback", "Don't Be Afraid to Ask Questions!", "Week 5 Feedback Survey", "Week 5 Graded Quiz using Teradata"], "title": "Strengthen and Test Your Understanding "}], "title": "Managing Big Data with MySQL"}, {"course_info": "About this course: This course covers the essential exploratory techniques for summarizing data. These techniques are typically applied before formal modeling commences and can help inform the development of more complex statistical models. Exploratory techniques are also important for eliminating or sharpening potential hypotheses about the world that can be addressed by the data. We will cover in detail the plotting systems in R as well as some of the basic principles of constructing data graphics. We will also cover some of the common multivariate statistical techniques used to visualize high-dimensional data.", "level": null, "package_name": "Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "4", "teach_by": [{"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "This week covers the basics of analytic graphics and the base plotting system in R. We've also included some background material to help you install R if you haven't done so already. ", "video": ["Welcome to Exploratory Data Analysis", "Syllabus", "Pre-Course Survey", "Introduction", "Exploratory Data Analysis with R Book", "The Art of Data Science", "Installing R on Windows (3.2.1)", "Installing R on a Mac (3.2.1)", "Installing R Studio (Mac)", "Setting Your Working Directory (Windows)", "Setting Your Working Directory (Mac)", "Principles of Analytic Graphics", "Exploratory Graphs (part 1)", "Exploratory Graphs (part 2) ", "Plotting Systems in R", "Base Plotting System (part 1)", "Base Plotting System (part 2)", "Base Plotting Demonstration", "Graphics Devices in R (part 1)", "Graphics Devices in R (part 2)", "Practical R Exercises in swirl Part 1", "swirl Lesson 1: Principles of Analytic Graphs", "swirl Lesson 2: Exploratory Graphs", "swirl Lesson 3: Graphics Devices in R", "swirl Lesson 4: Plotting Systems", "swirl Lesson 5: Base Plotting System", "Week 1 Quiz", "Course Project 1"], "title": "Week 1"}, {"description": "Welcome to Week 2 of Exploratory Data Analysis. This week covers some of the more advanced graphing systems available in R: the Lattice system and the ggplot2 system. While the base graphics system provides many important tools for visualizing data, it was part of the original R system and lacks many features that may be desirable in a plotting system, particularly when visualizing high dimensional data. The Lattice and ggplot2 systems also simplify the laying out of plots making it a much less tedious process.", "video": ["Lattice Plotting System (part 1)", "Lattice Plotting System (part 2)", "ggplot2 (part 1)", "ggplot2 (part 2)", "ggplot2 (part 3)", "ggplot2 (part 4)", "ggplot2 (part 5)", "Practical R Exercises in swirl Part 2", "swirl Lesson 1: Lattice Plotting System", "swirl Lesson 2: Working with Colors", "swirl Lesson 3: GGPlot2 Part1", "swirl Lesson 4: GGPlot2 Part2", "swirl Lesson 5: GGPlot2 Extras", "Week 2 Quiz"], "title": "Week 2"}, {"description": "Welcome to Week 3 of Exploratory Data Analysis. This week covers some of the workhorse statistical methods for exploratory analysis. These methods include clustering and dimension reduction techniques that allow you to make graphical displays of very high dimensional data (many many variables). We also cover novel ways to specify colors in R so that you can use color as an important and useful dimension when making data graphics. All of this material is covered in chapters 9-12 of my book Exploratory Data Analysis with R.", "video": ["Hierarchical Clustering (part 1)", "Hierarchical Clustering (part 2)", "Hierarchical Clustering (part 3)", "K-Means Clustering (part 1)", "K-Means Clustering (part 2)", "Dimension Reduction (part 1)", "Dimension Reduction (part 2)", "Dimension Reduction (part 3)", "Working with Color in R Plots (part 1)", "Working with Color in R Plots (part 2)", "Working with Color in R Plots (part 3)", "Working with Color in R Plots (part 4)", "Practical R Exercises in swirl Part 3", "swirl Lesson 1: Hierarchical Clustering", "swirl Lesson 2: K Means Clustering", "swirl Lesson 3: Dimension Reduction", "swirl Lesson 4: Clustering Example"], "title": "Week 3"}, {"description": "This week, we'll look at two case studies in exploratory data analysis. The first involves the use of cluster analysis techniques, and the second is a more involved analysis of some air pollution data. How one goes about doing EDA is often personal, but I'm providing these videos to give you a sense of how you might proceed with a specific type of dataset. ", "video": ["Clustering Case Study", "Air Pollution Case Study", "Practical R Exercises in swirl Part 4", "swirl Lesson 1: CaseStudy", "Post-Course Survey", "Course Project 2"], "title": "Week 4"}], "title": "Exploratory Data Analysis"}, {"course_info": "About this course: In this course, you will get hands-on instruction of advanced Excel 2013 functions.  You’ll learn to use PowerPivot to build databases and data models.  We’ll show you how to perform different types of scenario and simulation analysis and you’ll have an opportunity to practice these skills by leveraging some of Excel's built in tools including, solver, data tables, scenario manager and goal seek.  In the second half of the course, will cover how to visualize data, tell a story and explore data by reviewing core principles of data visualization and dashboarding.  You’ll use Excel to build complex graphs and Power View reports and then start to combine them into dynamic dashboards.\n\nNote: Learners will need PowerPivot to complete some of the exercises. Please use MS Excel 2013 version. If you have other MS Excel versions or a MAC you might not be able to complete all assignments.\n\nThis course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017.", "level": "Beginner", "package_name": "Data Analysis and Presentation Skills: the PwC Approach Specialization ", "created_by": "PwC", "package_num": "3", "teach_by": [{"name": "Alex Mannella", "department": null}], "target_audience": null, "rating": "4.8", "week_data": [{"description": "During this first week, you are going to learn about the development of data models and databases.  We will cover the components of data sets and the relational database models, database keys, relationships, and joins.  We will also look at a tool called PowerPivot that is used to import and prepare data to build relational models, as well as visualize data.  By the end of the week, you will have a working knowledge of how to develop a data model.\n", "video": ["Welcome to Course 3", "Course Overview and Syllabus", "Meet the PwC Instructors and Content Developers", "Meet Your Classmates", "Introduction to the Scenarios Used in this Course", "Tao of Excel Recap and Guiding Principles", "Guiding Principles Handout", "The Power of Visualization", "Introduction to Working with Data Sets", "Introduction to the Components of Data Sets and the Relational Database Model", "Introduction to Keys and Forming Data Tables", "Introduction to Joins", "Overview of SQL and Other Database Tools", "A Message from our Chief People Officer at PwC", "Week 1 Excel Student Exercise Workbook", "Introduction to PowerPivot", "Linking Data Tables and Building the Relational Model", "Using PowerPivot to Visualize Data", "Calculated Fields", "Multiple Data Tables and Filters", "Week 1 closing", "Week 1 Quiz"], "title": "Preparing a Professional Excel "}, {"description": "This week, we are going to explore three different analytical methods used to help model different scenarios and deal with variable uncertainty. These methods are scenario analysis, sensitivity analysis and simulation.  We’ll look at what each method is and then go deeper into why and how you use each.  Following some guided demonstration, you’ll be given a chance to practice in an Excel workbook and demonstrate what you’ve learned. \n", "video": ["Introduction to Week 2", "Advanced Scenario Analysis", "Week 2 Excel Student Exercise Workbook", "Goal Seek", "Scenario Manager", "One-way Data Tables", "Two-way Data Tables", "Simulations", "Solver", "Week 2 Closing", "Week 2 Quiz"], "title": "Advanced Scenario Analysis "}, {"description": "This week we are going to focus on data visualization. We will start off by discussing data visualization basics, outlining the theory and concepts behind data visualization. We will also discuss how to enable effective story telling through the correct selection, creation, and presentation of tables and charts. You’ll get a chance to learn how to created detailed graphs and charts to effectively tell a story about your data. \n", "video": ["Week 3 Opening", "Introduction to Data Visualization", "Effective and Ineffective Charting", "Selecting the Right Type of Chart", "Week 3 Excel Student Exercise Workbook", "Introduction to Chart Navigation", "Column Chart", "Combo Chart", "Stacked Column Chart", "Heatmap", "Gantt Project Plan", "Power View", "Week 3 closing", "Week 3 Quiz"], "title": "Data Visualization"}, {"description": "In the final week of this course, you are going to learn how to create a dynamic dashboard. We are going to discuss how to establish a good understanding of your audience and how to collect key requirements in order to determine what type of dashboard to build.  We will talk about some guiding design principles and things to consider when building a dashboard. You’ll have a chance to practice everything you learn this week by creating your own functional dashboard in Excel.  \n", "video": ["Introduction to Week 4", "Introduction to Dashboarding", "Dashboarding: Beyond the basics", "Week 4 Excel Student Exercise Workbooks", "Form Controls and Grouping", "Slicers and PowerPivot", "Conditional Formatting and KPIs", "Dashboard Finalization and Securing the Workbook", "Week 4 Closing and Course Recap", "Learn More About Career Opportunities at PwC", "Week 4 Quiz"], "title": "Dashboarding "}], "title": "Data Visualization with Advanced Excel"}, {"course_info": "About this course: Linear models, as their name implies, relates an outcome to a set of predictors of interest using linear assumptions.  Regression models, a subset of linear models, are the most important statistical analysis tool in a data scientist’s toolkit. This course covers regression analysis, least squares and inference using regression models. Special cases of the regression model, ANOVA and ANCOVA will be covered as well. Analysis of residuals and variability will be investigated. The course will cover modern thinking on model selection and novel uses of regression models including scatterplot smoothing.", "level": null, "package_name": "Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "7", "teach_by": [{"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "This week, we focus on least squares and linear regression.", "video": ["Welcome to Regression Models", "Book: Regression Models for Data Science in R", "Syllabus", "Pre-Course Survey", "Data Science Specialization Community Site", "Where to get more advanced material", "Regression", "Introduction to Regression", "Introduction: Basic Least Squares", "Technical details", "Technical Details (Skip if you'd like)", "Introductory Data Example", "Least squares", "Notation and Background", "Linear Least Squares", "Linear Least Squares Coding Example", "Technical Details (Skip if you'd like)", "Regression to the mean", "Regression to the Mean", "Practical R Exercises in swirl Part 1", "swirl Lesson 1: Introduction", "swirl Lesson 2: Residuals", "swirl Lesson 3: Least Squares Estimation", "Quiz 1"], "title": "Week 1: Least Squares and Linear Regression"}, {"description": "This week, we will work through the remainder of linear regression and then turn to the first part of  multivariable regression.", "video": ["*Statistical* linear regression models", "Statistical Linear Regression Models", "Interpreting Coefficients", "Linear Regression for Prediction", "Residuals", "Residuals", "Residuals, Coding Example", "Residual Variance", "Inference in regression", "Inference in Regression", "Coding Example", "Prediction", "Looking ahead to the project", "Really, really quick intro to knitr", "Practical R Exercises in swirl Part 2", "swirl Lesson 1: Residual Variation", "swirl Lesson 2: Introduction to Multivariable Regression", "swirl Lesson 3: MultiVar Examples", "Quiz 2"], "title": "Week 2: Linear Regression & Multivariable Regression"}, {"description": "This week, we'll build on last week's introduction to multivariable regression with some examples and then cover residuals, diagnostics, variance inflation, and model comparison. ", "video": ["Multivariable regression", "Multivariable Regression part I", "Multivariable Regression part II", "Multivariable Regression Continued", "Multivariable Regression Examples part I", "Multivariable Regression Examples part II", "Multivariable Regression Examples part III", "Multivariable Regression Examples part IV", "Adjustment", "Adjustment Examples", "Residuals", "Residuals and Diagnostics part I", "Residuals and Diagnostics part II", "Residuals and Diagnostics part III", "Model selection", "Model Selection part I", "Model Selection part II", "Model Selection part III", "Practical R Exercises in swirl Part 3", "swirl Lesson 1: MultiVar Examples2", "swirl Lesson 2: MultiVar Examples3", "swirl Lesson 3: Residuals Diagnostics and Variation", "(OPTIONAL) Data analysis practice with immediate feedback (NEW! 10/18/2017)", "Quiz 3"], "title": "Week 3: Multivariable Regression, Residuals, & Diagnostics"}, {"description": "This week, we will work on generalized linear models, including binary outcomes and Poisson regression. ", "video": ["GLMs", "GLMs", "Logistic regression", "Logistic Regression part I", "Logistic Regression part II", "Logistic Regression part III", "Count Data", "Poisson Regression part I", "Poisson Regression part II", "Mishmash", "Hodgepodge", "Practical R Exercises in swirl Part 4", "swirl Lesson 1: Variance Inflation Factors", "swirl Lesson 2: Overfitting and Underfitting", "swirl Lesson 3: Binary Outcomes", "swirl Lesson 4: Count Outcomes", "Post-Course Survey", "Quiz 4", "Regression Models Course Project"], "title": "Week 4: Logistic Regression and Poisson Regression"}], "title": "Regression Models"}, {"course_info": "About this course: In this first course of the specialization, you will discover just what data visualization is, and how we can use it to better see and understand data. Using Tableau, we’ll examine the fundamental concepts of data visualization and explore the Tableau interface, identifying and applying the various tools Tableau has to offer. By the end of the course you will be able to prepare and import data into Tableau and explain the relationship between data analytics and data visualization. This course is designed for the learner who has never used Tableau before, or who may need a refresher or want to explore Tableau in more depth.  No prior technical or analytical background is required.  The course will guide you through the steps necessary to create your first visualization story from the beginning based on data context,  setting the stage for you to advance to the next course in the Specialization.", "level": "Beginner", "package_name": "Data Visualization with Tableau Specialization ", "created_by": "University of California, Davis", "package_num": "1", "teach_by": [{"name": "Suk S. Brar, M.B.A.", "department": "Blue Shield of California"}, {"name": "Govind Acharya", "department": "Budget and Institutional Analysis"}], "target_audience": "Who is this class for: This course is primary targeted for newcomers to data visualization with no prior experience using Tableau.", "rating": "4.4", "week_data": [{"description": "Welcome to this first module, where you will begin to discover the power of data visualization. You will define the meaning and purpose of data visualization and explore the various types of data visualization tools, beyond Tableau. You will install Tableau on your own device and create your first visualization.", "video": ["Course Introduction and Specialization", "Introduction to Data Visualization", "Introduction to Data Visualization", "What is Data Visualization and Why Do We Do It?", "Data Visualization Challenges", "Tools for Data Visualization", "Visualization Tools", "Python and R", "Installing Tableau", "Installing Tableau", "Creating Your First Visualization", "Creating Your First Visualization (Activity)", "Interview with Dr. Ben Shneiderman (30 mins)", "Dr. Shneiderman's Key Elements", "Introduction to Data Visualization"], "title": "Getting Started & Introduction to Data Visualization"}, {"description": "With the last module, you were able to create your first visualization through guided practice. The secret to doing visualizations is really knowing the tool you will be using. For this module, you will explore and navigate the Tableau interface and be able to use specific tools as you begin your visualization journey.", "video": ["Exploring and Navigating Tableau Overview", "Importing Visualizations", "Downloading a Visualization", "Navigating the Tableau Public's Software - Part 1", "Navigating the Tableau Public's Software - Part 2", "Navigating the Tableau Public's Software - Part 3", "Navigating Tableau", "Additional Readings", "Exploring and Navigating Tableau"], "title": "Exploring and Navigating Tableau"}, {"description": "Creating visualizations require data and in this module, you will discuss the various data sources for visualization and specifically what can be used in Tableau. You will prepare your data and identify the types of data connections possible with Tableau. You will be able to connect and merge to multiple data sources which can help make your visualizations more powerful.", "video": ["Making Data Connections Overview", "Preparing Your Data For Import", "Primary Types of Connections", "Connecting and Merging Multiple Data Sources", "Tableau Public Sample Data Sets", "Tips to Connecting to Data", "TDE or Live? When to Use Tableau Data Extracts (or not)", "Tableau - Extract Your Data", "Connecting to Multiple Data Sources", "Making Data Connections"], "title": "Making Data Connections"}, {"description": "Data visualization is about telling a story using data. However, before you can be successful at data visualization, you must understand the \"who\", \"what\", and \"how\" of data context. In this final module, you will be able to determine who your audience will be and what your relationship to them is. You will analyze a real world application of data context and be able to write out a visualization story based on data context.", "video": ["Context of Data Visualization Overview", "The \"Who, What, and How\" of Data Visualization", "Resonate - Present Visual Stories that Transform Audiences", "Illustration of the \"Who, What, and How\"", "Visualization of the Day Exercise", "Context of Data Visualization", "Context of Data Visualization (Activity)", "Course Wrap-Up", "Course Reflection", "Storyboarding Your Visualization ", "Context of Data Visualization"], "title": "Context of Data Visualization & Course Wrap-Up"}], "title": "Fundamentals of Visualization with Tableau"}, {"course_info": "About this course: Statistical inference is the process of drawing conclusions about populations or scientific truths from data. There are many modes of performing inference including statistical modeling, data oriented strategies and explicit use of designs and randomization in analyses. Furthermore, there are broad theories (frequentists, Bayesian, likelihood, design based, …) and numerous complexities (missing data, observed and unobserved confounding, biases) for performing inference. A practitioner can often be left in a debilitating maze of techniques, philosophies and nuance. This course presents the fundamentals of inference in a practical approach for getting things done. After taking this course, students will understand the broad directions of statistical inference and use this information for making informed choices in analyzing data.", "level": null, "package_name": "Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "6", "teach_by": [{"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}], "target_audience": null, "rating": "4.1", "week_data": [{"description": "This week, we'll focus on the fundamentals including probability, random variables, expectations and more. ", "video": ["Introductory video", "Welcome to Statistical Inference", "Some introductory comments", "Pre-Course Survey", "Syllabus", "Course Book: Statistical Inference for Data Science", "Data Science Specialization Community Site", "Homework Problems", "Probability", "02 01 Introduction to probability", "02 02 Probability mass functions", "02 03 Probability density functions", "Conditional probability", "03 01 Conditional Probability", "03 02 Bayes' rule", "03 03 Independence", "Expected values", "04 01 Expected values", "04 02 Expected values, simple examples", "04 03 Expected values for PDFs", "Practical R Exercises in swirl 1", "swirl Lesson 1: Introduction", "swirl Lesson 2: Probability1", "swirl Lesson 3: Probability2", "swirl Lesson 4: ConditionalProbability", "swirl Lesson 5: Expectations", "Quiz 1"], "title": "Week 1: Probability & Expected Values"}, {"description": "We're going to tackle variability, distributions, limits, and confidence intervals.", "video": ["Variability", "05 01 Introduction to variability", "05 02 Variance simulation examples", "05 03 Standard error of the mean", "05 04 Variance data example", "Distributions", "06 01 Binomial distrubtion", "06 02 Normal distribution", "06 03 Poisson", "Asymptotics", "07 01 Asymptotics and LLN", "07 02 Asymptotics and the CLT", "07 03 Asymptotics and confidence intervals", "Practical R Exercises in swirl Part 2", "swirl Lesson 1: Variance", "swirl Lesson 2: CommonDistros", "swirl Lesson 3: Asymptotics", "Quiz 2"], "title": "Week 2: Variability, Distribution, & Asymptotics"}, {"description": "We will be taking a look at intervals, testing, and pvalues in this lesson.", "video": ["Confidence intervals", "08 01 T confidence intervals", "08 02 T confidence intervals example", "08 03 Independent group T intervals", "08 04 A note on unequal variance", "Hypothesis testing", "09 01 Hypothesis testing", "09 02 Example of choosing a rejection region", "09 03 T tests", "09 04 Two group testing", "P-values", "10 01 Pvalues", "10 02 Pvalue further examples", "Knitr", "Just enough knitr to do the project", "Practical R Exercises in swirl Part 3", "swirl Lesson 1: T Confidence Intervals", "swirl Lesson 2: Hypothesis Testing", "swirl Lesson 3: P Values", "Quiz 3"], "title": "Week: Intervals, Testing, & Pvalues"}, {"description": "We will begin looking into power, bootstrapping, and permutation tests.", "video": ["Power", "11 01 Power", "11 02 Calculating Power", "11 03 Notes on power", "11 04 T test power", "12 01 Multiple Comparisons", "Resampling", "13 01 Bootstrapping", "13 02 Bootstrapping example", "13 03 Notes on the bootstrap", "13 04 Permutation tests", "Practical R Exercises in swirl Part 4", "swirl Lesson 1: Power", "swirl Lesson 2: Multiple Testing", "swirl Lesson 3: Resampling", "Post-Course Survey", "Quiz 4", "Statistical Inference Course Project"], "title": "Week 4: Power, Bootstrapping, & Permutation Tests"}], "title": "Statistical Inference"}, {"course_info": "About this course: This course is all about presenting the story of the data, using PowerPoint. You'll learn how to structure a presentation, to include insights and supporting data. You'll also learn some design principles for effective visuals and slides. You'll gain skills for client-facing communication - including public speaking, executive presence and compelling storytelling. Finally, you'll be given a client profile, a business problem, and a set of basic Excel charts, which you'll need to turn into a presentation - which you'll deliver with iterative peer feedback.\n\nThis course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017.", "level": "Beginner", "package_name": "Data Analysis and Presentation Skills: the PwC Approach Specialization ", "created_by": "PwC", "package_num": "4", "teach_by": [{"name": "Alex Mannella", "department": null}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "This course is about presenting the story of the data, using PowerPoint. You'll learn how to structure a presentation and how to include insights and supporting data. You'll also learn some design principles for creating effective PowerPoint slides with visuals displaying data. Though application based exercises, you'll gain foundational communication skills - including public speaking, professional presence and compelling storytelling. Finally, you'll be given a client profile, a business problem, and a set of basic Excel charts, that you will use to create  a presentation.  You’ll receive peer feedback that you can use to enhance future presentations. This course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017", "video": ["Welcome to Course 4", "Course Overview and Syllabus", "Meet the PwC Instructors", "Meet Your Classmates", "Case Study and Materials", "Welcome to Week 1", "The eight-step approach to prepare for a presentation", "Step 1 - Know your audience and Step 2 - Know your purpose", "How do you get to know your audience?", "Step 3 - Structure the body of your presentation", "Outlining and Wireframing", "Step 4 - Plan how you will start your presentation", "Step 5 - Plan how you will end your presentation", "Step 6 - Prepare your visual aids", "Step 7 - Anticipate the questions you may be asked", "When do you field questions during a presentation?", "Step 8 - Practice your presentation", "Presenting on short notice", "Preparing for a presentation on short notice", "The eight-step approach to prepare for a presentation", "When is it appropriate to change the order of the eight-step approach? ", "Week 1 Closing", "A Message from our Chief People Officer at PwC", "Week 1 Quiz"], "title": "Preparing a Presentation"}, {"description": "This week, we will be covering the different types of communications styles.  You’ll start off by gaining an understanding of your personal professional presence and learn how to maximize it.  You’ll learn about verbal and nonverbal communications, and strategies to enhance your questioning and listening skills. We will also discuss how differences in culture can impact how you communicate.", "video": ["Introduction to Week 2", "Maximizing your professional presence", "Communicating with confidence", "Verbal communications", "Non-verbal communications", "How do verbal and non-verbal communications impact your message?", "Cultural Considerations in Communication", "Culture and Presentations", "Tip Sheet:  Communicating with confidence", "Questioning and listening skills", "Week 2 Closing", "Week 2 Quiz"], "title": "Communication styles"}, {"description": "This week, we're discussing how to create effective slides using PowerPoint. You’ll learn about the tools available within PowerPoint, how to structure your storyline, create storyboards, identify primary elements of slide design, display data and finalize your slide presentation. There is a peer review activity where you will apply the skills learned and create a storyboard. Finally, you will also get a chance to identify errors in a presentation to test your knowledge of standard industry practices.", "video": ["Introduction to Week 3", "Introduction to PowerPoint (2013)", "PowerPoint Practice Activity", "What other tools have you used to create a presentation? ", "What type of deck should you use?", "Structure your storyline", "Types of logic", "Creating a storyboard", "Tip Sheet: Storyboarding", "Primary elements of slide design", "Slide writing guide", "Displaying data", "Tip Sheet: Displaying data", "Finalizing your deck", "What experiences do you have using cloud based presentation tools?", "Week 3 Closing", "Create a storyboard in PowerPoint", "Identifying errors in a deck exercise", "Week 3 Quiz"], "title": "Creating effective slides using PowerPoint"}, {"description": "This week, you’re going to build and deliver a presentation to your peers, and receive feedback from them.  You will create a presentation of about 10 slides, employing the guidelines and industry best practices that have been discussed in this course. You can use the presentation storyboard that you created last week, which your peers have reviewed and given you feedback on.  Review what you’ve developed so far, and make changes or additions that you think will enhance the presentation. Once you’ve finalized your presentation, you will present it in a video using your smartphone or computer.\nOnce you’re satisfied with the PowerPoint presentation and video, you will be submitting both for peer review.  You can use this feedback for current and future presentations that you will make during your career.", "video": ["Introduction to Week 4", "Final course simulation", "Best tips for recording your own video", "Week 4 and Course Wrap-Up", "Simulation Validation Quiz", "Delivering your final presentation"], "title": "Delivering a presentation"}], "title": "Effective Business Presentations with Powerpoint"}, {"course_info": "About this course: Organizations large and small are inundated with data about consumer choices. But that wealth of information does not always translate into better decisions. Knowing how to interpret data is the challenge -- and marketers in particular are increasingly expected to use analytics to inform and justify their decisions. \n\nMarketing analytics enables marketers to measure, manage and analyze marketing performance to maximize its effectiveness and optimize return on investment (ROI). Beyond the obvious sales and lead generation applications, marketing analytics can offer profound insights into customer preferences and trends, which can be further utilized for future marketing and business decisions. \n\nThis course gives you the tools to measure brand and customer assets, understand regression analysis, and design experiments as a way to evaluate and optimize marketing campaigns. You'll leave the course with a solid understanding of how to use marketing analytics to predict outcomes and systematically allocate resources.\n\nFor more information on marketing analytics, you may visit; http://dmanalytics.org. You can also follow my posts in Twitter, @rajkumarvenk, and on linkedin; www.linkedin.com/in/rajkumar-venkatesan-14970a3.", "level": "Beginner", "package_name": null, "created_by": "University of Virginia", "package_num": null, "teach_by": [{"name": "Rajkumar Venkatesan", "department": "Darden School of Business"}], "target_audience": "Who is this class for: This course is ideal for learners who want to grow their knowledge, develop their career portfolio, and improve the effectiveness of their marketing campaigns.", "rating": "4.5", "week_data": [{"description": "Welcome! We'll start with an overview of the marketing process and the transformational role of analytics. Then we'll walk through a case study. Ever heard of Airbnb? They're a powerhouse of the online community marketplace matching travelers to hosts. You'll see how they use analytics and the surprising results of their analyses.", "video": ["Data, Data Everywhere!", "Course Overview", "Course Overview & Requirements", "Use Discussion Forums to Deepen Your Learning", "Why Marketing Analytics?", "Introduction to the Marketing Process", "Airbnb Marketing Process", "Practice Quiz on the Marketing Process", "Airbnb's Strategic Challenge", "Airbnb's Marketing Strategy with Data", "Using Text Analytics", "Utilizing Data to Improve Marketing Strategy", "Practice Quiz on Data Analytics Basics", "Takeaways: Improving the Marketing Process with Analytics", "Week 1 Quiz: the Marketing Process"], "title": "The Marketing Process"}, {"description": "Firms spend millions on branding for one reason: It allows them to charge more for their products and services. In this module, we'll explore this valuable, if intangible, asset.  We'll discuss how to build and define a brand architecture and how to measure the impact of marketing efforts on brand value over time. By the end of this module, you'll be able to measure and track brand value. So let's get started!", "video": ["Intro to Metrics for Measuring Brand Assets", "Snapple and Brand Value", "Developing Brand Personality", "Brand Personality: Red Bull", "Pick a brand, any brand!", "Developing Brand Architecture", "Brand Architecture: Red Bull", "Brand Architecture: Etch A Sketch", "Practice Quiz on Brand and Brand Architecture", "Etch A Sketch's Brand Pyramid", "Measuring Brand Value", "Measuring Brand Value: Key Points", "Revenue Premium as a Measure of Brand Equity", "Calculating Brand Value: Snapple", "Practice Quiz on Calculating Brand Value", "Takeaways: Measuring Brand Value", "Week 2 Quiz on Measuring Brand Assets", "Building Brand Architecture"], "title": "Metrics for Measuring Brand Assets"}, {"description": "How valuable are your customers? That's a tough question that we'll show you how to answer in this module where we'll explore Customer Lifetime Value, or the future net value of a customer relationship. This forward-looking measure of the customer relationship helps you connect marketing strategies to future financial consequences and invest marketing dollars in the right place to maximize return over a customer's lifetime. By the end of this module, you will know how to measure customer lifetime value  and evaluate strategic marketing alternatives based on whether they improve customer retention and lifetime value.", "video": ["Welcome to Week 3", "Customer Lifetime Value (CLV)", "Customer Lifetime Value: Netflix", "Calculating CLV", "Understanding the CLV Formula", "Applying the CLV Formula: Netflix", "How long is a customer worth keeping?", "Extending the CLV Formula, Part 1", "Extending the CLV Formula, Part 2", "Challenge yourself!", "Using CLV to Make Decisions: IBM", "CLV: A Forward Looking Measure", "Practice Quiz on CLV", "Takeaways: CLV", "Week 3 Quiz on CLV"], "title": "Customer Lifetime Value"}, {"description": "Ever wonder how much you have to cut prices to drive the most sales? Or which advertisement copy is more effective in customer conversion? Do an experiment! Experiments allow you to understand the effectiveness of different marketing strategies and forecast expected ROI. This week, we'll explore how to design basic experiments so that you can assess your marketing efforts and invest your marketing dollars most effectively. We'll help you avoid a gap between your test results and field implementation, and explore how web experiments can be implemented cheaply and quickly.  By the end of this module, you'll be able to design and conduct effective experiments that test your marketing campaigns--and then use the results to make future marketing decisions. ", "video": ["Welcome to Week 4", "Spreadsheet with Formulas", "Determining Cause and Effect through Experiments", "Designing Basic Experiments", "Designing Before - After Experiments", "Practice Quiz 1 on Designing Experiments", "Designing Full Factorial Web Experiments", "Designing an Experiment: Etch A Sketch", "Analyzing an Experiment: Etch A Sketch", "Practice Quiz 2 on Calculating Break Even and Lift", "Analyzing an Experiment: Betty Spaghetty", "Projecting Lift", "Calculating Projected Lift: Betty Spaghetty", "Pitfalls of Marketing Experiments: Betty Spaghetty", "Maximizing Effectiveness: Nanoblocks", "Transformation of Marketing at the Ohio Art Company (abridged)", "Takeaways: Marketing Experiments", "MBTN Assessments", "Practice Quiz 3 on Projecting Lift", "Week 4: Marketing Experiments Quiz", "Design a Marketing Experiment"], "title": "Marketing Experiments"}, {"description": "Ever wonder how variables influence consumer behavior in the real world--like how weather and a price promotion affect ice cream consumption? In this module, we will take a look at regression and how it's used to understand that relationship. We will discuss how to set up regressions and interpret outputs, explore confounding effects and biases, and distinguish between economic and statistical significance. We'll finish the week with a series of interviews with real marketing professionals who share their experiences and knowledge about how they use analytics on the job.", "video": ["Welcome to Week 5", "Using Regression Analysis", "What Regressions Reveal", "Interpreting Regression Outputs", "Multivariable Regressions", "Omitted Variable Bias", "Using Price Elasticity to Evaluate Marketing", "Understanding Log-Log Models", "Marketing Mix Models", "Takeaways: Regressions", "Practice Quiz: Regressions", "Course Conclusion", "Interview with Jennifer Chick, VP Marketing, Hilton Worldwide - Part 1", "Interview with Jennifer Chick, VP Marketing, Hilton Worldwide - Part 2", "Interview with Paul Hunter, Head of Operations, dunnhumby, on Using Data - Part 1", "Interview with Paul Hunter, Head of Operations, dunnhumby, on Using Data - Part 2", "Interview with Paul Flugel, VP of Global Marketing Performance - Part 1", "Interview with Paul Flugel, VP of Global Marketing Performance - Part 2", "What did you learn from each of the guest speakers?", "Week 5 Quiz on Regression Analysis"], "title": "Regression Basics"}], "title": "Marketing Analytics"}, {"course_info": "About this course: This course covers commonly used statistical inference methods for numerical and categorical data. You will learn how to set up and perform hypothesis tests, interpret p-values, and report the results of your analysis in a way that is interpretable for clients or the public. Using numerous data examples, you will learn to report estimates of quantities in a way that expresses the uncertainty of the quantity of interest. You will be guided through installing and using R and RStudio (free statistical software), and will use this software for lab exercises and a final project. The course introduces practical tools for performing data analysis and explores the fundamental concepts necessary to interpret and report results for both categorical and numerical data", "level": "Beginner", "package_name": "Statistics with R Specialization ", "created_by": "Duke University", "package_num": "2", "teach_by": [{"name": "Mine Çetinkaya-Rundel", "department": "Department of Statistical Science"}], "target_audience": null, "rating": "4.8", "week_data": [{"description": "This short module introduces basics about Coursera specializations and courses in general, this specialization: Statistics with R, and this course: Inferential Statistics. Please take several minutes to browse them through. Thanks for joining us in this course!", "video": ["About Statistics with R Specialization", "More about Inferential Statistics"], "title": "About the Specialization and the Course"}, {"description": "Welcome to Inferential Statistics! In this course we will discuss Foundations for Inference. Check out the learning objectives, start watching the videos, and finally work on the quiz and the labs of this week. In addition to videos that introduce new concepts, you will also see a few videos that walk you through application examples related to the week's topics. In the first week we will introduce Central Limit Theorem (CLT) and confidence interval.", "video": ["Lesson Learning Objectives", "Introduction", "Sampling Variability and CLT", "CLT (for the mean) examples", "Lesson Learning Objectives", "Confidence Interval (for a mean)", "Accuracy vs. Precision", "Required Sample Size for ME", "CI (for the mean) examples", "Week 1 Suggested Readings and Practice Exercises", "Week 1 Practice Quiz", "Week 1 Lab Instructions", "Week 1 Quiz", "Week 1 Lab"], "title": "Central Limit Theorem and Confidence Interval"}, {"description": "Welcome to Week Two! This week we will discuss formal hypothesis testing and relate testing procedures back to estimation via confidence intervals. These topics will be introduced within the context of working with a population mean, however we will also give you a brief peek at what's to come in the next two weeks by discussing how the methods we're learning can be extended to other estimators. We will also discuss crucial considerations like decision errors and statistical vs. practical significance. The labs for this week will illustrate concepts of sampling distributions and confidence levels.", "video": ["Lesson Learning Objectives", "Another Introduction to Inference", "Hypothesis Testing (for a mean)", "HT (for the mean) examples", "Lesson Learning Objectives", "Inference for Other Estimators", "Decision Errors", "Significance vs. Confidence Level", "Statistical vs. Practical Significance", "Week 2 Suggested Readings and Practice Exercises", "Week 2 Practice Quiz", "Week 2 Lab Instructions", "Week 2 Quiz", "Week 2 Lab"], "title": "Inference and Significance"}, {"description": "Welcome to Week Three of the course! This week we will introduce the t-distribution and comparing means as well as a simulation based method for creating a confidence interval: bootstrapping. If you have questions or discussions, please use this week's forum to ask/discuss with peers.", "video": ["Lesson Learning Objectives", "Introduction", "t-distribution", "Inference for a mean", "Inference for comparing two independent means", "Inference for comparing two paired means", "Power", "Lesson Learning Objectives", "Comparing more than two means", "ANOVA", "Conditions for ANOVA", "Multiple comparisons", "Bootstrapping", "Week 3 Suggested Readings and Practice Exercises", "Week 3 Practice Quiz", "Week 3 Lab Instructions", "Week 3 Quiz", "Week 3 Lab"], "title": "Inference for Comparing Means"}, {"description": "Welcome to Week Four of our course! In this unit, we’ll discuss inference for categorical data. We use methods introduced this week to answer questions like “What proportion of the American public approves of the job of the Supreme Court is doing?”.", "video": ["Lesson Learning Objectives", "Introduction", "Sampling Variability and CLT for Proportions", "Confidence Interval for a Proportion", "Hypothesis Test for a Proportion", "Estimating the Difference Between Two Proportions", "Hypothesis Test for Comparing Two Proportions", "Lesson Learning Objectives", "Small Sample Proportions", "Examples", "Comparing Two Small Sample Proportions", "Chi-Square GOF Test", "The Chi-Square Independence Test", "Week 4 Suggested Readings and Practice Exercises", "Week 4 Practice Quiz", "Week 4 Lab Instructions", "Week 4 Quiz", "Week 4 Lab"], "title": "Inference for Proportions"}, {"description": "In this week you will use the data set provided to complete and report on a data analysis question. Please read the background information, review the report template (downloaded from the link in Lesson Project Information), and then complete the peer review assignment. ", "video": ["Project Information", "Data Analysis Project"], "title": " Data Analysis Project"}], "title": "Inferential Statistics"}, {"course_info": "About this course: This course will introduce the learner to network analysis  through the NetworkX library. The course begins with an understanding of what network analysis is  and motivations for why we might model phenomena as networks. The second week introduces the concept of connectivity and network robustness.. The third week will explore ways of measuring the importance or centrality of a node in a network. The final week will explore the evolution of networks over time and cover models of network generation and the link prediction problem.\n\nThis course should be taken after: Introduction to Data Science in Python, Applied Plotting, Charting & Data Representation in Python, and Applied Machine Learning in Python.", "level": "Intermediate", "package_name": "Applied Data Science with Python Specialization ", "created_by": "University of Michigan", "package_num": "5", "teach_by": [{"name": "Daniel Romero", "department": "School of Information"}], "target_audience": "Who is this class for: This course is part of “Applied Data Science with Python“ and is intended for learners who have basic python or programming background, and want to apply statistics, machine learning, information visualization, social network analysis, and text analysis techniques to gain new insight into data. Only minimal statistics background is expected, and the first course in the specialization contains a refresh of these basic concepts. There are no geographic restrictions. Learners with a formal training in Computer Science but without formal training in data science will still find the skills they acquire in these courses valuable in their studies and careers.", "rating": "4.6", "week_data": [{"description": "Module One introduces you to different types of networks in the real world and why we study them. You'll learn about the basic elements of networks, as well as different types of networks. You'll also learn how to represent and manipulate networked data using the NetworkX library. The assignment will give you an opportunity to use NetworkX to analyze a networked dataset of employees in a small company.", "video": ["Course Syllabus", "Help us learn more about you!", "Networks: Definition and Why We Study Them", "Network Definition and Vocabulary", "Node and Edge Attributes", "Bipartite Graphs", "Notice for Auditing Learners: Assignment Submission", "Loading Graphs in NetworkX", "TA Demonstration: Loading Graphs in NetworkX", "Assignment 1", "Module 1 Quiz", "Assignment 1 Submission"], "title": "Why Study Networks and Basics on NetworkX"}, {"description": "In Module Two you'll learn how to analyze the connectivity of a network based on measures of distance, reachability, and redundancy of paths between nodes. In the assignment, you will practice using NetworkX to compute measures of connectivity of a network of email communication among the employees of a mid-size manufacturing company. ", "video": ["Clustering Coefficient", "Distance Measures", "Connected Components", "Network Robustness", "Simple Network Visualizations in NetworkX", "TA Demonstration: Simple Network Visualizations in NetworkX", "Assignment 2", "Module 2 Quiz", "Assignment 2 Submission"], "title": "Network Connectivity"}, {"description": "In Module Three, you'll explore ways of measuring the importance or centrality of a node in a network, using measures such as Degree, Closeness, and Betweenness centrality, Page Rank, and Hubs and Authorities. You'll learn about the assumptions each measure makes, the algorithms we can use to compute them, and the different functions available on NetworkX to measure centrality. In the assignment, you'll practice choosing the most appropriate centrality measure on a real-world setting.", "video": ["Degree and Closeness Centrality", "Betweenness Centrality", "Basic Page Rank", "Scaled Page Rank", "Hubs and Authorities", "Centrality Examples", "PageRank and Centrality in a real-life network", "Assignment 3", "Module 3 Quiz", "Assignment 3 Submission"], "title": "Influence Measures and Network Centralization"}, {"description": "In Module Four, you'll explore the evolution of networks over time, including the different models that generate networks with realistic features, such as the Preferential Attachment Model and Small World Networks. You will also explore the link prediction problem, where you will learn useful features that can predict whether a pair of disconnected nodes will be connected in the future. In the assignment, you will be challenged to identify which model generated a given network. Additionally, you will have the opportunity to combine different concepts of the course by predicting the salary, position, and future connections of the employees of a company using their logs of email exchanges. \n", "video": ["Preferential Attachment Model", "Power Laws and Rich-Get-Richer Phenomena (Optional)", "Small World Networks", "Link Prediction", "Extracting Features from Graphs", "The Small-World Phenomenon (Optional)", "Assignment 4", "Post-Course Survey", "Module 4 Quiz", "Assignment 4 Submission"], "title": "Network Evolution"}], "title": "Applied Social Network Analysis in Python"}, {"course_info": "About this course: This course is for novice programmers or business people who would like to understand the core tools used to wrangle and analyze big data. With no prior experience, you will have the opportunity to walk through hands-on examples with Hadoop and Spark frameworks, two of the most common in the industry. You will be comfortable explaining the specific components and basic processes of the Hadoop architecture, software stack, and execution environment.   In the assignments you will be guided in how data scientists apply the important concepts and techniques such as Map-Reduce that are used to solve fundamental problems in big data.  You'll feel empowered to have conversations about big data and the data analysis process.", "level": null, "package_name": null, "created_by": "University of California, San Diego", "package_num": null, "teach_by": [{"name": "Natasha Balac", "department": "San Diego Supercomputer Center"}, {"name": "Paul Rodriguez", "department": "San Diego Supercomputer Center (SDSC)"}, {"name": "Andrea Zonca", "department": "San Diego Supercomputer Center (SDSC)"}], "target_audience": null, "rating": null, "week_data": [{"description": "Welcome to the first module of the Big Data Platform course. This first module will provide insight into Big Data Hype, its technologies opportunities and challenges.  We will take a deeper look into the Hadoop stack and tool and technologies associated with Big Data solutions.  \n", "video": ["Hadoop Stack Basics", "The Apache Framework: Basic Modules", "Hadoop Distributed File System (HDFS)", "The Hadoop \"Zoo\"", "Hadoop Ecosystem Major Components", "Apache Hadoop Ecosystem", "Lesson 1 Slides (PDF)", "Hardware & Software Requirements", "Exploring the Cloudera VM: Hands-On Part 1", "Exploring the Cloudera VM: Hands-On Part 2", "Lesson 2 Slides - Cloudera VM Tour", "Basic Hadoop Stack"], "title": "Hadoop Basics"}, {"description": "In this module we will take a detailed look at the Hadoop stack ranging from the basic HDFS components, to application execution frameworks, and languages, services.", "video": ["Overview of the Hadoop Stack", "The Hadoop Distributed File System (HDFS) and HDFS2", "MapReduce Framework and YARN", "Hadoop Basics - Lesson 1 Slides", "The Hadoop Execution Environment", "YARN, Tez, and Spark", "Hadoop Resource Scheduling", "Lesson 2: Hadoop Execution Environment - Slides", "Hadoop-Based Applications", "Introduction to Apache Pig", "Introduction to Apache HIVE", "Introduction to Apache HBASE", "Lesson 3: Hadoop-Based Applications Overview - All Slides", "Command list for Applications Slides", "Tips to handle service connection errors", "References for Applications", "Overview of Hadoop Stack", "Hadoop Execution Environment", "Hadoop Applications"], "title": "Introduction to the Hadoop Stack"}, {"description": "In this module we will take a detailed look at the Hadoop Distributed File System (HDFS). We will cover the main design goals of HDFS, understand the read/write process to HDFS, the main configuration parameters that can be tuned to control HDFS performance and robustness, and get an overview of the different ways you can access data on HDFS.", "video": ["Overview of HDFS Architecture", "The HDFS Performance Envelope", "Read/Write Processes in HDFS", "Lesson 1: Introduction to HDFS - Slides", "HDFS references", "HDFS Tuning Parameters", "HDFS Performance and Robustness", "Lesson 2: HDFS Performance and Tuning - Slides", "Overview of HDFS Access, APIs, and Applications", "HDFS Commands", "Native Java API for HDFS", "REST API for HDFS", "HDFS Access, APIs", "Lesson 3: HDFS Access, APIs, Applications - Slides", "HDFS Architecture", "HDFS performance,tuning, and robustness", "Accessing HDFS"], "title": "Introduction to Hadoop Distributed File System (HDFS)"}, {"description": "This module will introduce Map/Reduce concepts and practice.  You will learn about the big idea of Map/Reduce and you will learn how to design, implement, and execute tasks in the map/reduce framework. You will also learn the trade-offs in map/reduce and how that motivates other tools.", "video": ["Introduction to Map/Reduce", "The Map/Reduce Framework", "A MapReduce Example: Wordcount in detail", "Lesson 1: Introduction to MapReduce - Slides", "A note on debugging map/reduce programs.", "MapReduce: Intro to Examples and Principles", "MapReduce Example: Trending Wordcount", "MapReduce Example: Joining Data", "MapReduce Example: Vector Multiplication", "Computational Costs of Vector Multiplication", "MapReduce Summary", "Lesson 2: MapReduce Examples and Principles - Slides", "Running Wordcount with Hadoop streaming, using Python code", "Lesson 1 Review", "Joining Data "], "title": "Introduction to Map/Reduce"}, {"description": "Welcome to module 5, Introduction to Spark, this week we will focus on the Apache Spark cluster computing framework, an important contender of Hadoop MapReduce in the Big Data Arena.\n\nSpark provides great performance advantages over Hadoop MapReduce,especially for iterative algorithms, thanks to in-memory caching. Also, gives Data Scientists an easier way to write their analysis pipeline in Python and Scala,even providing interactive shells to play live with data.", "video": ["Introduction to Apache Spark", "Architecture of Spark", "Setup PySpark on the Cloudera VM", "Lesson 1: Intro to Apache Spark - Slides", "Resilient Distributed Datasets", "Spark Transformations", "Wide Transformations", "Lesson 2: RDD and Transformations - Slides", "Directed Acyclic Graph (DAG) Scheduler", "Actions in Spark", "Memory Caching in Spark", "Broadcast Variables", "Accumulators", "Lesson 3: Scheduling, Actions, Caching - Slides", "Spark Lesson 1", "Spark Lesson 2", "Simple Join in Spark", "Spark Lesson 3", "Advanced Join in Spark"], "title": "Spark"}], "title": "Hadoop Platform and Application Framework"}, {"course_info": "About this course: Database Management Essentials provides the foundation you need for a career in database development, data warehousing, or business intelligence, as well as for the entire Data Warehousing for Business Intelligence specialization. In this course, you will create relational databases, write SQL statements to extract information to satisfy business reporting requests, create entity relationship diagrams (ERDs) to design databases, and analyze table designs for excessive redundancy. As you develop these skills, you will use either Oracle or MySQL to execute SQL statements and a database diagramming tool such as the ER Assistant to create ERDs. We’ve designed this course to ensure a common foundation for specialization learners. Everyone taking the course can jump right in with writing SQL statements in Oracle or MySQL.", "level": null, "package_name": "Data Warehousing for Business Intelligence Specialization ", "created_by": "University of Colorado System", "package_num": "1", "teach_by": [{"name": "Michael Mannino", "department": "Business School, University of Colorado Denver"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "Module 1 provides the context for Database Management Essentials. When you’re done, you’ll understand the objectives for the course and know what topics and assignments to expect. Keeping these course objectives in mind will help you succeed throughout the course! You should read about the database software requirements in the last lesson of module 1. I recommend that you try to install the DBMS software this week before assignments begin in week 2.", "video": ["Specialization Introduction video lesson", "Course introduction video lecture", "Course objectives video lecture", "Powerpoint lecture notes for lesson 1", "Topics and  assignments video lecture", "Powerpoint lecture notes for lesson 2", "Optional textbook", "Overview of database management software requirements", "Oracle installation notes", "Making a connection to a database on a local Oracle server"], "title": "Course Introduction"}, {"description": "We’ll launch into an exploration of databases and database technology and their impact on organizations in Module 2. We’ll investigate database characteristics, database technology features, including non-procedural access, two key processing environments, and an evolution of the database software industry. This short informational module will ensure that we all have the same background and context, which is critical for success in the later modules that emphasize details and hands-on skills.\n", "video": ["Database characteristics video lecture", "Powerpoint lecture notes for lesson 1 and extras", "Learning check-in", "Organizational Roles video lecture", "Powerpoint lecture notes for lesson 2", "DBMS overview and database definition feature video lecture", "Powerpoint lecture notes for lesson 3", "Non-procedural access video lecture", "Powerpoint lecture notes for lesson 4", "Transaction processing overview video lecture", "Powerpoint lecture notes for lesson 5", "Data warehouse processing overview video lecture", "Powerpoint lecture notes for lesson 6", "DBMS technology evolution video lecture", "Powerpoint lecture notes for lesson 7", "Optional reading", "Module02 Quiz"], "title": "Introduction to Databases and DBMSs"}, {"description": "Now that you have the informational context for database features and environments, you’ll start building! In this module, you’ll learn relational data model terminology, integrity rules, and the CREATE TABLE statement. You’ll apply what you’ve learned in practice and graded problems using a database management system (DBMS), either Oracle or MySQL, creating tables using the SQL CREATE TABLE statement and populating your tables using given SQL INSERT statements.\n", "video": ["Basics of relational databases video lecture", "Powerpoint lecture notes for lesson 1 and extras", "Integrity rules video lecture", "Powerpoint lecture notes for lesson 2", "Basic SQL CREATE TABLE statement video lecture", "Powerpoint lecture notes for lesson 3", "University database statements for Oracle and MySQL", "Integrity constraint syntax video lecture", "Powerpoint lecture notes for lesson 4", "Assignment 1 Notes video lecture", "Powerpoint lecture notes for lesson 5", "Optional reading", "DBMS installation and configuration notes", "Practice Problems for Module 3", "Quiz for Module 3 practice problems", "Extra Problems for Module 3", "Assignment for Module 3", "Module 3 Assignment"], "title": "Relational Data Model and the CREATE TABLE Statement"}, {"description": "This module is all about acquiring query formulation skills. Now that you know the relational data model and have basic skills with the CREATE TABLE statement, we can cover basic syntax of the SQL SELECT statement and the join operator for combining tables. SELECT statement examples are presented for single table conditions, join operations, and grouping operations. You’ll practice writing simple SELECT statements using the tables that you created in the assignment for module 3.\n", "video": ["SQL Overview video lecture", "Powerpoint lecture notes for lesson 1", "University database examples", "SELECT statement introduction video lecture", "Powerpoint lecture notes for lesson 2", "Join Operator video lecture", "Powerpoint lecture notes for lesson 3", "Using Join operations in SQL SELECT statements video lecture", "Powerpoint lecture notes for lesson 4", "GROUP BY clause video lecture", "Powerpoint lecture notes for lesson 5", "Practice Problems for Module 4", "Quiz for Module 4 Practice Problems", "Extra Problems for Module 4", "Assignment for Module 4", "Optional reading", "DBMS installation and configuration notes", "Module 4 Assignment"], "title": "Basic Query Formulation with SQL"}, {"description": "Now that you can identify and use the SELECT statement and the join operator, you’ll extend your problem solving skills in this module so you can gain confidence on more complex queries. You will work on retrieval problems with multiple tables and grouping. In addition, you’ll learn to use the UNION operator in the SQL SELECT statement and write SQL modification statements.\n", "video": ["Query formulation guidelines video lecture", "Powerpoint lecture notes for lesson 1 and extras", "Multiple table problems video lecture", "Powerpoint lecture notes for lesson 2", "Problems involving join and grouping operations video lecture", "Powerpoint lecture notes for lesson 3", "SQL set operators video lecture", "Powerpoint lecture notes for lesson 4", "SQL modification statements video lecture", "Powerpoint lecture notes for lesson 5", "Optional textbook reading material", "DBMS installation and configuration notes", "Practice Problems for Module 5", "Quiz for Module 5 Practice Problems", "Extra Problems for Module 5", "Assignment for Module 5", "Module 5 Assignment"], "title": "Extended Query Formulation with SQL"}, {"description": "Module 6 represents another shift in your learning. In previous modules, you’ve created and populated tables and developed query formulation skills using the SQL SELECT statement. Now you’ll start to develop skills that allow you to create a database design to support business requirements. You’ll learn basic notation used in entity relationship diagrams (ERDs), a graphical notation for data modeling. You will create simple ERDs using basic diagram symbols and relationship variations to start developing your data modeling skills. \n", "video": ["Database development goals video lecture", "Powerpoint lecture notes for lesson 1 and extras", "Basic ERD notation video lecture", "Powerpoint lecture notes for lesson 2", "Relationship variations I video lecture", "Powerpoint lecture notes for lesson 3", "Relationship variations II video lecture", "Powerpoint lecture notes for lesson 4", "Optional textbook reading material", "Practice Problems for Module 6", "Assignment for Module 6", "Module 6 Assignment"], "title": "Notation for Entity Relationship Diagrams"}, {"description": "Module 7 builds on your knowledge of database development using basic ERD symbols and relationship variations. We’ll be practicing precise usage of ERD notation and basic problem solving skills. You will learn about diagram rules and work problems to help you gain confidence using and creating ERDs.\n", "video": ["Basic diagram rules video lecture", "Powerpoint lecture notes for lesson 1 and extras", "Extended diagram rules video lecture", "Powerpoint lecture notes for lesson 2", "ERD problems I video lecture", "Powerpoint lecture notes for lesson 3", "ERD problems II video lecture", "Powerpoint lecture notes for lesson 4", "ER Assistant Demonstration video", "ER Assistant download", "Optional textbook reading material", "Practice Problems for Module 7", "Assignment for Module 7", "Module 7 Assignment"], "title": "ERD Rules and Problem Solving"}, {"description": "In Module 8, you’ll use your ERD notation skills and your ability to avoid diagram errors to develop ERDs that satisfy specific business data requirements. You will learn and practice powerful problem-solving skills as you analyze narrative statements and transformations to generate alternative ERDs.\n", "video": ["Conceptual data modeling goals and challenges", "Powerpoint lecture notes for lesson 1 and extras", "Analyzing narrative problems", "Powerpoint lecture notes for lesson 2", "Design transformations I", "Powerpoint lecture notes for lesson 3", "Design transformations II video lecture", "Powerpoint lecture notes for lesson 4", "Optional textbook reading material", "Practice Problems for Module 8", "Assignment for Module 8", "Module 8 Assignment "], "title": "Developing Business Data Models"}, {"description": "Now that you have practiced data modeling techniques, you’ll get to wrestle with narrative problem analyses and transformations for generating alternative database designs in Module 9. At the end of this module, you’ll learn guidelines for documentation and detection of design errors that will serve you well as you design databases for business situations.\n", "video": ["Data modeling problems I video lecture", "Powerpoint lecture notes for lesson 1 and extras", "Data modeling problems II video lecture", "Powerpoint lecture notes for lesson 2", "Finalizing an ERD video lecture", "Powerpoint lecture notes for lesson 3", "Optional textbook reading material", "Practice Problems for Module 9", "Assignment for Module 9", "Module 9 Assignment"], "title": "Data Modeling Problems and Completion of an ERD"}, {"description": "Modules 6 to 9 covered conceptual data modeling, emphasizing precise usage of ERD notation, analysis of narrative problems, and generation of alternative designs. Modules 10 and 11 cover logical database design, the next step in the database development process. In Module 10, we’ll cover schema conversion, the first step in the logical database design phase. You will learn to convert an ERD into a table design that can be implemented on a relational DBMS.\n", "video": ["Goals and steps of logical database design video lecture", "Powerpoint lecture notes for lesson 1 and extras", "Conversion rules video lecture", "Powerpoint lecture notes for lesson 2", "Conversion problems video lecture", "Powerpoint lecture notes for lesson 3", "Optional textbook reading material", "Practice Problems for Module 10", "Assignment for Module 10", "Module 10 Assignment"], "title": "Schema Conversion"}, {"description": "Module 11 covers normalization, the second part of the logical database design process. Normalization provides tools to remove unwanted redundancy in a table design. You’ll discover the motivation for normalization, constraints to reason about unwanted redundancy, and rules that detect excessive redundancy in a table design. You’ll practice integrating and applying normalization techniques in the final lesson of this course.\n", "video": ["Modification anomalies video lecture", "Powerpoint lecture notes for lesson 1 and extras", "Functional dependencies video lecture", "Powerpoint lecture notes for lesson 2", "Normal forms video lecture", "Powerpoint lecture notes for lesson 3", "Practical concerns video lecture", "Powerpoint lecture notes for lesson 4", "Normalization problems video lecture", "Powerpoint lecture notes for lesson 5", "Course Conclusion", "Optional textbook reading materials", "Practice Problems for Module 11", "Assignment for Module 11", "Module 11 Assignment"], "title": "Normalization Concepts and Practice"}], "title": "Database Management Essentials"}, {"course_info": "About this course: Learn the general concepts of data mining along with basic methodologies and applications. Then dive into one subfield in data mining: pattern discovery. Learn in-depth concepts, methods, and applications of pattern discovery in data mining. We will also introduce methods for pattern-based classification and some interesting applications of pattern discovery. This course provides you the opportunity to learn skills and content to practice and engage in scalable pattern discovery methods on massive transactional data, discuss pattern evaluation measures, and study methods for mining diverse kinds of patterns, sequential patterns, and sub-graph patterns.", "level": null, "package_name": "Data Mining  Specialization ", "created_by": "University of Illinois at Urbana-Champaign", "package_num": "1", "teach_by": [{"name": "John C. Hart", "department": "Department of Computer Science"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course.", "video": ["Welcome to Data Visualization!", "Syllabus", "About the Discussion Forums", "Updating Your Profile", "Social Media", "Resources", "Orientation Quiz"], "title": "Course Orientation"}, {"description": "In this week's module, you will learn what data visualization is, how it's used, and how computers display information. You'll also explore different types of visualization and how humans perceive information.", "video": ["Week 1 Overview", "Week 1 Introduction", "How the Programming Assignments Work", "1.1.1. Some Books on Data Visualization", "1.1.2. Overview of Visualization", "1.2.1. 2-D Graphics", "SVG-example", "1.2.2. 2-D Drawing", "1.2.3. 3-D Graphics", "1.2.4. Photorealism", "1.2.5. Non-Photorealism", "1.3.1. The Human", "1.3.2. Memory", "1.3.3. Reasoning", "1.3.4. The Human Retina", "1.3.5. Perceiving Two Dimensions", "1.3.6. Perceiving Perspective", "Week 1 Discussion ", "Week 1 Quiz"], "title": "Week 1: The Computer and the Human"}, {"description": "In this week's module, you will start to think about how to visualize data effectively. This will include assigning data to appropriate chart elements, using glyphs, parallel coordinates, and streamgraphs, as well as implementing principles of design and color to make your visualizations more engaging and effective.", "video": ["Week 2 Overview", "Week 2 Introduction", "2.1.1. Data", "2.1.2. Mapping", "2.1.3. Charts", "2.2.1. Glyphs (Part 1)", "2.2.1. Glyphs (Part 2)", "2.2.2. Parallel Coordinates", "2.2.3. Stacked Graphs (Part 1)", "2.2.3. Stacked Graphs (Part 2)", "2.3.1. Tufte's Design Rules", "2.3.2. Using Color", "Programming Assignment 1: Visualize Data Using a Chart", "Programming Assignment 1 Rubric", "Programming Assignment 1 Help Forum", "Programming Assignment 1"], "title": "Week 2: Visualization of Numerical Data"}, {"description": "In this week's module, you will learn how to visualize graphs that depict relationships between data items. You'll also plot data using coordinates that are not specifically provided by the data set.", "video": ["Week 3 Overview", "Week 3 Introduction", "3.1.1. Graphs and Networks", "3.1.2. Embedding Planar Graphs", "3.1.3. Graph Visualization", "3.1.4. Tree Maps", "3.2.1. Principal Component Analysis", "3.2.2. Multidimensional Scaling", "3.3.1. Packing", "Programming Assignment 2: Visualize Network Data", "Programming Assignment 2 Rubric", "Programming Assignment 2 Help Forum", "Programming Assignment 2"], "title": "Week 3: Visualization of Non-Numerical Data"}, {"description": "In this week's module, you will start to put together everything you've learned by designing your own visualization system for large datasets and dashboards. You'll create and interpret the visualization you created from your data set, and you'll also apply techniques from user-interface design to create an effective visualization system.", "video": ["Week 4 Overview", "Week 4 Introduction", "4.1.1. Visualization Systems", "4.1.2. The Information Visualization Mantra: Part 1", "4.1.2. The Information Visualization Mantra: Part 2", "4.1.2. The Information Visualization Mantra: Part 3", "4.1.3. Database Visualization Part: 1", "4.1.3. Database Visualization Part: 2", "4.1.3. Database Visualization Part: 3", "4.2.1. Visualization System Design", "Week 4 Quiz"], "title": "Week 4: The Visualization Dashboard"}], "title": "Data Visualization"}, {"course_info": "About this course: Ever wonder how major search engines such as Google, Bing and Yahoo rank your website within their searches? Or how content such as videos or local listings are shown and ranked based on what the search engine considers most relevant to users? Welcome to the world of Search Engine Optimization (SEO). This course is the first within the SEO Specialization and it is intended to give you a taste of SEO. \n\nYou will be introduced to the foundational elements of how search engines work, how the SEO landscape has changed and what you can expect in the future. You discuss core SEO strategies and tactics used to drive more organic search results to a specific website or set of websites, as well as tactics to avoid to prevent penalization from search engines. You will also discover how to position yourself for a successful career in SEO should this subject prove interesting to you. We hope this taste of SEO, will entice you to continue through the Specialization.", "level": null, "package_name": "Search Engine Optimization (SEO) Specialization ", "created_by": "University of California, Davis", "package_num": "1", "teach_by": [{"name": "Rebekah May", "department": "LeadQual & UC Davis Extension"}], "target_audience": "Who is this class for: This course is primarily aimed at people interested in SEO, but have limited to no knowledge of the subject. Expect to learn foundational, basic elements.", "rating": "4.6", "week_data": [{"description": "Welcome to the first week of the course! In the lessons that follow, you will discuss and define the foundational overview of SEO and explore types of careers and salary expectations within SEO industry. By the end of this module, you should be able to define Search Engine Optimization and explain the basics of SEO as a business (as well as how SEO shapes the Internet itself.) You'll also be able to explain the differences between three main SEO job types, and be prepared to choose a career that best suits your current goals. ", "video": ["Course Introduction and Specialization", "Introduction", "Introduction to SEO", "Why a Career in SEO?", "What You Can Expect from a Career in SEO", "What You Can Expect from a Career in SEO", "How I Got Started in SEO", "My Role as an SEO Manager", "Key Skills We Look for When Hiring", "Tips for Finding a Job", "How I Got Started in SEO", "A Day in the Life of an SEO Professional", "Key Skills We Look for When Hiring", "Specific Skills Managers Want", "Tips for Finding a Job", "Tips for Interviewing", "Building a Marketable Portfolio", "Overview of In-house SEO", "Building a Successful Career in SEO", "Getting Started & Introduction to SEO"], "title": "Getting Started & Introduction to SEO"}, {"description": "Welcome to Module 2! We hope you're enjoying the course so far, and are ready to learn more about the evolution of SEO! By the end of this module, you should be able to at least summarize the timeline of search engine development, as well as demonstrate an understanding of key time periods and individuals who changed the way search worked and the way humans interact with the Web. Later on in the module, you'll gain the ability to critique the role of advertisements and corporate funding in the development of search, and explain the process by which the Web became monetized. This high-level look at the history of SEO will give you a solid foundation for understanding the material we'll be covering later in the course. ", "video": ["Evolution of SEO", "How Search Engines Work", "How Search Engines Work", "History of Search Engines (1945 - 1993)", "History of Search Engines (1945 - 1993)", "Advancement of Search (1993 - 2000)", "Building a Search Engine?", "Evolution of SEO"], "title": "Evolution of SEO"}, {"description": "In Module 3, we'll be discussing items that SEOs spend a great deal of time dealing with: SEO best practices, the algorithm updates that look for them and potential penalties for not adhering to them. By the end of this module, you'll be able to illustrate the concept of relevancy as it applies to search results, compare and contrast the functionality of search engine algorithm updates, and critically examine the ways in which webmasters attempt to circumvent these algorithms. You'll also be able to define important ranking factors used by modern search engines, and learn the necessary steps to avoid (or correct) any penalties applied by search engine algorithms. ", "video": ["Current SEO Best Practices", "Introduction to Search Engine Algorithms", "Introduction to Search Engine Algorithms", "SEO Best Practices and Ranking Factors", "SEO Best Practices and Ranking Factors", "Early Algorithms: Florida, Austin, and Brandy", "Updates: Personalized, Universal, Vince, and Caffeine", "Personalized, Universal, Vince, and Caffeine", "Panda: The Game Changer for Content", "Panda: The Game Changer for Content", "Cleaning Up Links with Penguin", "Cleaning Up Links with Penguin", "Refinements: Hummingbird, Pigeon, and Mobile", "Hummingbird, Pigeon, and Mobile", "How You Can Avoid Penalties", "How You Can Avoid Penalties", "Is SEO Spam?", "Current SEO Best Practices"], "title": "Current SEO Best Practices"}, {"description": "You've made it to Module 4! In this module, you will gain an understanding of where SEO fits into the broader digital marketing landscape. By the end of this module, you will be able to explain how concepts like topic association and semantic analysis relate to the relevancy and trustworthiness of search results. You'll also be well prepared to write and optimize your own content for a website that will improve search results, as well as develop an optimization strategy for a client to implement that would help to increase their ranking while following best practices. Finally, by the end of this module you will be able to demonstrate the impact of brands and branding on search results, and critically analyze the role of social media and other emerging technologies on the landscape of SEO today, tomorrow, and beyond!", "video": ["SEO of Today, Tomorrow, and Beyond", "Evolution of Keyword Optimization", "Strengthening Your Keyword Strategy", "Strengthening Your Keyword Strategy", "How Does Branding Influence Website Rank?", "How Does Branding Influence Website Rank?", "Share a question", "Poll Family & Friends", "Course Summary and Wrap-Up", "SEO of Today, Tomorrow and Beyond"], "title": "SEO of Today, Tomorrow and Beyond & Course Wrap-Up"}], "title": "Introduction to Search Engine Optimization"}, {"course_info": "About this course: This course describes Bayesian statistics, in which one's inferences about parameters or hypotheses are updated as evidence accumulates. You will learn to use Bayes’ rule to transform prior probabilities into posterior probabilities, and be introduced to the underlying theory and perspective of the Bayesian paradigm. The course will apply Bayesian methods to several practical problems, to show end-to-end Bayesian analyses that move from framing the question to building models to eliciting prior probabilities to implementing in R (free statistical software) the final posterior distribution. Additionally, the course will introduce credible regions, Bayesian comparisons of means and proportions, Bayesian regression and inference using multiple models, and discussion of Bayesian prediction.\n\nWe assume learners in this course have background knowledge equivalent to what is covered in the earlier three courses in this specialization: \"Introduction to Probability and Data,\" \"Inferential Statistics,\" and \"Linear Regression and Modeling.\"", "level": "Intermediate", "package_name": "Statistics with R Specialization ", "created_by": "Duke University", "package_num": "4", "teach_by": [{"name": "Mine Çetinkaya-Rundel", "department": "Department of Statistical Science"}, {"name": "David Banks", "department": "Statistical Science"}, {"name": "Colin Rundel ", "department": "Statistical Science"}, {"name": "Merlise A Clyde", "department": "Department of Statistical Science"}], "target_audience": null, "rating": "3.8", "week_data": [{"description": "This short module introduces basics about Coursera specializations and courses in general, this specialization: Statistics with R, and this course: Bayesian Statistics. Please take several minutes read this information. Thanks for joining us in this course!", "video": ["Introduction to Statistics with R", "About Statistics with R Specialization", "About Bayesian Statistics", "Special Thanks", "Weekly Feedback Forms", "Introduce Yourself"], "title": "About the Specialization and the Course"}, {"description": "<p>Welcome! Over the next several weeks, we will together explore Bayesian statistics. <p>In this module, we will work with conditional probabilities, which is the probability of event B given event A. Conditional probabilities are very important in medical decisions. By the end of the week, you will be able to solve problems using Bayes' rule, and update prior probabilities.</p><p>Please use the learning objectives and practice quiz to help you learn about Bayes' Rule, and apply what you have learned in the lab and on the quiz. ", "video": ["The Basics of Bayesian Statistics", "Module Learning Objectives", "Conditional Probabilities and Bayes' Rule", "Bayes' Rule and Diagnostic Testing", "Bayes Updating", "Bayesian vs. frequentist definitions of probability", "Inference for a Proportion: Frequentist Approach", "Inference for a Proportion: Bayesian Approach", "Effect of Sample Size on the Posterior", "Frequentist vs. Bayesian Inference", "Week 1 Practice Quiz", "Week 1 Lab Instructions", "Week 1 Feedback Form", "Week 1 Quiz", "Week 1 Lab"], "title": "The Basics of Bayesian Statistics"}, {"description": "In this week, we will discuss the continuous version of Bayes' rule and show you how to use it in a conjugate family, and discuss credible intervals. By the end of this week, you will be able to understand and define the concepts of prior, likelihood, and posterior probability and identify how they relate to one another.", "video": ["Bayesian Inference", "Module Learning Objectives", "From the Discrete to the Continuous", "Elicitation", "Conjugacy", "Inference on a Binomial Proportion", "The Gamma-Poisson Conjugate Families", "The Normal-Normal Conjugate Families", "Non-Conjugate Priors", "Credible Intervals", "Predictive Inference", "Week 2 Practice Quiz", "Week 2 Lab Instructions", "Weekly Feedback Form", "Week 2 Quiz", "Week 2 Lab"], "title": "Bayesian Inference"}, {"description": "In this module, we will discuss Bayesian decision making, hypothesis testing, and Bayesian testing. By the end of this week, you will be able to make optimal decisions based on Bayesian statistics and compare multiple hypotheses using Bayes Factors. ", "video": ["Decision making", "Module Learning Objectives", "Losses and decision making", "Working with loss functions", "Minimizing expected loss for hypothesis testing", "Posterior probabilities of hypotheses and Bayes factors", "Comparing two proportions using Bayes factors: assumptions", "Comparing two proportions using Bayes factors", "Comparing two paired means using Bayes factors", "What to report?", "Posterior probability, p-values and paradoxes", "Comparing two independent means", "Comparing two independent means: hypothesis testing", "Week 3 Practice Quiz", "Week 3 Lab Instructions", "Weekly Feedback Form", "Week 3 Quiz", "Week 3 Lab"], "title": "Decision Making"}, {"description": "This week, we will look at Bayesian linear regressions and model averaging, which allows you to make inferences and predictions using several models. By the end of this week, you will be able to implement Bayesian model averaging, interpret Bayesian multiple linear regression and understand its relationship to the frequentist linear regression approach. ", "video": ["Bayesian regression", "Module Learning Objectives", "Bayesian simple linear regression", "Checking for outliers", "Bayesian multiple regression", "Model selection criteria", "Bayesian model uncertainty", "Bayesian model averaging", "Stochastic exploration", "Priors for Bayesian model uncertainty", "R demo: crime and punishment", "Decisions under model uncertainty", "Week 4 Practice Quiz", "Week 4 Lab Instructions", "Week 4 Lab Supplement", "Weekly Feedback Form", "Week 4 Quiz", "Week 4 Lab"], "title": "Bayesian Regression"}, {"description": "This week consists of interviews with statisticians on how they use Bayesian statistics in their work, as well as the final project in the course.", "video": ["About this module", "Bayesian inference: a talk with Jim Berger", "Bayesian methods and big data: a talk with David Dunson", "Bayesian methods in biostatistics and public health: a talk with Amy Herring", "Weekly Feedback Form"], "title": "Perspectives on Bayesian Applications"}, {"description": "In this module you will use the data set provided to complete and report on a data analysis question. Please read the background information, review the report template (downloaded from the link in Lesson Project Information), and then complete the peer review assignment. ", "video": ["Project information", "Weekly Feedback Form", "Data Analysis Project"], "title": "Data Analysis Project"}], "title": "Bayesian Statistics"}, {"course_info": "About this course: The simple spreadsheet is one of the most powerful data analysis tools that exists, and it’s available to almost anyone. Major corporations and small businesses alike use spreadsheet models to determine where key measures of their success are now, and where they are likely to be in the future. But in order to get the most out of a spreadsheet, you have know how to use it. This course is designed to give you an introduction to basic spreadsheet tools and formulas so that you can begin harness the power of spreadsheets to map the data you have now and to predict the data you may have in the future. Through short, easy-to-follow demonstrations, you’ll learn how to use Excel or Sheets so that you can begin to build models and decision trees in future courses in this Specialization. \nBasic familiarity with, and access to, Excel or Sheets is required.", "level": null, "package_name": "Business and Financial Modeling Specialization ", "created_by": "University of Pennsylvania", "package_num": "2", "teach_by": [{"name": "Don Huesman", "department": "Innovation Group- Wharton School"}], "target_audience": null, "rating": "4.2", "week_data": [{"description": "This module was designed to introduce you to the history of spreadsheets, their basic capabilities, and how they can be used to create models. You'll learn the different types of data used in spreadsheets, spreadsheet notations for mathematical operations, common built-in formulas and functions, conditional expressions, relative and absolute references, and how to identify and correct circular references. By the end of this module, you'll understand the context of spreadsheets, be able to navigate a spreadsheet, use built-in formulas and functions in spreadsheets, create your own simple formulas, and identify and correct common errors so you can put spreadsheets to work for you.", "video": ["1.1 Objectives and a little spreadsheet history", "1.2 Navigating a spreadsheet and crafting formulas", "1.3 Using functions", "1.4 Using conditional expressions in formulas", "1.5 Common errors in spreadsheets", "1.6 Differences between Sheets and Excel", "PDF of Module 1 Lecture Slides", "Module 1 examples", "Module 1 Quiz: History, Formulas, Functions and Errors"], "title": "Spreadsheets: A Tool for Thinking with Numbers"}, {"description": "In this module, you'll move from spreadsheet to model, so you can begin to create your own models that reflect real-world events. You'll learn how to organize and lay out model elements, as well as the types of objective functions and their use. You'll also learn what-if analysis and scenarios, sensitivity analysis, and other classic models. By the end of this module, you'll be able to design a spreadsheet reflecting assumptions, decision variables, and outcomes, create a basic cashflow model, evaluate a small business opportunity, conduct what-if analysis, identify key variables using sensitivity analysis, and linear programming models and deterministic models.", "video": ["2.0 Module Introduction", "2.1 Using assumptions and decision variables in spreadsheet models", "2.2 Structuring a spreadsheet to model variables, objectives, and objective functions", "2.3 Constructing simple cashflow model", "2.4 What-if analysis and sensitivity analysis", "2.5 Limits to simple, deterministic models", "PDF of Module 2 Lecture Slides", "Module 2 examples", "Module 2 Quiz: Classic Models"], "title": "From Spreadsheet to Model"}, {"description": "This module was designed to introduce you to how you can use spreadsheets to address uncertainty and probability. You'll learn about random variables, probability distributions, power, exponential, and log functions in model formulas, models for calculating probability trees and decision trees, how to use regression tools to make predictions, as well as multiple regression. By the end of this module, you'll be able to measure correlations between variables using spreadsheet statistical functions, understand the results of functions that calculate correlations, use regression tools to make predictions, and improve forecasts with multiple regression.", "video": ["3.0 Introduction", "3.1 Random variables and probability distributions", "3.2 Changes in discrete and continuous time", "3.3 Power, exponential, and log functions", "3.4 Probability trees and decision trees", "3.5 Correlation and Regression", "PDF of Module 3 Lecture Slides", "Module 3 examples", "Additional reading on exponential and other functions", "Module 3 Quiz: Probability, Correlation, and Regression"], "title": "Addressing Uncertainty and Probability in Models"}, {"description": "In this module, you'll learn to use spreadsheets to implement Monte Carlo simulations as well as linear programs for optimization. You'll examine the purpose of Monte Carlo simulations, how to implement Monte Carlo simulations in spreadsheets, the types of problems you can address with linear programs and how to implement those linear programs in spreadsheets. By the end of this module, you'll be able to model uncertainty and risk in spreadsheets, and use Excel's solver to optimize resources to reach a desired outcome.  You'll also be able to identify the similarities and differences between Excel and Sheets, and be prepared for the next course in the Business and Financial Modeling Specialization.", "video": ["4.0 Introduction", "4.1 Monte Carlo Simulations", "4.2 Linear Programming", "4.3 Next Steps, and Differences between Excel and Sheets", "PDF of Module 4 Lecture Slides", "Module 4 examples", "Links and other resources for further study", "Module 4 Quiz: Simulations, Scenarios, and Optimization"], "title": " Simulation and Optimization"}], "title": "Introduction to Spreadsheets and Models"}, {"course_info": "About this course: This course introduces simple and multiple linear regression models. These models allow you to assess the relationship between variables in a data set and a continuous response variable. Is there a relationship between the physical attractiveness of a professor and their student evaluation scores? Can we predict the test score for a child based on certain characteristics of his or her mother? In this course, you will learn the fundamental theory behind linear regression and, through data examples, learn to fit, examine, and utilize regression models to examine relationships between multiple variables, using the free statistical software R and RStudio.", "level": "Beginner", "package_name": "Statistics with R Specialization ", "created_by": "Duke University", "package_num": "3", "teach_by": [{"name": "Mine Çetinkaya-Rundel", "department": "Department of Statistical Science"}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "This short module introduces basics about Coursera specializations and courses in general, this specialization: Statistics with R, and this course: Linear Regression and Modeling. Please take several minutes to browse them through. Thanks for joining us in this course!", "video": ["Introduction to Statistics with R", "About Statistics with R Specialization", "More about Linear Regression and Modeling"], "title": "About Linear Regression and Modeling"}, {"description": "In this week we’ll introduce linear regression. Many of you may be familiar with regression from reading the news, where graphs with straight lines are overlaid on scatterplots. Linear models can be used for prediction or to evaluate whether there is a linear relationship between two numerical variables. ", "video": ["Lesson Learning Objectives", "Introduction", "Correlation", "Residuals", "Least Squares Line", "Lesson Learning Objectives", "Prediction and Extrapolation", "Conditions for Linear Regression", "R Squared", "Regression with Categorical Explanatory Variables", "Week 1 Suggested Readings and Practice", "Week 1 Practice Quiz", "Week 1 Quiz"], "title": "Linear Regression"}, {"description": "Welcome to week 2! In this week, we will look at outliers, inference in linear regression and variability partitioning. Please use this week to strengthen your understanding on linear regression. Don't forget to post your questions, concerns and suggestions in the discussion forum!", "video": ["Lesson Learning Objectives", "Outliers in Regression", "Inference for Linear Regression", "Variability Partitioning", "Week 2 Suggested Readings and Exercises", "Week 2 Practice Quiz", "Instructions for Week 1 & 2 Lab", "Week 2 Quiz", "Week 1 & 2 Lab"], "title": "More about Linear Regression"}, {"description": "In this week, we’ll explore multiple regression, which allows us to model numerical response variables using multiple predictors (numerical and categorical). We will also cover inference for multiple linear regression, model selection, and model diagnostics. Hope you enjoy!", "video": ["Introduction", "Lesson Learning Objectives", "Multiple Predictors", "Adjusted R Squared", "Collinearity and Parsimony", "Lesson Learning Objectives", "Inference for MLR", "Model Selection", "Diagnostics for MLR", "Week 3 Suggested Readings and Exercises", "Week 3 Practice Quiz", "Instructions for Week 3 Lab", "Week 3 Quiz", "Week 3 Lab"], "title": "Multiple Regression"}, {"description": "In this week you will use the data set provided to complete and report on a data analysis question. Please read the background information, review the report template (downloaded from the link in Lesson Project Information), and then complete the peer review assignment. ", "video": ["Project Files and Rubric", "Data Analysis Project"], "title": "Final Project"}], "title": "Linear Regression and Modeling "}, {"course_info": "About this course: Once you’ve identified a big data issue to analyze, how do you collect, store and organize your data using Big Data solutions?  In this course, you will experience various data genres and management tools appropriate for each.  You will be able to describe the reasons behind the evolving plethora of new big data platforms from the perspective of big data management systems and analytical tools.  Through guided hands-on tutorials, you will become familiar with techniques using real-time and semi-structured data examples.  Systems and tools discussed include: AsterixDB, HP Vertica, Impala, Neo4j, Redis, SparkSQL. This course provides techniques to extract value from existing untapped data sources and discovering new data sources.\n\nAt the end of this course, you will be able to:\n * Recognize different data elements in your own work and in everyday life problems\n * Explain why your team needs to design a Big Data Infrastructure Plan and Information System Design\n * Identify the frequent data operations required for various types of data\n * Select a data model to suit the characteristics of your data \n * Apply techniques to handle streaming data\n * Differentiate between a traditional Database Management System and a Big Data Management System\n * Appreciate why there are so many data management systems\n * Design a big data information system for an online game company\n\nThis course is for those new to data science.  Completion of Intro to Big Data is recommended.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  Refer to the specialization technical requirements for complete hardware and software specifications.\n\nHardware Requirements: \n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. \n\nSoftware Requirements: \nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+.", "level": null, "package_name": "Big Data Specialization ", "created_by": "University of California, San Diego", "package_num": "2", "teach_by": [{"name": "Ilkay Altintas", "department": "San Diego Supercomputer Center"}, {"name": "Amarnath Gupta", "department": "San Diego Supercomputer Center (SDSC)"}], "target_audience": null, "rating": "4.3", "week_data": [{"description": "Welcome to this course on big data modeling and management. Modeling and managing data is a central focus of all big data projects. In these lessons we introduce you to the concepts behind big data modeling and management and set the stage for the remainder of the course. ", "video": ["Welcome to Big Data Modeling and Management", "Why is this a New Course in the Big Data Specialization?", "Getting to know  you: Tell us about yourself and why you are taking this course", "Summary of Introduction to Big Data (Part 1)", "Summary of Introduction to Big Data (Part 2)", "Summary of Introduction to Big Data (Part 3)", "Slides: Summary of Introduction to Big Data", "Big Data Management \"Must-Ask Questions\"", "Data Ingestion", "Data Storage", "Data Quality", "Data Operations", "Data Scalability and Security", "Slides: Big Data Management", "Let's discuss: What area of big data management interests you most?", "Reading on Storage Systems", "Energy Data Management Challenges at ConEd", "Slides: Energy Data Management Challenges at ConEd", "Gaming Industry Data Management: Q&A with Apmetrix CTO Mark Caldwell", "Flight Data Management at FlightStats: A Lecture by CTO Chad Berkley", "Slides: Flight Data Management at FlightStats", "Let's discuss: What are the design criteria in the big data applications you have heard?", "Downloading and Installing the Cloudera VM Instructions (Windows)", "Downloading and Installing the Cloudera VM Instructions (Mac)", "Instructions for Downloading Hands On Datasets"], "title": "Introduction to Big Data Modeling and Management"}, {"description": "Modeling big data depends on many factors including data structure, which operations may be performed on the data, and what constraints are placed on the models. In these lessons you will learn the details about big data modeling and you will gain the practical skills you will need for modeling your own big data projects.", "video": ["Introduction to Data Models", "Data Model Structures", "Data Model Operations", "Data Model Constraints", "Slides: What Is A Data Model?", "Let's discuss: Modeling data in your daily life", "Introduction to CSV Data", "Introduction to CSV Data", "What is a Relational Data Model?", "Slides: What Is A Relational Data Model?", "What is a Semistructured Data Model?", "Slides: What is a Semistructured Data Model?", "Let's discuss: Utilization of XML or JSON on the Internet", "Exploring the Relational Data Model of Comma Separated Values (CSV)", "Exploring the Relational Data Model of CSV Files", "Exploring the Semistructured Data Model of JSON data", "Exploring the Semistructured Data Model of JSON data", "Exploring the Array Data Model of an Image", "Exploring the Array Data Model of an Image", "Exploring Sensor Data", "Exploring Sensor Data", "Practical Quiz for Week 2 Hands-On Lectures"], "title": "Big Data Modeling"}, {"description": "These lessons continue to shed light on big data modeling with specific approaches including vector space models, graph data models, and more. ", "video": ["Vector Space Model", "Slides: Vector Space Model", "Graph Data Model", "Slides: Graph Data Model", "Other Data Models", "Slides: Other Data Models", "Exploring Vector Data Models with Lucene", "Exploring the Lucene Search Engine's Vector Data Model", "Exploring Graph Data Models with Gephi", "Exploring Graph Data Models with Gephi", "Data Models Quiz"], "title": "Big Data Modeling (Part 2)"}, {"description": "Data models deal with many different types of data formats. Streaming data is becoming ubiquitous, and working with streaming data requires a different approach from working with static data. In these lessons you will gain practical hands-on experience working with different forms of streaming data including weather data and twitter feeds. ", "video": ["Data Model vs. Data Format", "Slides: Data Model vs. Data Format", "What is a Data Stream?", "Slides: What is a Data Stream?", "Why is Streaming Data different?", "Slides: Why is Streaming Data Different?", "Understanding Data Lakes", "Slides: Understanding Data Lakes", "Let's discuss: Streaming data applications", "Exploring Streaming Sensor Data", "Exploring Streaming Sensor Data", "Instructions for Creating a Twitter App (Optional)", "Exploring Streaming Twitter Data (Optional)", "Exploring Streaming Twitter Data (Optional)", "Data Formats and Streaming Data Quiz"], "title": "Working With Data Models"}, {"description": "Managing big data requires a different approach to database management systems because of the wide variation in data structure which does not lend itself to traditional DBMSs. There are many applications available to help with big data management. In these lessons we introduce you to some of these applications and provide insight into how and when they might be appropriate for your own big data management challenges. ", "video": ["DBMS-based and non-DBMS-based Approaches to Big Data", "Slides: DBMS-based and non-DBMS-based Approaches to Big Data", "From DBMS to BDMS", "Redis: An Enhanced Key-Value Store", "Aerospike: a New Generation KV Store", "Semistructured Data – AsterixDB", "Solr: Managing Text", "Relational Data – Vertica", "Slides: From DBMS to BDMS", "BDMS Quiz"], "title": "Big Data Management: The \"M\" in DBMS"}, {"description": "In these lessons we give you the opportunity to learn about big data modeling and management using a fictitious online game called \"Catch the Pink Flamingo\". ", "video": ["A Game by Eglence Inc. : Catch The Pink Flamingo", "Let's discuss: Analytical tasks to make Catch the Pink Flamingo better", "Let's discuss: Using the data model for Catch the Pink Flamingo", "Designing a Data Model for 'Catch the Pink Flamingo'"], "title": "Designing a Big Data Management System for an Online Game"}], "title": "Big Data Modeling and Management Systems"}, {"course_info": "About this course: Case Studies: Finding Similar Documents\n\nA reader is interested in a specific news article and you want to find similar articles to recommend.  What is the right notion of similarity?  Moreover, what if there are millions of other documents?  Each time you want to a retrieve a new document, do you need to search through all other documents?  How do you group similar documents together?  How do you discover new, emerging topics that the documents cover?   \n\nIn this third case study, finding similar documents, you will examine similarity-based algorithms for retrieval.  In this course, you will also examine structured representations for describing the documents in the corpus, including clustering and mixed membership models, such as latent Dirichlet allocation (LDA).  You will implement expectation maximization (EM) to learn the document clusterings, and see how to scale the methods using MapReduce.\n\nLearning Outcomes:  By the end of this course, you will be able to:\n   -Create a document retrieval system using k-nearest neighbors.\n   -Identify various similarity metrics for text data.\n   -Reduce computations in k-nearest neighbor search by using KD-trees.\n   -Produce approximate nearest neighbors using locality sensitive hashing.\n   -Compare and contrast supervised and unsupervised learning tasks.\n   -Cluster documents by topic using k-means.\n   -Describe how to parallelize k-means using MapReduce.\n   -Examine probabilistic clustering approaches using mixtures models.\n   -Fit a mixture of Gaussian model using expectation maximization (EM).\n   -Perform mixed membership modeling using latent Dirichlet allocation (LDA).\n   -Describe the steps of a Gibbs sampler and how to use its output to draw inferences.\n   -Compare and contrast initialization techniques for non-convex optimization objectives.\n   -Implement these techniques in Python.", "level": null, "package_name": "Machine Learning Specialization ", "created_by": "University of Washington", "package_num": "4", "teach_by": [{"name": "Emily Fox", "department": "Statistics"}, {"name": "Carlos Guestrin", "department": "Computer Science and Engineering"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "Clustering and retrieval are some of the most high-impact machine learning tools out there.  Retrieval is used in almost every applications and device we interact with, like in providing a set of products related to one a shopper is currently considering, or a list of people you might want to connect with on a social media platform.  Clustering can be used to aid retrieval, but is a more broadly useful tool for automatically discovering structure in data, like uncovering groups of similar patients.<p>This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have.", "video": ["Slides presented in this module", "Welcome and introduction to clustering and retrieval tasks", "Course overview", "Module-by-module topics covered", "Assumed background", "Software tools you'll need for this course", "A big week ahead!"], "title": "Welcome"}, {"description": "We start the course by considering a retrieval task of fetching a document similar to one someone is currently reading.  We cast this problem as one of nearest neighbor search, which is a concept we have seen in the Foundations and Regression courses.  However, here, you will take a deep dive into two critical components of the algorithms: the data representation and metric for measuring similarity between pairs of datapoints.  You will examine the computational burden of the naive nearest neighbor search algorithm, and instead implement scalable alternatives using KD-trees for handling large datasets and locality sensitive hashing (LSH) for providing approximate nearest neighbors, even in high-dimensional spaces.  You will explore all of these ideas on a Wikipedia dataset, comparing and contrasting the impact of the various choices you can make on the nearest neighbor results produced.", "video": ["Slides presented in this module", "Retrieval as k-nearest neighbor search", "1-NN algorithm", "k-NN algorithm", "Document representation", "Distance metrics: Euclidean and scaled Euclidean", "Writing (scaled) Euclidean distance using (weighted) inner products", "Distance metrics: Cosine similarity", "To normalize or not and other distance considerations", "Choosing features and metrics for nearest neighbor search", "Complexity of brute force search", "KD-tree representation", "NN search with KD-trees", "Complexity of NN search with KD-trees", "Visualizing scaling behavior of KD-trees", "Approximate k-NN search using KD-trees", "(OPTIONAL) A worked-out example for KD-trees", "Limitations of KD-trees", "LSH as an alternative to KD-trees", "Using random lines to partition points", "Defining more bins", "Searching neighboring bins", "LSH in higher dimensions", "(OPTIONAL) Improving efficiency through multiple tables", "Implementing Locality Sensitive Hashing from scratch", "A brief recap", "Representations and metrics", "Choosing features and metrics for nearest neighbor search", "KD-trees", "Locality Sensitive Hashing", "Implementing Locality Sensitive Hashing from scratch"], "title": "Nearest Neighbor Search"}, {"description": "In clustering, our goal is to group the datapoints in our dataset into disjoint sets.  Motivated by our document analysis case study, you will use clustering to discover thematic groups of articles by \"topic\".  These topics are not provided in this unsupervised learning task; rather, the idea is to output such cluster labels that can be post-facto associated with known topics like \"Science\", \"World News\", etc.  Even without such post-facto labels, you will examine how the clustering output can provide insights into the relationships between datapoints in the dataset.  The first clustering algorithm you will implement is k-means, which is the most widely used clustering algorithm out there.  To scale up k-means, you will learn about the general MapReduce framework for parallelizing and distributing computations, and then how the iterates of k-means can utilize this framework.  You will show that k-means can provide an interpretable grouping of Wikipedia articles when appropriately tuned.", "video": ["Slides presented in this module", "The goal of clustering", "An unsupervised task", "Hope for unsupervised learning, and some challenge cases", "The k-means algorithm", "k-means as coordinate descent", "Smart initialization via k-means++", "Assessing the quality and choosing the number of clusters", "Clustering text data with k-means", "Motivating MapReduce", "The general MapReduce abstraction", "MapReduce execution overview and combiners", "MapReduce for k-means", "Other applications of clustering", "A brief recap", "k-means", "Clustering text data with K-means", "MapReduce for k-means"], "title": "Clustering with k-means"}, {"description": "In k-means, observations are each hard-assigned to a single cluster, and these assignments are based just on the cluster centers, rather than also incorporating shape information.  In our second module on clustering, you will perform probabilistic model-based clustering that provides (1) a more descriptive notion of a \"cluster\" and (2) accounts for uncertainty in assignments of datapoints to clusters via \"soft assignments\".  You will explore and implement a broadly useful algorithm called expectation maximization (EM) for inferring these soft assignments, as well as the model parameters.  To gain intuition, you will first consider a visually appealing image clustering task.  You will then cluster Wikipedia articles, handling the high-dimensionality of the tf-idf document representation considered.", "video": ["Slides presented in this module", "Motiving probabilistic clustering models", "Aggregating over unknown classes in an image dataset", "Univariate Gaussian distributions", "Bivariate and multivariate Gaussians", "Mixture of Gaussians", "Interpreting the mixture of Gaussian terms", "Scaling mixtures of Gaussians for document clustering", "Computing soft assignments from known cluster parameters", "(OPTIONAL) Responsibilities as Bayes' rule", "Estimating cluster parameters from known cluster assignments", "Estimating cluster parameters from soft assignments", "EM iterates in equations and pictures", "Convergence, initialization, and overfitting of EM", "Relationship to k-means", "(OPTIONAL) A worked-out example for EM", "A brief recap", "Implementing EM for Gaussian mixtures", "Clustering text data with Gaussian mixtures", "EM for Gaussian mixtures", "Implementing EM for Gaussian mixtures", "Clustering text data with Gaussian mixtures"], "title": "Mixture Models"}, {"description": "The clustering model inherently assumes that data divide into disjoint sets, e.g., documents by topic.  But, often our data objects are better described via memberships in a collection of sets, e.g., multiple topics.  In our fourth module, you will explore latent Dirichlet allocation (LDA) as an example of such a mixed membership model particularly useful in document analysis.  You will interpret the output of LDA, and various ways the output can be utilized, like as a set of learned document features.  The mixed membership modeling ideas you learn about through LDA for document analysis carry over to many other interesting models and applications, like social network models where people have multiple affiliations.<p>Throughout this module, we introduce aspects of Bayesian modeling and a Bayesian inference algorithm called Gibbs sampling.  You will be able to implement a Gibbs sampler for LDA by the end of the module.", "video": ["Slides presented in this module", "Mixed membership models for documents", "An alternative document clustering model", "Components of latent Dirichlet allocation model", "Goal of LDA inference", "The need for Bayesian inference", "Gibbs sampling from 10,000 feet", "A standard Gibbs sampler for LDA", "What is collapsed Gibbs sampling?", "A worked example for LDA: Initial setup", "A worked example for LDA: Deriving the resampling distribution", "Using the output of collapsed Gibbs sampling", "A brief recap", "Modeling text topics with Latent Dirichlet Allocation", "Latent Dirichlet Allocation", "Learning LDA model via Gibbs sampling", "Modeling text topics with Latent Dirichlet Allocation"], "title": "Mixed Membership Modeling via Latent Dirichlet Allocation"}, {"description": "In the conclusion of the course, we will recap what we have covered.  This represents both techniques specific to clustering and retrieval, as well as foundational machine learning concepts that are more broadly useful.<p>We provide a quick tour into an alternative clustering approach called hierarchical clustering, which you will experiment with on the Wikipedia dataset.  Following this exploration, we discuss how clustering-type ideas can be applied in other areas like segmenting time series.  We then briefly outline some important clustering and retrieval ideas that we did not cover in this course.<p> We conclude with an overview of what's in store for you in the rest of the specialization.  ", "video": ["Slides presented in this module", "Module 1 recap", "Module 2 recap", "Module 3 recap", "Module 4 recap", "Why hierarchical clustering?", "Divisive clustering", "Agglomerative clustering", "The dendrogram", "Agglomerative clustering details", "Hidden Markov models", "Modeling text data with a hierarchy of clusters", "What we didn't cover", "Thank you!", "Modeling text data with a hierarchy of clusters"], "title": "Hierarchical Clustering & Closing Remarks"}], "title": "Machine Learning: Clustering & Retrieval"}, {"course_info": "About this course: Spreadsheet software remains one of the most ubiquitous pieces of software used in workplaces across the world. Learning to confidently operate this software means adding a highly valuable asset to your employability portfolio. In the United States alone, millions of job advertisements requiring Excel skills are posted every day. Research by Burning Glass Technologies and Capital One shows that digitals skills lead to higher income and better employment opportunities at a time when digital skills job are growing much faster than non-digital jobs.\n\nIn this second course of our Excel specialization Excel Skills for Business you will build on the strong foundations of the Essentials course. Intermediate Skills I will expand your Excel knowledge to new horizons. You are going to discover a whole range of skills and techniques that will become a standard component of your everyday use of Excel. In this course, you will build a solid layer of more advanced skills so you can manage large datasets and create meaningful reports. These key techniques and tools will allow you to add a sophisticated layer of automation and efficiency to your everyday tasks in Excel.\n\nOnce again, we have brought together a great teaching team that will be with you every step of the way. Prashan and Nicky will guide you through each week (and I am even going to make a guest appearance in Week 5 to help you learn about my favourite tool in Excel - shh, no spoilers!). Work through each new challenge step-by-step and in no time you will surprise yourself by how far you have come. This time around, we are going to follow Uma's trials and tribulations as she is trying to find her feet in a new position in the fictitious company PushPin. For those of you who have done the Essentials course, you will already be familiar with the company. Working through her challenges which are all too common ones that we encounter everyday, will help you to more easily relate to the skills and techniques learned in each week and apply them to familiar and new contexts.", "level": "Intermediate", "package_name": "Excel Skills for Business Specialization ", "created_by": "Macquarie University", "package_num": "2", "teach_by": [{"name": "Dr Yvonne Breyer", "department": "Faculty of Business and Economics"}], "target_audience": "Who is this class for: Completing Excel Skills for Business: Intermediate I will be a highly valuable asset for anyone wanting to improve their employment portfolio. This course is for anyone who works with Excel on a semi-regular or daily basis, for self-taught learners who want to fill gaps in their knowledge and for anyone wanting to consolidate their foundational knowledge of Excel.  ", "rating": "5.0", "week_data": [{"description": "", "video": ["Welcome to Excel Skills for Business: Intermediate I", "Course Goals and Weekly Learning Objectives", "Excel: Who, Why, How", "Week 1 Introduction", "Week 1 Discussion", "Read me before you start: Quizzes and Navigation", "Read me before you start: Versions and regions", "Download the Week 1 workbooks", "Practice Video: Multiple Worksheets", "Multiple Worksheets", "Practice Video: 3D Formulas", "3D Formulas", "Practice Video: Linking Workbooks", "Linking Workbooks", "Practice Video: Consolidating by Position", "Consolidating by Position", "Practice Video: Consolidating by Category (Reference)", "Consolidating by Category (Reference)", "Week 1 Wrap-Up", "Week 1: Practice Challenge", "Keyboard Shortcuts, Excel Terminology and Ninja Tips", "Test your Skills: Working with Multiple Worksheets & Workbooks"], "title": "Working with Multiple Worksheets & Workbooks"}, {"description": "", "video": ["Week 2 Introduction", "Week 2 Discussion", "Excel - how, when, where, why", "Download the Week 2 workbooks", "Practice Video: Combining Text (CONCAT, &)", "Combining Text (CONCAT, &)", "Practice Video: Changing Text Case (UPPER, LOWER, PROPER)", "Changing Text Case (UPPER, LOWER, PROPER)", "Practice Video: Extracting Text (LEFT, MID, RIGHT)", "Extracting Text (LEFT, MID, RIGHT)", "Practice Video: Finding Text (FIND)", "Finding Text (FIND)", "Practice Video: Date Calculations (NOW, TODAY, YEARFRAC)", "Date Calculations (NOW, TODAY, YEARFRAC)", "Week 2 Wrap-Up", "Week 2: Practice Challenge", "Keyboard Shortcuts, Excel Terminology and Ninja Tips", "Test your Skills: Text and Date Functions"], "title": "Text and Date Functions"}, {"description": "", "video": ["Week 3 Introduction", "Week 3 Discussion", "Download the Week 3 workbooks", "Practice Video: Introducing Named Ranges", "Introducing Named Ranges", "Practice Video: Creating Named Ranges", "Creating Named Ranges", "Practice Video: Managing Named Ranges", "Managing Named Ranges", "Practice Video: Named Ranges in Formulas", "Named Ranges in Formulas", "Practice Video: Apply Names", "Apply Names", "Week 3 Wrap-Up", "Week 3: Practice Challenge", "Keyboard Shortcuts, Terminology and Ninja Tips", "Test your Skills: Named Ranges"], "title": "Named Ranges"}, {"description": "", "video": ["Week 4 Introduction", "Week 4 Discussion", "Download the Week 4 workbooks", "Practice Video: COUNT functions", "COUNT functions", "Practice Video: Counting with Criteria (COUNTIFS)", "Counting with Criteria (COUNTIFS)", "Practice Video: Adding with Criteria (SUMIFS)", "Adding with Criteria (SUMIFS)", "Practice Video: Sparklines", "Sparklines", "Practice Video: Advanced Charting", "Advanced Charting", "Practice Video: Trendlines", "Trendlines", "Week 4 Wrap-Up", "Week 4: Practice Challenge", "Keyboard Shortcuts, Excel Terminology and Ninja Tips", "Test your Skills: Summarising Data"], "title": "Summarising Data"}, {"description": "", "video": ["Week 5 Introduction", "Week 5 Discussion", "Download the Week 5 workbooks", "Practice Video: Creating and Formatting Tables", "Creating and Formatting Tables", "Practice Video: Working with Tables", "Working with Tables", "Practice Video: Sorting and Filtering in Tables", "Sorting and Filtering in Tables", "Practice Video: Automation with Tables", "Automation with Tables", "Practice Video: Converting to Range and Subtotaling", "Converting to Range and Subtotaling", "Week 5 Wrap-Up", "Week 5: Practice Challenge", "Keyboard Shortcuts, Excel Terminology and Ninja Tips", "Test your Skills: Tables"], "title": "Tables"}, {"description": "", "video": ["Week 6 Introduction", "Week 6 Discussion", "Download the Week 6 workbooks", "Practice Video: Creating and Modifying a Pivot Table", "Creating and Modifying a Pivot Table", "Practice Video: Value Field Settings", "Value Field Settings", "Practice Video: Sorting and Filtering a Pivot Table", "Sorting and Filtering a Pivot Table", "Practice Video: Reporting Filter Pages", "Reporting Filter Pages", "Practice Video: Pivoting Charts", "Pivoting Charts", "Practice Video: Pivoting Slicers", "Pivoting Slicers", "Week 6 Wrap-Up", "Week 6: Practice Challenge", "Keyboard Shortcuts, Excel Terminology and Ninja Tips", "Test your Skills: Pivot Tables, Charts and Slicers"], "title": "Pivot Tables, Charts and Slicers"}, {"description": "", "video": ["Final Assessment"], "title": "Final Assessment"}], "title": "Excel Skills for Business: Intermediate I"}, {"course_info": "About this course: This 1-week accelerated on-demand course introduces participants to the Big Data and Machine Learning capabilities of Google Cloud Platform (GCP). It provides a quick overview of the Google Cloud Platform and a deeper dive of the data processing capabilities.\n\nAt the end of this course, participants will be able to:\n• Identify the purpose and value of the key Big Data and Machine Learning products in the Google Cloud Platform\n• Use CloudSQL and Cloud Dataproc to migrate existing MySQL and Hadoop/Pig/Spark/Hive workloads to Google Cloud Platform\n• Employ BigQuery and Cloud Datalab to carry out interactive data analysis\n• Choose between Cloud SQL, BigTable and Datastore\n• Train and use a neural network using TensorFlow\n• Choose between different data processing products on the Google Cloud Platform\n\nBefore enrolling in this course, participants should have roughly one (1) year of experience with one or more of the following:\n• A common query language such as SQL\n• Extract, transform, load activities\n• Data modeling\n• Machine learning and/or statistics\n• Programming in Python\n\nGoogle Account Notes:\n• You'll need a Google/Gmail account and a credit card or bank account to sign up for the Google Cloud Platform free trial (Google is currently blocked in China).\n• There is a known issue with certain EU countries where individuals are not able to sign up, but you may sign up as \"business\" status and intend to see a potential economic benefit from the trial. More details at: https://support.google.com/cloud/answer/6090602\n• More Google Cloud Platform free trial FAQs are available at: https://cloud.google.com/free-trial/", "level": "Intermediate", "package_name": "Data Engineering on Google Cloud Platform Specialization ", "created_by": "Google Cloud", "package_num": "1", "teach_by": [{"name": "Google Cloud Training", "department": null}], "target_audience": "Who is this class for: This class is intended for Data analysts, Data scientists and Business analysts. It is also suitable for IT decision makers evaluating Google Cloud Platform for use by data scientists.\n\nThis class is for people who do the following with big data:\n\n• Extracting, Loading, Transforming, cleaning, and validating data for use in analytics\n• Designing pipelines and architectures for data processing\n• Creating and maintaining machine learning and statistical models\n• Querying datasets, visualizing query results and creating reports", "rating": "4.6", "week_data": [{"description": "", "video": ["Introduction to the Data and Machine Learning Specialization", "Please read me", "Intro to Big Data and Machine Learning Fundamentals", "Course Overview and Agenda", "Meet Your Instructor"], "title": "Introduction to the Data and Machine Learning on Google Cloud Platform Specialization"}, {"description": "In this module you will be introduced to Google Cloud Platform and the data handling aspects of the platform.", "video": ["Welcome to Introduction to GCP and its Big Data Products", "What is the Google Cloud Platform?", "GCP Big Data Products", "Usage Scenarios", "How to do the Labs", "Sign Up for the Free Trial and Create a Project", "Module 1 Resources", "Module 1 Review"], "title": "Module 1: Introduction to Google Cloud Platform and its Big Data Products"}, {"description": "In this module, we introduce the foundations of the Google Cloud Platform: compute and storage and introduce how they work to provide data ingest, storage, and federated analysis.", "video": ["Welcome to Foundations of GCP Compute and Storage", "CPUs On Demand", "Lab 2a Overview", "Start Compute Engine Instance (Lab 2a)", "Lab 2a Review", "A Global Filesystem", "Lab 2b Overview", "Interact with Cloud Storage", "Lab 2b Review", "Module 2 Review", "Module 2 Resources", "Module 2 Resources", "Module 2 Review"], "title": "Module 2: Foundations of GCP Compute and Storage"}, {"description": "In this module we introduce the common Big Data use cases that Google will manage for you. These are the things that are widely done in industry today and for which we provide easy migration to the cloud.", "video": ["Intro to Managed Services for Common Use Cases", "Stepping Stones to Transformation", "Your SQL Database in the Cloud", "Lab 3a Overview", "Setup rentals data in Cloud SQL", "Lab 3a Review", "Managed Hadoop in the Cloud", "Lab 3b Overview", "Recommendations ML with Dataproc", "Lab 3b Review", "Module 3 Review", "Module 3 Resources", "Module 3 Review"], "title": "Module 3: Data Analysis on the Cloud"}, {"description": "This module is about the more transformational technologies in Google Cloud platform that may not have immediate parallels to technologies that attendees are using (“what's next”).", "video": ["Intro to Scaling Data Analysis: Change How You Compute with GCP", "Fast Random Access", "Interactive, Iterative Development & Demo", "Warehouse and Interactively Query Petabytes", "Lab 4a Overview", "Create ML Dataset with BigQuery (Lab 4a)", "Lab 4a Review", "Machine Learning with TensorFlow", "Training and Creating a Neural Network Model", "Lab 4b Overview", "Carry out ML with TensorFlow", "Fully Build Machine Learning Models", "Lab 4c Overview", "Machine Learning APIs (Lab 4c)", "Module 4 Review", "Module 4 Resources", "Module 4 Resources", "Module 4 Review"], "title": "Module 4: Scaling Data Analysis: Compute with GCP"}, {"description": "In this module we will introduce you to data processing architectures in Google Cloud Platform: Asynchronous processing with TaskQueues. Message-oriented architectures with Pub/Sub. Creating pipelines with Dataflow.", "video": ["Intro to Data Processing Architectures", "Message-oriented Architectures", "Serverless Data Pipelines", "Module 5 Review", "Module 5 Resources", "Module 5 Review"], "title": "Module 5: Data Processing Architectures: Scalable Ingest, Transform and Load"}, {"description": "", "video": ["Summary of GCP, Big Data, and ML", "Next Steps", "Module 6 Resources", "Module 6 Resources"], "title": "Module 6: Summary of Google Cloud Platform, Big Data, and ML"}], "title": "Google Cloud Platform Big Data and Machine Learning Fundamentals"}, {"course_info": "About this course: People analytics is a data-driven approach to managing people at work. For the first time in history, business leaders can make decisions about their people based on deep analysis of data rather than the traditional methods of personal relationships, decision making based on experience, and risk avoidance. In this brand new course, three of Wharton’s top professors, all pioneers in the field of people analytics, will explore the state-of-the-art techniques used to recruit and retain great people, and demonstrate how these techniques are used at cutting-edge companies. They’ll explain how data and sophisticated analysis is brought to bear on people-related issues, such as recruiting, performance evaluation, leadership, hiring and promotion, job design, compensation, and collaboration. This course is an introduction to the theory of people analytics, and is not intended to prepare learners to perform complex talent management data analysis. By the end of this course, you’ll understand how and when hard data is used to make soft-skill decisions about hiring and talent development, so that you can position yourself as a strategic partner in your company’s talent management decisions. This course is intended to introduced you to Organizations flourish when the people who work in them flourish. Analytics can help make both happen. This course in People Analytics is designed to help you flourish in your career, too.", "level": null, "package_name": "Business Analytics Specialization ", "created_by": "University of Pennsylvania", "package_num": "3", "teach_by": [{"name": "Cade Massey", "department": "The Wharton School"}, {"name": "Martine Haas", "department": "The Wharton School"}, {"name": "Matthew Bidwell", "department": "The Wharton School"}], "target_audience": null, "rating": null, "week_data": [{"description": "In this module, you'll meet Professors Massey, Bidwell, and Haas, cover the structore and scope of the course, and dive into the first topic: Performance Evaluation. Performance evaluation plays an influential role in our work lives, whether it is used to reward or punish and/or to gather feedback. Yet its fundamental challenge is that the measures we used to evaluate performance are imperfect: we can't infer how hard or smart an employee is working based solely on outcomes. In this module, you’ll learn the four key issues in measuring performance: regression to the mean, sample size, signal independence, and process vs. outcome, and see them at work in current companies, including an extended example from the NFL. By the end of this module, you’ll understand how to separate skill from luck and learn to read noisy performance measures, so that you can go into your next performance evaluation sensitive to the role of chance, knowing your environment, and aware of the four most common biases, so that you can make more informed data-driven decisions about your company's most valuable asset: its employees.", "video": ["Introduction to People Analytics", "Goals for the Course", "Course Outline and Overview", "People Analytics in Practice", "Performance Evaluation: the Challenge of Noisy Data", "Chance vs. Skill: the NFL Draft", "Finding Persistence: Regression to the Mean", "Extrapolating from Small Samples", "The Wisdom of Crowds: Signal Independence", "Process vs. Outcome", "Summary of Performance Evaluation", "Performance Analytics Slides PDF", "People Analytics in Action: Additional Reading", "Performance Evaluation Quiz"], "title": "Introduction to People Analytics, and Performance Evaluation"}, {"description": "In this module, you'll learn how to use data to better analyze the key components of the staffing cycle: hiring, internal mobility and career development, and attrition. You'll explore different analytic approaches to predicting performance for hiring and for optimizing internal mobility, to understanding and reducing turnover, and to predicting attrition. You'll also learn the critical skill of understanding causality so that you can avoid using data incorrectly. By the end of this module, you'll be able to use data to improve the quality of the decisions you make in getting the right people into the right jobs and helping them stay there, to benefit not only your organization but also employee's individual careers. ", "video": ["Introduction to Professor Bidwell", "Staffing Analytics Overview", "Hiring 1: Predicting Performance", "Hiring 2: Fine-tuning Predictors", "Hiring 3: Using Data Analysis to Predict Performance", "Internal Mobility 1: Analyzing Promotibility", "Internal Mobility 2: Optimizing Movement within the Organization", "Causality 1", "Causality 2", "Attrition: Understanding and Reducing Turnover", "Turnover: Predicting Attrition", "Staffing Analytics Conclusion", "Staffing Analytics Slides PDF", "Staffing Analytics in Action: Additional Reading", "Staffing Quiz"], "title": "Staffing"}, {"description": "In this module, you'll learn the basic principles behind using people analytics to improve collaboration between employees inside an organization so they can work together more successfully. You'll explore how data is used to describe, map, and evaluate collaboration networks, as well as how to intervene in collaboration networks to improve collaboration using examples from real-world companies. By the end of this module, you'll know how to deploy the tools and techniques of organizational network analysis to understand and improve collaboration patterns inside your organization to make your organization, and the people working within in it, more productive, effective, and successful. ", "video": ["Introduction to Professor Haas", "Basics of Collaboration", "Describing Collaboration Networks", "Mapping Collaboration Networks", "Evaluating Collaboration Networks", "Measuring Outcomes", "Intervening in Collaboration Networks", "Collaboration Slides PDF", "Collaboration Research in Action: Additional Readings", "Collaboration Quiz"], "title": "Collaboration"}, {"description": "In this module, you explore talent analytics: how data may be used in talent assessment and development to maximize employee ability. You'll learn how to use data to move from performance evaluation to a more deeper analysis of employee evaluation so that you may be able to improve the both the effectiveness and the equitability of the promotion process at your firm. By the end of this module, you'll will understand the four major challenges of talent analytics: context, interdependence, self-fulfilling prophecies, and reverse causality, the challenges of working with algorithms, and some practical tips for incorporating data sensitively, fairly, and effectively into your own talent assessment and development processes to make your employees and your organization more successful. In the course conclusion, you'll also learn the current challenges and future directions of the field of people analytics, so that you may begin putting employee data to work in a ways that are smarter, practical and more powerful.", "video": ["Talent Analytics: The Importance of Context", "Interdependence", "Self-fulfilling Prophecies", "Reverse Causality", "Special Topics: Tests and Algorithms", "Prescriptions: Navigating the Challenges of Talent Analytics", "Course Conclusion: Organizational Challenges 1", "Course Conclusion: Organizational Challenges 2 and Future Directions", "Goodbye and Good Luck!", "Talent Analytics and Conclusion Slides PDF", "Talent Management in Action: Additional Readings", "Talent Management Quiz"], "title": "Talent Management and Future Directions"}], "title": "People Analytics"}, {"course_info": "About this course: This course focuses on the concepts and tools behind reporting modern data analyses in a reproducible manner. Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them.  The need for reproducibility is increasing dramatically as data analyses become more complex, involving larger datasets and more sophisticated computations. Reproducibility allows for people to focus on the actual content of a data analysis, rather than on superficial details reported in a written summary. In addition, reproducibility makes an analysis more useful to others because the data and code that actually conducted the analysis are available. This course will focus on literate statistical analysis tools which allow one to publish data analyses in a single document that allows others to easily execute the same analysis to obtain the same results.", "level": null, "package_name": "Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "5", "teach_by": [{"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "This week will cover the basic ideas of reproducible research since they may be unfamiliar to some of you. We also cover structuring and organizing a data analysis to help make it more reproducible. I recommend that you watch the videos in the order that they are listed on the web page, but watching the videos out of order isn't going to ruin the story. ", "video": ["Introduction", "Syllabus", "Pre-course survey", "Course Book: Report Writing for Data Science in R", "What is Reproducible Research About?", "Reproducible Research: Concepts and Ideas (part 1)", "Reproducible Research: Concepts and Ideas (part 2) ", "Reproducible Research: Concepts and Ideas (part 3) ", "Scripting Your Analysis ", "Structure of a Data Analysis (part 1)", "Structure of a Data Analysis (part 2)", "Organizing Your Analysis", "Week 1 Quiz"], "title": "Week 1: Concepts, Ideas, & Structure"}, {"description": "This week we cover some of the core tools for developing reproducible documents. We cover the literate programming tool knitr and show how to integrate it with Markdown to publish reproducible web documents. We also introduce the first peer assessment which will require you to write up a reproducible data analysis using knitr. ", "video": ["Coding Standards in R", "Markdown", "R Markdown", "R Markdown Demonstration", "knitr (part 1)", "knitr (part 2) ", "knitr (part 3) ", "knitr (part 4) ", "Introduction to Course Project 1", "Week 2 Quiz", "Course Project 1"], "title": "Week 2: Markdown & knitr"}, {"description": "This week covers what one could call a basic check list for ensuring that a data analysis is reproducible. While it's not absolutely sufficient to follow the check list, it provides a necessary minimum standard that would be applicable to almost any area of analysis.", "video": ["Communicating Results", "RPubs ", "Reproducible Research Checklist (part 1)", "Reproducible Research Checklist (part 2) ", "Reproducible Research Checklist (part 3) ", "Evidence-based Data Analysis (part 1)", "Evidence-based Data Analysis (part 2) ", "Evidence-based Data Analysis (part 3) ", "Evidence-based Data Analysis (part 4) ", "Evidence-based Data Analysis (part 5) "], "title": "Week 3: Reproducible Research Checklist & Evidence-based Data Analysis"}, {"description": "This week there are two \ncase studies involving the importance of reproducibility in science for you to watch.", "video": ["Caching Computations", "Case Study: Air Pollution", "Case Study: High Throughput Biology", "Commentaries on Data Analysis", "Introduction to Peer Assessment 2", "Post-Course Survey", "Course Project 2"], "title": "Week 4: Case Studies & Commentaries"}], "title": "Reproducible Research"}, {"course_info": "About this course: This course is designed to impact the way you think about transforming data into better decisions. Recent extraordinary improvements in data-collecting technologies have changed the way firms make informed and effective business decisions. The course on operations analytics, taught by three of Wharton’s leading experts, focuses on how the data can be used to profitably match supply with demand in various business settings. In this course, you will learn how to model future demand uncertainties, how to predict the outcomes of competing policy choices and how to choose the best course of action in the face of risk. The course will introduce frameworks and ideas that provide insights into a spectrum of real-world business challenges, will teach you methods and software available for tackling these challenges quantitatively as well as the issues involved in gathering the relevant data.\n\nThis course is appropriate for beginners and business professionals with no prior analytics experience.", "level": null, "package_name": "Business Analytics Specialization ", "created_by": "University of Pennsylvania", "package_num": "2", "teach_by": [{"name": "Senthil Veeraraghavan", "department": "The Wharton School"}, {"name": "Sergei Savin", "department": "The Wharton School"}, {"name": "Noah Gans", "department": "The Wharton School"}], "target_audience": null, "rating": null, "week_data": [{"description": "In this module you’ll be introduced to the Newsvendor problem, a fundamental operations problem of matching supply with demand in uncertain settings. You'll also cover the foundations of descriptive analytics for operations, learning how to use historical demand data to build forecasts for future demand.  Over the week, you’ll be introduced to underlying analytic concepts, such as random variables, descriptive statistics, common forecasting tools, and measures for judging the quality of  your forecasts.", "video": ["Course Introduction and Welcome", "The Newsvendor Problem", "Moving Averages", "Trends, Seasonality", "Week 1 Wrap-up, Apparel Industry", "Excel Files, Slides and Practice Problems", "Newsvendor and Forecasting Quiz"], "title": "Introduction, Descriptive and Predictive Analytics"}, {"description": "In this module, you'll learn how to identify the best decisions in settings with low uncertainty by building optimization models and applying them to specific business challenges. During the week, you’ll use algebraic formulations to concisely express optimization problems, look at how algebraic models should be converted into a spreadsheet format, and learn how to use spreadsheet Solvers as tools for identifying the best course of action. ", "video": ["How to Build an Optimization Model", "Optimizing with Solver", "Network Optimization Example", "(Optional) Week 2 Review", "(Optional) Solver on Mac", "(Optional) Solver in Google Sheets", "Excel Files, Slides, and Practice Problems", "Decisions with Low Uncertainty Quiz"], "title": "Prescriptive Analytics, Low Uncertainty"}, {"description": "How can you evaluate and compare decisions when their impact is uncertain? In this module you will learn how to build and interpret simulation models that can help you to evaluate complex business decisions in uncertain settings. During the week, you will be introduced to some common measures of risk and reward, you’ll use simulation to estimate these quantities, and you’ll learn how to interpret and visualize your simulation results.", "video": ["Comparing Decisions in Uncertain Settings", "Simulating Uncertain Outcomes in Excel", "Interpreting and Visualizing Simulation Output", "(Optional) Week 3 Review", "Excel file, Slides, Practice Problems", "Risk and Evaluation Quiz"], "title": "Predictive Analytics, Risk"}, {"description": "This module introduces decision trees, a useful tool for evaluating decisions made under uncertainty. Using a concrete example, you'll learn how optimization, simulation, and decision trees can be used together to solve more complex business problems with high degrees of uncertainty. You'll also discover how the Newsvendor problem introduced in Week 1 can be solved with the simulation and optimization framework introduced in Weeks 2 and 3.", "video": ["Decision Trees", "Using Simulation with Decision Trees", "Using Optimization Together with Simulation", "Week 4 Wrap-up", "(Optional) Session 4 Review", "(Optional) Advanced Session on Optimization", "Excel files, Slides, and Practice Problems", "Decision Tree Analysis Quiz"], "title": "Prescriptive Analytics, High Uncertainty "}], "title": "Operations Analytics"}, {"course_info": "About this course: Spreadsheet software remains one of the most ubiquitous pieces of software used in workplaces around the world. Learning to confidently operate this software means adding a highly valuable asset to your employability portfolio. Across the globe, millions of job advertisements requiring Excel skills are posted every day. At a time when digital skills jobs are growing much faster than non-digital jobs, completing this course will position you ahead of others, so keep reading.\n\nIn this last course of our Specialization Excel Skills for Business you will build on the strong foundations of the first three courses: Essentials, Intermediate I + II.  In the Advanced course, we will prepare you to become a power user of Excel - this is your last step before specializing at a professional level. The topics we have prepared will challenge you as you learn how to use advanced formula techniques and sophisticated lookups. You will clean and prepare data for analysis, and learn how to work with dates and financial functions. An in-depth look at spreadsheet design and documentation will prepare you for our big finale, where you will learn how to build professional dashboards in Excel.", "level": "Intermediate", "package_name": "Excel Skills for Business Specialization ", "created_by": "Macquarie University", "package_num": "4", "teach_by": [{"name": "Dr Yvonne Breyer", "department": "Faculty of Business and Economics"}], "target_audience": "Who is this class for: Taking Excel Skills for Business: Advanced will complete the learning journey of this Specialization. Attaining the certificate at the end of the fourth course will be a highly valuable asset for anyone wanting to improve their employment portfolio. This course is for anyone who works with Excel on a regular basis, for self-taught learners with well-developed intermediate skills who want to fill gaps in their knowledge and for anyone wanting to consolidate their well-developed knowledge of Excel.  ", "rating": null, "week_data": [{"description": "", "video": ["Welcome", "Course Structure & Assessments", "Course goals and weekly learning objectives", "Excel: Who, Why, How?", "Week 1 Introduction", "Week 1 Discussion", "Read me before you start: Quizzes and navigation", "Read me before you start: Versions and regions", "Download the Week 1 workbooks", "Practice Video: Spreadsheet Design Principles", "Spreadsheet Design Principles", "Practice Video: Calculations", "Calculations", "Practice Video: Formatting", "Formatting", "Practice Video: Documentation", "Documentation", "Practice Video: Interface and Navigation", "Keyboard Shortcuts, Terminology, and Ninja Tips", "Week 1 Final Assignment"], "title": "Spreadsheet Design and Documentation"}, {"description": "", "video": ["Week 2 Introduction", "Week 2 Discussion", "Download the Week 2 workbooks", "Practice Video: Tables and Structured Referencing", "Tables and Structured Referencing", "Practice Video: Using Functions to Sort Data", "Using Functions to Sort Data", "Practice Video: Introduction to Array Formulas", "Introduction to Array Formulas", "Practice Video: Working with an Array Function (TRANSPOSE)", "Practice Video: Solving Problems with Array Formulas", "Week 2 Practice Challenge", "Keyboard Shortcuts, Terminology, and Ninja Tips", "Week 2 Final Assignment"], "title": "Advanced Formula Techniques"}, {"description": "", "video": ["Week 3 Introduction", "Week 3 Discussion", "Download the Week 3 workbooks", "Practice Video: Replace blanks with repeating values", "Replace blanks with repeating values", "Practice Video: Fix Dates (DATE, MONTH, YEAR, DAY, TEXT)", "Fix Dates (DATE, MONTH, YEAR, DAY, TEXT)", "Practice Video: Remove Unwanted Spaces (TRIM, CLEAN)", "Remove Unwanted Spaces (TRIM, CLEAN)", "Practice Video: Diagnostic Tools (ISNUMBER, LEN, CODE)", "Diagnostic Tools (ISNUMBER, LEN, CODE)", "Practice Video: Remove Unwanted Characters (SUBSTITUTE, CHAR, VALUE)", "Remove Unwanted Characters (SUBSTITUTE, CHAR, VALUE)", "Week 3 Practice Challenge", "Keyboard Shortcuts, Terminology, and Ninja Tips", "Week 3 Final Assignment"], "title": "Data Cleaning and Preparation"}, {"description": "", "video": ["Week 4 Introduction", "Week 4 Discussion", "Download the Week 4 workbooks", "Practice Video: Working with Dates (EOMONTH, EDATE, WORKDAY.INTL)", "Working with Dates (EOMONTH, EDATE, WORKDAY.INTL)", "Practice Video: Financial Functions (FV, PV, PMT)", "Financial Functions (FV, PV, PMT)", "Practice Video: Loan Schedule (PMT, EDATE)", "Loan Schedule (PMT, EDATE)", "Practice Video: Net Present Value and Internal Rate of Return (NPV, IRR)", "Net Present Value and Internal Rate of Return (NPV, IRR)", "Practice Video: Depreciation Functions (SLN, SYD, DDB)", "Depreciation Functions (SLN, SYD, DDB)", "Keyboard Shortcuts, Excel Terminology, and Ninja Tips", "Week 4 Final Assignment"], "title": "Financial Functions and Working with Dates"}, {"description": "", "video": ["Week 5 Introduction", "Week 5 Discussion", "Download the Week 5 workbooks", "Practice Video: INDIRECT", "INDIRECT", "Practice Video: ADDRESS", "ADDRESS", "Practice Video: Introduction to OFFSET", "Introduction to OFFSET", "Practice Video: Solving Problems with OFFSET", "Keyboard Shortcuts, Excel Terminology, and Ninja Tips", "Week 5 Final Assignment"], "title": "Advanced Lookup Functions"}, {"description": "", "video": ["Week 6 Introduction", "Week 6 Discussion", "Download the Week 6 workbooks", "Practice Video: Dashboard Design", "Dashboard Design", "Practice Video: Prepare Data", "Practice Video: Construct Dashboard", "Practice Video: Creative Charting", "Practice Video: Interactive Dashboard", "Keyboard Shortcuts, Excel Terminology, and Ninja Tips"], "title": "Building Professional Dashboards"}, {"description": "", "video": ["Final Assessment"], "title": "Final Assessment"}], "title": "Excel Skills for Business: Advanced"}, {"course_info": "About this course: A data product is the production output from a statistical analysis. Data products automate complex analysis tasks or use technology to expand the utility of a data informed model, algorithm or inference. This course covers the basics of creating data products using Shiny, R packages, and interactive graphics. The course will focus on the statistical fundamentals of creating a data product that can be used to tell a story about data to a mass audience.", "level": null, "package_name": "Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "9", "teach_by": [{"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "In this overview module, we'll go over some information and resources to help you get started and succeed in the course. ", "video": ["Welcome to Developing Data Products", "Syllabus", "Welcome", "Book: Developing Data Products in R", "Community Site", "R and RStudio Links & Tutorials"], "title": "Course Overview"}, {"description": "Now we can turn to the first substantive lessons. In this module, you'll learn how to develop basic applications and interactive graphics in shiny, compose interactive HTML graphics with GoogleVis, and prepare data visualizations with Plotly.", "video": ["Shiny", "Shinyapps.io Project", "Shiny 1.1", "Shiny 1.2", "Shiny 1.3", "Shiny 1.4", "Shiny 1.5", "Shiny 2.1", "Shiny 2.2", "Shiny 2.3", "Shiny 2.4", "Shiny 2.5", "Shiny 2.6", "Shiny Gadgets 1.1", "Shiny Gadgets 1.2", "Shiny Gadgets 1.3", "GoogleVis 1.1", "GoogleVis 1.2", "Plotly 1.1", "Plotly 1.2", "Plotly 1.3", "Plotly 1.4", "Plotly 1.5", "Plotly 1.6", "Plotly 1.7", "Plotly 1.8", "Quiz 1"], "title": "Shiny, GoogleVis, and Plotly"}, {"description": "During this module, we'll learn how to create R Markdown files and embed R code in an Rmd. We'll also explore Leaflet and use it to create interactive annotated maps.", "video": ["R Markdown 1.1", "R Markdown 1.2", "R Markdown 1.3", "R Markdown 1.4", "R Markdown 1.5", "R Markdown 1.6", "Three Ways to Share R Markdown Products", "Leaflet 1.1", "Leaflet 1.2", "Leaflet 1.3", "Leaflet 1.4", "Leaflet 1.5", "Leaflet 1.6", "Quiz 2", "R Markdown and Leaflet"], "title": "R Markdown and Leaflet"}, {"description": "In this module, we'll dive into the world of creating R packages and practice developing an R Markdown presentation that includes a data visualization built using Plotly.", "video": ["R Packages", "R Packages (Part 1)", "R Packages (Part 2)", "Building R Packages Demo", "R Classes and Methods (Part 1)", "R Classes and Methods (Part 2)", "Quiz 3", "R Markdown Presentation & Plotly"], "title": "R Packages"}, {"description": "Week 4 is all about the Course Project, producing a Shiny Application and reproducible pitch.", "video": ["Swirl 1.1", "Swirl 1.2", "Swirl 1.3", "Post-Course Survey", "Course Project: Shiny Application and Reproducible Pitch"], "title": "Swirl and Course Project "}], "title": "Developing Data Products"}, {"course_info": "About this course: At the end of the course, you will be able to:\n\n*Retrieve data from example database and big data management systems \n*Describe the connections between data management operations and the big data processing patterns needed to utilize them in large-scale analytical applications\n*Identify when a big data problem needs data integration\n*Execute simple big data integration and processing on Hadoop and Spark platforms\n\nThis course is for those new to data science.  Completion of Intro to Big Data is recommended.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  Refer to the specialization technical requirements for complete hardware and software specifications.\n\nHardware Requirements: \n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. \n\nSoftware Requirements: \nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+. ", "level": "Beginner", "package_name": "Big Data Specialization ", "created_by": "University of California, San Diego", "package_num": "3", "teach_by": [{"name": "Ilkay Altintas", "department": "San Diego Supercomputer Center"}, {"name": "Amarnath Gupta", "department": "San Diego Supercomputer Center (SDSC)"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "Welcome to the third course in the Big Data Specialization. This week you will be introduced to basic concepts in big data integration and processing. You will be guided through installing the Cloudera VM, downloading the data sets to be used for this course, and learning how to run the Jupyter server. ", "video": ["What is in this Course?", "Summary of Big Data Modeling and Management", "Why is Big Data Processing Different?", "Getting to know you: Tell us about yourself and why you are taking this course.", "Slides: Summary & Why Is Big Data Processing Different", "Downloading and Installing the Cloudera VM Instructions (Windows)", "Downloading and Installing the Cloudera VM Instructions (Mac)", "Software Installation Frequently Asked Questions (FAQ)", "Instructions for Downloading Hands On Datasets", "Instructions for Starting Jupyter"], "title": "Welcome to Big Data Integration and Processing"}, {"description": "This module covers the various aspects of data retrieval and relational querying. You will also be introduced to the Postgres database. ", "video": ["What is Data Retrieval? Part 1", "What is Data Retrieval? Part 2", "Querying Two Relations", "Subqueries", "Slides: What is Data Retrieval?", "Querying Relational Data with Postgres", "Querying Relational Data with Postgres"], "title": "Retrieving Big Data (Part 1)"}, {"description": "This module covers the various aspects of data retrieval for NoSQL data, as well as data aggregation and working with data frames. You will be introduced to MongoDB and Aerospike, and you will learn how to use Pandas to retrieve data from them.", "video": ["Querying JSON Data with MongoDB", "Aggregation Functions", "Let's Discuss: MongoDB", "Querying Aerospike", "Slides: Querying Data Part 2", "Querying Documents in MongoDB", "Querying Documents in MongoDB", "Exploring Pandas DataFrames", "Exploring Pandas DataFrames", "Retrieving Big Data Quiz", "Postgres, MongoDB, and Pandas"], "title": "Retrieving Big Data (Part 2)"}, {"description": "In this module you will be introduced to data integration tools including Splunk and Datameer, and you will gain some practical insight into how information integration processes are carried out. ", "video": ["Overview of Information Integration", "A Data Integration Scenario", "Integration for Multichannel Customer Analytics", "Let's Discuss: Big Data Integration", "Slides: Information Integration", "Big Data Management and Processing Using Splunk and Datameer", "Why Splunk?", "Connected Cars with Ford's OpenXC and Splunk", "Big Data Management and Processing using Datameer", "Downloading Splunk Enterprise", "Installing Splunk Enterprise on Windows", "Installing Splunk Enterprise on Linux", "Exploring Splunk Queries", "Exploring Splunk Queries", "Optional: Instructions for Splunk Pivot Tutorial", "Optional: Creating Pivot Reports in Splunk", "Information Integration - Quiz", "Hands-On With Splunk"], "title": "Big Data Integration"}, {"description": "This module introduces Learners to big data pipelines and workflows as well as processing and analysis of big data using Apache Spark. ", "video": ["Big Data Processing Pipelines", "Some High-Level Processing Operations in Big Data Pipelines", "Aggregation Operations in Big Data Pipelines", "Typical Analytical Operations in Big Data Pipelines", "Let's Discuss: Big Data Pipelines in Your World", "Big Data Processing Pipelines Slides", "Overview of Big Data Processing Systems", "Big Data Workflow Management", "The Integration and Processing Layer", "Introduction to Apache Spark", "Getting Started with Spark", "Let's Discuss: Big Data Processing Systems", "Slides for Big Data Processing Tools and Systems", "WordCount in Spark", "WordCount in Spark", "Let's Discuss: Word Count", "Pipeline and Tools", "WordCount in Spark"], "title": "Processing Big Data"}, {"description": "In this module, you will go deeper into big data processing by learning the inner workings of the Spark Core. You will be introduced to two key tools in the Spark toolkit: Spark MLlib and GraphX. ", "video": ["Spark Core: Programming In Spark using RDDs in Pipelines", "Spark Core: Transformations", "Spark Core: Actions", "Slides for Module 5 Lesson 1", "Spark SQL", "Spark Streaming", "Spark MLLib", "Spark GraphX", "Let's Discuss: The Spark Ecosystem", "Slides for Module 5 Lesson 2", "Exploring SparkSQL and Spark DataFrames", "Exploring SparkSQL and Spark DataFrames", "Instructions for Configuring VirtualBox for Spark Streaming", "Analyzing Sensor Data with Spark Streaming", "Analyzing Sensor Data with Spark Streaming", "More on Spark", "SparkSQL and Spark Streaming"], "title": "Big Data Analytics using Spark"}, {"description": "In this module you will get some practical hands-on experience applying what you learned about Spark and MongoDB to analyze Twitter data. ", "video": ["Let's Analyze Soccer Tweets!", "Expressing Analytical Questions as MongoDB Queries", "Exporting Data from MongoDB to a CSV File", "Analyzing Tweets About Countries", "Check Your Query Results", "Check Your Analysis Results"], "title": "Learn By Doing: Putting MongoDB and Spark to Work"}], "title": "Big Data Integration and Processing"}, {"course_info": "About this course: This course will cover the major techniques for mining and analyzing text data to discover interesting patterns, extract useful knowledge, and support decision making, with an emphasis on statistical approaches that can be generally applied to arbitrary text data in any natural language with no or minimum human effort. \n\nDetailed analysis of text data requires understanding of natural language text, which is known to be a difficult task for computers. However, a number of statistical approaches have been shown to work well for the \"shallow\" but robust analysis of text data for pattern finding and knowledge discovery. You will learn the basic concepts, principles, and major algorithms in text mining and their potential applications.", "level": null, "package_name": "Data Mining  Specialization ", "created_by": "University of Illinois at Urbana-Champaign", "package_num": "3", "teach_by": [{"name": "ChengXiang Zhai", "department": "Department of Computer Science"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course.", "video": ["Welcome to Text Mining and Analytics!", "Syllabus", "About the Discussion Forums", "Updating your Profile", "Social Media", "Introduction to Text Mining and Analytics", "Course Prerequisites & Completion", "Pre-Quiz", "Orientation Quiz"], "title": "Orientation"}, {"description": "During this module, you will learn the overall course design, an overview of natural language processing techniques and text representation, which are the foundation for all kinds of text-mining applications, and word association mining with a particular focus on mining one of the two basic forms of word associations (i.e., paradigmatic relations).   ", "video": ["Week 1 Overview", "1.1 Overview Text Mining and Analytics: Part 1", "1.2 Overview Text Mining and Analytics: Part 2", "1.3 Natural Language Content Analysis: Part 1", "1.4 Natural Language Content Analysis: Part 2", "1.5 Text Representation: Part 1", "1.6 Text Representation: Part 2", "1.7 Word Association Mining and Analysis", "1.8 Paradigmatic Relation Discovery Part 1", "1.9 Paradigmatic Relation Discovery Part 2", "Week 1 Practice Quiz", "Week 1 Quiz"], "title": "Week 1"}, {"description": "During this module, you will learn more about word association mining with a particular focus on mining the other basic form of word association (i.e., syntagmatic relations), and start learning topic analysis with a focus on techniques for mining one topic from text. ", "video": ["Week 2 Overview", "2.1 Syntagmatic Relation Discovery: Entropy", "2.2 Syntagmatic Relation Discovery: Conditional Entropy", "2.3 Syntagmatic Relation Discovery: Mutual Information: Part 1", "2.4 Syntagmatic Relation Discovery: Mutual Information: Part 2", "2.5 Topic Mining and Analysis: Motivation and Task Definition", "2.6 Topic Mining and Analysis: Term as Topic", "2.7 Topic Mining and Analysis: Probabilistic Topic Models", "2.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1", "2.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2", "2.10 Probabilistic Topic Models: Mining One Topic", "Week 2 Practice Quiz", "Week 2 Quiz"], "title": "Week 2"}, {"description": "During this module, you will learn topic analysis in depth, including mixture models and how they work, Expectation-Maximization (EM) algorithm and how it can be used to estimate parameters of a mixture model, the basic topic model, Probabilistic Latent Semantic Analysis (PLSA), and how Latent Dirichlet Allocation (LDA) extends PLSA. ", "video": ["Week 3 Overview", "3.1 Probabilistic Topic Models: Mixture of Unigram Language Models", "3.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1", "3.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2", "3.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1", "3.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2", "3.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3", "3.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1", "3.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2", "3.9 Latent Dirichlet Allocation (LDA): Part 1", "3.10 Latent Dirichlet Allocation (LDA): Part 2", "Week 3 Practice Quiz", "Programming Assignments Overview", "Quiz: Week 3 Quiz", "Programming Assignment"], "title": "Week 3"}, {"description": "During this module, you will learn text clustering, including the basic concepts, main clustering techniques, including probabilistic approaches and similarity-based approaches, and how to evaluate text clustering. You will also start learning text categorization, which is related to text clustering, but with pre-defined categories that can be viewed as pre-defining clusters.   ", "video": ["Week 4 Overview", "4.1 Text Clustering: Motivation", "4.2 Text Clustering: Generative Probabilistic Models Part 1", "4.3 Text Clustering: Generative Probabilistic Models Part 2", "4.4 Text Clustering: Generative Probabilistic Models Part 3", "4.5 Text Clustering: Similarity-based Approaches", "4.6 Text Clustering: Evaluation", "4.7 Text Categorization: Motivation", "4.8 Text Categorization: Methods", "4.9 Text Categorization: Generative Probabilistic Models", "Week 4 Practice Quiz", "Week 4 Quiz"], "title": "Week 4"}, {"description": "During this module, you will continue learning about various methods for text categorization, including multiple methods classified under discriminative classifiers, and you will also learn sentiment analysis and opinion mining, including a detailed introduction to a particular technique for sentiment classification (i.e., ordinal regression). ", "video": ["Week 5 Overview", "5.1 Text Categorization: Discriminative Classifier Part 1", "5.2 Text Categorization: Discriminative Classifier Part 2", "5.3 Text Categorization: Evaluation Part 1", "5.4 Text Categorization: Evaluation Part 2", "5.5 Opinion Mining and Sentiment Analysis: Motivation", "5.6 Opinion Mining and Sentiment Analysis: Sentiment Classification", "5.7 Opinion Mining and Sentiment Analysis: Ordinal Logistic Regression", "Week 5 Practice Quiz", "Week 5 Quiz"], "title": "Week 5"}, {"description": "During this module, you will continue learning about sentiment analysis and opinion mining with a focus on Latent Aspect Rating Analysis (LARA), and you will learn about techniques for joint mining of text and non-text data, including contextual text mining techniques for analyzing topics in text in association with various context information such as time, location, authors, and sources of data. You will also see a summary of the entire course.", "video": ["Week 6 Overview", "6.1 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 1", "6.2 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 2", "6.3 Text-Based Prediction", "6.4 Contextual Text Mining: Motivation", "6.5 Contextual Text Mining: Contextual Probabilistic Latent Semantic Analysis", "6.6 Contextual Text Mining: Mining Topics with Social Network Context", "6.7 Contextual Text Mining: Mining Casual Topics with Time Series Supervision", "6.8 Course Summary", "Week 6 Practice Quiz", "Week 6 Quiz"], "title": "Week 6"}], "title": "Text Mining and Analytics"}, {"course_info": "About this course: This course (The English copy of \"用Python玩转数据\" <https://www.coursera.org/learn/hipython/home/welcome>)  is mainly for non-computer majors. It starts with the basic syntax of Python, to how to acquire data in Python locally and from network, to how to present data, then to how to conduct basic and advanced statistic analysis and visualization of data, and finally to how to design a simple GUI to present and process data, advancing level by level. \nThis course, as a whole, based on Finance data and through establishment of popular cases one after another, enables learners to more vividly feel the simplicity, elegance and robustness of Python. Also, it discusses the fast, convenient and efficient data processing capacity of Python in humanities and social sciences fields like literature, sociology and journalism and science and engineering fields like mathematics and biology, in addition to business fields. Similarly, it may also be flexibly applied into other fields.\n\nThe course has been updated. Updates in the new version are : \n\n1) the whole course has moved from Python 2.x to Python 3.x \n2) Added manual webpage fetching and parsing. Web API is also added. \n3) Improve the content order and enrich details of some content especially for some practice projects.", "level": "Beginner", "package_name": null, "created_by": "Nanjing University", "package_num": null, "teach_by": [{"name": "ZHANG Li", "department": "Department of Computer Science"}], "target_audience": null, "rating": "4.3", "week_data": [{"description": "Hi, guys, welcome to learn “Data Processing Using Python”(The English version of \"用Python玩转数据\", url is https://www.coursera.org/learn/hipython/home/welcome)!In this course, I tell in a manner that enables non-computer majors to understand how to utilize this simple and easy programming language – Python to rapidly acquire, express, analyze and present data based on SciPy, Requests, Beautiful Soup libraries etc. Many cases are provided to enable you to easily and happily learn how to use Python to process data in many fields.  ", "video": ["Promotion Video", "Teaching Methods", "FAQ"], "title": "Welcome to learn Data Processing Using Python!"}, {"description": "Hi, guys, welcome to learn Module 01 “Basics of Python”! I’ll first guide you to have a glimpse of its simplicity for learning as well as elegance and robustness. Less is more: the author of Python must know this idea well. After learning this module, you can master the basic language structures, data types, basic operations, conditions, loops, functions and modules in Python. With them, we can write some useful programs! ", "video": ["1 Introduction to Python", "2 The First Python Program", "3 Basics of Python Syntax", "4 Data Types of Python", "5 Basic Operations of Python", "6 Functions, Modules and Packages of Python", "1.1 References", "1.1 Programming exercises(Not Graded)", "1 Conditions", "2 range", "3 Loops", "4 break, continue and else in Loops", "5 Self-defined Functions", "6 Recursion", "7 Scope of Variable", "1.2 Coding and programs reading(Not Graded)", "A1: Standard Library Functions", "A2: Exceptions", "the characteristic of recursive algorithm", "Walk into Python quiz", "More About Python quiz", "find out the 6-th Monisen number(3 points)"], "title": "Basics of Python"}, {"description": "Welcome to learn Module 02 “Data Acquisition and Presentation”! After learning this module, you can master the modes of acquiring local data and network data in Python and use the basic and yet very powerful data structure sequence, string, list and tuple in Python to fast and effectively present data and simply process data. ", "video": ["1 Local Data Acquisition", "2 Network Data Retrieval", "2.1 References(re)", "2.1 Internet Data Retrival Programming exercise(Not Graded)", "2.1 code snippets for reference only", "1 Sequence", "2 String", "3 List", "4 Tuple", "Sequence fuctions practice", "KO Math Whiz", "Sequences and Files Programming Exercise(No Graded)", "Data Acquisition and Presentation quiz"], "title": "Data Acquisition and Presentation"}, {"description": "Welcome to learn Module 03 “Powerful Data Structures and Python Extension Libraries”! Have you felt you are closer to using Python to process data? After learning this module, you can master the intermediate-level and advanced uses of Python: data structure dictionaries and sets. In some applications, they can be very convenient. What’s special here is that, you can also feel the charm of such concise and efficient data structures: ndarray, Series and DataFrame in the most famous and widely applied scientific computing package SciPy in Python. ", "video": ["1 Why Are Dictionaries Needed", "2  Dictionary Use", "3  Set", "3.1 Programming exercise(Not Graded)", "1 Extension Library SciPy", "2 ndarray", "3 Series", "4 DataFrame", "3.2 References", "ufunc functions", "3.2 Programming exercise for DataFrame(Not Graded)", "3.2 Modify the DataFrames", "Word Frequency Counter", "Powerful Data Structures and Python Extension Libraries quiz"], "title": "Powerful Data Structures and Python Extension Libraries"}, {"description": "Welcome to learn Module 04 “Python Data Statistics and Visualization”! In this module, I will show you, over the entire process of data processing, the unique advantages of Python in data processing and analysis, and use many cases familiar to and loved by us to learn about and master methods and characteristics. After learning this module, you can fast and effectively mine your desired or expected or unknown results from a large amount of data, and can also present those data in various images. In addition, the data statistics modes of all third party packages in Python are extraordinarily and surprisingly strong, but we, as average persons, can still understand and possess them. ", "video": ["1 Convenient and Fast Data Acquisition", "2 Data Preparations", "3 Data Display", "4 Data Selection", "5 Simple Statistics and Processing", "6 Grouping", "7 Merge", "4.1 References", "4.1.1 code snippets for reference only", "4.1.2 code snippets for reference only", "Chinese Web API - TuShare", "1 Cluster", "2 Basics of Matplotlib Plotting", "3 Control of Matplotlib Image Attributes", "4 Plotting with pandas", "5 Data Access", "6 Applications of Python into Science and Engineering Fields", "7 Applications into Humanities and Social Sciences Fields", "4.2 Programming exercise for comparing the stock data(No Graded)", "4.2 code snippets for reference only", "4.2.1 Extension: Scikit-learn Machine Learning Basics", "4.2.4&4.2.5: Analyze test results using Box-plot", "4.2.6 Extension: Introduction to WAV audio processing", "4.2.7 Learn More about NLTK", "4.2.1K-means algorithm", "Basic Data Statistics of Python quiz", "Advanced Data Processing and Visualization of Python quiz", "Movies review programming exerciese(4 points)"], "title": "Python Data Statistics and Visualization"}, {"description": "Welcome to Module 05 “Object Orientation and Graphical User Interface”! In this module, I will guide you to understand what object orientation is and the relationship between graphical user interface and object orientation. Learners are only required to understand the concepts so that you can more freely and easily pick up various new functions in future. No program writing is required here. Besides, you also need to master the basic framework of GUI, common components and layout management. After learning them, you will find development with GUI is actually not remote. It has an Easter egg, too ~~~ ", "video": ["1 GUI and Object Orientation", "2 Abstraction", "3 Inheritance", "1 Basic Framework of GUI", "2 Common Components of GUI", "3 Layout Management", "4 Other GUI Libraries", "5 Comprehensive Applications", "5.2 Comprehensive practice project", "5 code snippets for reference only", "Object Orientation and Graphical User Interface quiz", "Examination"], "title": "Object Orientation and Graphical User Interface"}], "title": "Data Processing Using Python"}, {"course_info": "About this course: No doubt working with huge data volumes is hard, but to move a mountain, you have to deal with a lot of small stones. But why strain yourself? Using  Mapreduce and Spark you tackle the issue partially, thus leaving some space for high-level tools. Stop  struggling to make your big data workflow productive and efficient,  make use of the tools we are offering you.\n \nThis course will teach you how to:\n- Warehouse your data efficiently using Hive, Spark SQL and Spark DataFframes. \n- Work with large graphs, such as social graphs or networks. \n- Optimize your Spark applications for maximum performance.\n\nPrecisely, you will master your knowledge in:\n- Writing and executing Hive & Spark SQL queries;\n- Reasoning how the queries are translated into actual execution primitives (be it MapReduce jobs or Spark transformations);\n- Organizing your data in Hive to optimize disk space usage and execution times;\n- Constructing Spark DataFrames and using them to write ad-hoc analytical jobs easily;\n- Processing large graphs with Spark GraphFrames;\n- Debugging, profiling and optimizing Spark application performance.\n \nStill in doubt? Check this out. Become a data ninja by taking this course!\n\nSpecial thanks to:\n- Prof. Mikhail Roytberg, APT dept., MIPT, who was the initial reviewer of the project, the supervisor and mentor of half of the BigData team. He was the one, who helped to get this show on the road.\n- Oleg Sukhoroslov (PhD, Senior Researcher at IITP RAS), who has been teaching  MapReduce, Hadoop and friends since 2008. Now he is leading the infrastructure team.\n- Oleg Ivchenko (PhD student APT dept., MIPT), Pavel Akhtyamov (MSc. student at APT dept., MIPT) and Vladimir Kuznetsov (Assistant at P.G. Demidov Yaroslavl State University), superbrains who have developed and now maintain the infrastructure used for practical assignments in this course.\n- Asya Roitberg, Eugene Baulin, Marina Sudarikova. These people never sleep to babysit this course day and night, to make your learning experience productive, smooth and exciting.", "level": "Advanced", "package_name": "Big Data for Data Engineers Specialization ", "created_by": "Yandex", "package_num": "2", "teach_by": [{"name": "Natalia Pritykovskaya", "department": null}, {"name": "Pavel Klemenkov", "department": "Rambler&Co"}, {"name": "Pavel Mezentsev ", "department": "PulsePoint inc"}, {"name": "Alexey A. Dral", "department": "Algorithms and Programming Technologies dept. MIPT"}], "target_audience": "Who is this class for: This course is aimed to everybody, who feel interest in Big Data. As the technologies covered throughout the course operate in Unix environment, we expect you to have basic understanding of the subject. Things like processes and files assumed to be familiar for the learner. Python is required to complete programming assignments.", "rating": "3.9", "week_data": [{"description": "", "video": ["Computations Optimization", "What is BigData Analysis?", "Tools For BigData Analysis", "Graph Data Analysis", "Meet Alexey Dral", "Meet Pavel Mezentsev", "Meet Natalia Pritykovskaya", "Meet Pavel Klemenkov"], "title": "Welcome to the Second Course: Big Data Analysis"}, {"description": "", "video": ["Analytics: Business Use Cases", "HTTP Web Service: Access Log Format", "Business Use Cases: Solution with Hive", "(optional) SQL: likbez", "Hive Data Definition Language (DDL)", "Hive Data Manipulation Language (DML)", "Hive: SQL over Hadoop MapReduce", "Hive Analytics: RegexSerDe, Views", "(optional) Regular Expressions, Likbez", "Hive Analytics: UDF, UDAF, UDTF", "Hive Streaming", "Hive PTF (Window Functions)", "Hive Analytics with UDF and Streaming", "Hive Optimization: Partitioning, Bucketing and Sampling", "Hive Map-Side Joins: Plain, Bucket, Sort-Merge", "Hive Optimization: Data Skew", "Hive Optimization: Row-Columnar File Formats, Compression", "Hive assignment. Intro", "Rate this week", "Instructions for Hive programming assignment", "Hive final"], "title": "Big Data SQL: Hive"}, {"description": "", "video": ["First Assignment: instructions", "How to submit your first assignment", "Hive assignment. Intro", "Rate this week", "Demo Assignment", "Hive assignment. Task1", "Hive assignment. Task2", "Hive assignment. Task3"], "title": "Big Data SQL: Hive (practice week)"}, {"description": "", "video": ["Advantages of Spark SQL", "What is Pandas DataFrame and how to create it", "How to process a DataFrame as SQL", "Working with Hive", "Reading and Writing Files", "Introducing DataFrame and SQL", "RDD vs. DF vs. SQL", "Projection and Filtering", "Functions", "Aggregates", "Join", "User Defined Functions", "Time Processing", "Window Functions", "Two-Dimensional Distributions", "Rate this week", "Spark SQL and Spark Dataframe"], "title": "Spark SQL and Spark Dataframe"}, {"description": "", "video": ["Graph examples", "Graph representation", "Counting common friends. Part I", "Counting common friends. Part II", "Counting common friends. Part III", "Graph Representations", "GraphFrames: Introduction", "Motif Finding: DSL", "Motif Finding: Counting Mutual Friends", "Motif Finding: Under The Hood. Part 1", "Motif Finding: Under The Hood. Part 2", "Motif Finding", "Triangles Count: Introduction", "Triangles Count: Edge Lists", "Triangles Count: GraphFrame", "Triangles Count", "Rate this week", "Graph Analysis from Big Data Perspective", "Counting number of the mutual friends"], "title": "Graph Analysis from Big Data Perspective"}, {"description": "", "video": ["Introduction", "Algorithm", "GraphFrames", "Connected Components", "Random Walk", "Page Rank Algorithm", "RDD Implementation", "GraphFrames API", "PageRank", "Taste Graph. Part I", "Taste Graph. Part II", "Taste Graph. Part III", "Label Propagation Algorithm (LPA)", "Graph based Music Recommender", "Rate this week", "PageRank and Recent Advances", "Graph based Music Recommender. Task 1", "Graph based Music Recommender. Task 2", "Graph based Music Recommender. Task 3", "Graph based Music Recommender. Task 4", "Graph based Music Recommender. Task 5", "Graph based Music Recommender. Task 6"], "title": "PageRank and Recent Advances"}, {"description": "", "video": ["Welcome", "Spark Execution Model", "Shuffle. Where to send data?", "Shuffle. How to send data?", "Optimizing Functions", "PageRank Optimization", "Spark Execution Model & RDD Internals", "Spark SQL. Motivation", "Catalyst", "Catalyst Optimization Example", "Joins", "Optimizing Joins", "UDF Optimization", "Spark SQL and Catalyst", "Persistance and Checkpointing", "Memory Management", "Resource Allocation", "Dynamic Allocation", "Speculative Execution", "Memory management and resource allocation", "Deployment of the environment", "Rate this week", "Final Quiz", "Breadth-first search in Spark SQL"], "title": "Spark Internals and Optimization"}], "title": "Big Data Analysis: Hive, Spark SQL, DataFrames and GraphFrames"}, {"course_info": "About this course: This one-week course describes the process of analyzing data and how to manage that process. We describe the iterative nature of data analysis and the role of stating a sharp question, exploratory data analysis, inference, formal statistical modeling, interpretation, and communication. In addition, we will describe how to direct analytic activities within a team and to drive the data analysis process towards coherent and useful results. \n\nThis is a focused course designed to rapidly get you up to speed on the process of data analysis and how it can be managed. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward.\n\nAfter completing this course you will know how to….\n\n1. Describe the basic data analysis iteration\n2. Identify different types of questions and translate them to specific datasets\n3. Describe different types of data pulls\n4. Explore datasets to determine if data are appropriate for a given question\n5. Direct model building efforts in common data analyses\n6. Interpret the results from common data analyses\n7. Integrate statistical findings to form coherent data analysis presentations\n\nCommitment: 1 week of study, 4-6 hours\n\nCourse cover image by fdecomite. Creative Commons BY https://flic.kr/p/4HjmvD", "level": null, "package_name": "Executive Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "3", "teach_by": [{"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "Welcome to Managing Data Analysis! This course is one module, intended to be taken in one week. The course works best if you follow along with the material in the order it is presented. Each lecture consists of videos and reading materials that expand on the lecture. I'm excited to have you in the class and look forward to your contributions to the learning community. If you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center. Good luck as you get started, and I hope you enjoy the course!", "video": ["What this Course is About", "Pre-Course Survey", "Course Textbook: The Art of Data Science", "Conversations on Data Science", "Data Science as Art", "Data Analysis Iteration", "Stages of Data Analysis", "Epicycles of Analysis", "Six Types of Questions", "Six Types of Questions", "Characteristics of a Good Question", "Characteristics of a Good Question", "Exploratory Data Analysis Goals & Expectations", "EDA Check List", "Using Statistical Models to Explore Your Data (Part 1)", "Using Statistical Models to Explore Your Data (Part 2)", "Assessing a Distribution", "Assessing Linear Relationships", "Exploratory Data Analysis: When to Stop", "Exploratory Data Analysis: When Do We Stop?", "Making Inferences from Data: Introduction", "Populations Come in Many Forms", "Inference: What Can Go Wrong", "Factors Affecting the Quality of Inference", "A Note on Populations", "General Framework", "Associational Analyses", "Prediction Analyses", "Inference vs. Prediction", "Inference vs. Prediction", "Interpreting Your Results", "Interpreting Your Results", "Routine Communication in Data Analysis", "Making a Data Analysis Presentation", "Routine Communication", "Post-Course Survey", "Data Analysis Iteration", "Stating and Refining the Question", "Exploratory Data Analysis", "Inference", "Formal Modeling, Inference vs. Prediction", "Interpretation", "Communication"], "title": "Managing Data Analysis"}], "title": "Managing Data Analysis"}, {"course_info": "About this course: Spreadsheet software remains one of the most ubiquitous pieces of software used in workplaces across the world. Learning to confidently operate this software means adding a highly valuable asset to your employability portfolio. In this third course of our Excel specialization Excel Skills for Business you will delve deeper into some of the most powerful features Excel has to offer. When you have successfully completed the course you will be able to\n\nCheck for and prevent errors in spreadsheets; \nCreate powerful automation in spreadsheets; \nApply advanced formulas and conditional logic to help make informed business decisions; and\nCreate spreadsheets that help forecast and model data. \n\nOnce again, we have brought together a great teaching team that will be with you every step of the way. Nicky, Prashan and myself will guide you through each week. As we are exploring these more advanced topics, we are following Alex who is an Excel consultant called in by businesses that experience issues with their spreadsheets.", "level": "Intermediate", "package_name": "Excel Skills for Business Specialization ", "created_by": "Macquarie University", "package_num": "3", "teach_by": [{"name": "Dr Yvonne Breyer", "department": "Faculty of Business and Economics"}], "target_audience": "Who is this class for: Completing Excel Skills for Business: Intermediate II will be a highly valuable asset for anyone wanting to improve their employment portfolio. This course is for anyone who works with Excel on a regular or daily basis, for self-taught learners who want to fill gaps in their knowledge and for anyone wanting to consolidate their intermediate knowledge of Excel.  ", "rating": "4.9", "week_data": [{"description": "", "video": ["Welcome", "Course goals and weekly learning objectives", "Excel: Who, Why, How?", "Week 1 Introduction", "Week 1 Discussion", "Read me before you start: Quizzes and navigation", "Read me before you start: Versions and regions", "Download the Week 1 workbooks", "Practice Video: Data Validation", "Data Validation", "Practice Video: Creating Drop-down Lists", "Creating Drop-down Lists", "Practice Video: Using Formulas in Data Validation", "Using Formulas in Data Validation", "Practice Video: Working with Data Validation", "Working with Data Validation", "Practice Video: Advanced Conditional Formatting", "Advanced Conditional Formatting", "Week 1 Wrap-Up", "Week 1 Practice Challenge", "Keyboard Shortcuts, Excel Terminology, and Ninja Tips", "Week 1 Final Assignment"], "title": "Data Validation"}, {"description": "", "video": ["Week 2 Introduction", "Week 2 Discussion", "Download the Week 2 workbooks", "Practice Video: Logical Functions I: IF", "Logical Functions I: IF", "Practice Video: Logical Functions II: AND, OR", "Logical Functions II: AND, OR", "Practice Video: Combining Logical Functions I: IF, AND, OR", "Combining Logical Functions I: IF, AND, OR", "Practice Video: Combining Logical Functions II: Nested IFs", "Combining Logical Functions II: Nested IFs", "Practice Video: Handling Errors: IFERROR, IFNA", "Handling Errors: IFERROR, IFNA", "Week 2 Wrap-Up", "Week 2 Practice Challenge", "Keyboard Shortcuts, Excel Terminology, and Ninja Tips", "Week 2 Final Assignment"], "title": "Conditional Logic"}, {"description": "", "video": ["Week 3 Introduction", "Week 3 Discussion", "Download the Week 3 workbooks", "Practice Video: Introduction to Lookups: CHOOSE", "Introduction to Lookups: CHOOSE", "Practice Video: Approximate Matches: Range VLOOKUP", "Approximate Matches: Range VLOOKUP", "Practice Video: Exact Matches: Exact Match VLOOKUP", "Exact Matches: Exact Match VLOOKUP", "Practice Video: Finding a Position: MATCH", "Finding a Position: MATCH", "Practice Video: Dynamic Lookups: INDEX, MATCH", "Dynamic Lookups: INDEX, MATCH", "Week 3 Wrap-Up", "Week 3 Practice Challenge", "Keyboard Shortcuts, Excel Terminology, and Ninja Tips", "Week 3 Final Assignment"], "title": "Automating Lookups"}, {"description": "", "video": ["Week 4 Introduction", "Week 4 Discussion", "Download the Week 4 workbooks", "Practice Video: Error Checking", "Error Checking", "Practice Video: Formula Calculation Options", "Formula Calculation Options", "Practice Video: Trace Precedents and Dependents", "Tracing Precedents and Dependents", "Practice Video: Evaluate Formula, Watch Window", "Evaluate Formula, Watch Window", "Practice Video: Protecting Workbooks and Worksheets", "Protecting Workbooks and Worksheets", "Week 4 Wrap-Up", "Find the remaining errors", "Week 4 Practice Challenge", "Keyboard Shortcuts, Excel Terminology, and Ninja Tips", "Week 4 Final Assignment"], "title": "Formula Auditing and Protection"}, {"description": "", "video": ["Week 5 Introduction", "Week 5 Discussion", "Download the Week 5 workbooks", "Practice Video: Modelling Functions: SUMPRODUCT", "Modelling Functions (SUMPRODUCT)", "Practice Video: Data Tables", "Data Tables", "Practice Video: Goal Seek", "Goal Seek", "Practice Video: Scenario Manager", "Scenario Manager", "Practice Video: Solver", "Solver", "Week 5 Wrap-Up", "Week 5 Practice Challenge", "Keyboard Shortcuts, Excel Terminology, and Ninja Tips", "Week 5 Final Assignment"], "title": "Data Modelling"}, {"description": "", "video": ["Week 6 Introduction", "Week 6 Discussion", "Download the Week 6 workbooks", "Practice Video: Record a Macro", "Record a Macro", "Practice Video: Run a Macro", "Run your Macro", "Practice Video: Edit a Macro", "Edit a Macro", "Practice Video: Working with Macros", "Working with Macros", "Practice Video: Relative Reference Macros", "Relative Reference Macros", "Week 6 Wrap-Up", "Week 6 Practice Challenge", "Keyboard Shortcuts, Excel Terminology, and Ninja Tips", "Week 6 Final Assignment"], "title": "Recording Macros"}, {"description": "", "video": ["Final Assessment"], "title": "Final Assessment"}], "title": "Excel Skills for Business: Intermediate II"}, {"course_info": "About this course: Learn how to model social and economic networks and their impact on human behavior.  How do networks form, why do they exhibit certain patterns, and how does their structure impact diffusion, learning, and other behaviors?   We will bring together models and techniques from economics, sociology, math, physics, statistics and computer science to answer these questions.\n\nThe course begins with some empirical background on social and economic networks, and an overview of concepts used to describe and measure networks. Next, we will cover a set of models of how networks form, including random network models as well as strategic formation models, and some hybrids. We will then discuss a series of models of how networks impact behavior, including contagion, diffusion, learning, and peer influences.\n\nYou can find a more detailed syllabus here:  http://web.stanford.edu/~jacksonm/Networks-Online-Syllabus.pdf \n\nYou can find a short introductory videao here: http://web.stanford.edu/~jacksonm/Intro_Networks.mp4", "level": "Advanced", "package_name": null, "created_by": "Stanford University", "package_num": null, "teach_by": [{"name": "Matthew O. Jackson", "department": "Economics"}], "target_audience": "Who is this class for: The course is aimed at people interested in researching social and economic networks, but should be accessible to advanced undergraduates and other people who have some prerequisites in mathematics and statistics. For example, it will be assumed that students are comfortable with basic concepts from linear algebra (e.g., matrix multiplication), probability theory (e.g., probability distributions, expected values, Bayes' rule), and statistics (e.g., hypothesis testing). Beyond those concepts, the course is self-contained.", "rating": "4.8", "week_data": [{"description": "Examples of Social Networks and their Impact, Definitions, Measures and Properties: Degrees, Diameters, Small Worlds, Weak and Strong Ties, Degree Distributions", "video": ["An Introduction to the Course", "Syllabus", "1.1: Introduction", "1.2: Examples and Challenges ", "1.2.5 Background Definitions and Notation (Basic - Skip if familiar  8:23)", "1.3: Definitions and Notation ", "1.4: Diameter ", "1.5: Diameter and Trees ", "1.6: Diameters of Random Graphs (Optional/Advanced 11:12)", "1.7: Diameters in the World ", "1.8: Degree Distributions ", "1.9: Clustering ", "1.10: Week 1 Wrap", "Quiz Week 1", "Optional: Empirical Analysis of Network Data using Gephi or Pajek", "Slides from Lecture 1, with References", "OPTIONAL - Advanced Problem Set 1", "Problem Set 1"], "title": "Introduction, Empirical Background and Definitions  "}, {"description": "Homophily, Dynamics, Centrality Measures: Degree, Betweenness, Closeness, Eigenvector, and Katz-Bonacich. Erdos and Renyi Random Networks: Thresholds and Phase Transitions", "video": ["2.1: Homophily", "2.2: Dynamics and Tie Strength ", "2.3: Centrality Measures ", "2.4: Centrality – Eigenvector Measures ", "2.5a: Application - Centrality Measures ", "2.5b: Application – Diffusion Centrality ", "2.6: Random Networks ", "2.7: Random Networks - Thresholds and Phase Transitions ", "2.8: A Threshold Theorem (optional/advanced 13:00)", "2.9: A Small World Model ", "2.10 Week 2 Wrap", "Quiz Week 2", "Optional: Empirical Analysis of Network Data", "Slides from Lecture 2, with references", "OPTIONAL - Advanced Problem Set 2", "OPTIONAL - Solutions to Advanced PS 1", "Problem Set 2"], "title": "Background, Definitions, and Measures Continued"}, {"description": "Poisson Random Networks, Exponential Random Graph Models, Growing Random Networks, Preferential Attachment and Power Laws, Hybrid models of Network Formation.", "video": ["3.1: Growing Random Networks", "3.2: Mean Field Approximations ", "3.3: Preferential Attachment ", "3.4: Hybrid Models ", "3.5: Fitting Hybrid Models ", "3.6: Block Models ", "3.7: ERGMs ", "3.8: Estimating ERGMs ", "3.9: SERGMs ", "3.10: SUGMs ", "3.11: Estimating SUGMs (Optional/Advanced 21:03)", "3.12: Week 3 Wrap", "Quiz Week 3", "Optional: Empirical Analysis of Network Data", "Optional:  Using Statnet in R to Estimate an ERGM", "Slides from Lecture 3, with references", "OPTIONAL - Advanced Problem Set 3", "OPTIONAL - Solutions to Advanced PS 2", "Problem Set 3"], "title": "Random Networks"}, {"description": "Game Theoretic Modeling of Network Formation, The Connections Model, The Conflict between Incentives and Efficiency, Dynamics, Directed Networks, Hybrid Models of Choice and Chance.", "video": ["4.1: Strategic Network Formation", "4.2: Pairwise Stability and Efficiency ", "4.3: Connections Model ", "4.4: Efficiency in the Connections Model (Optional/Advanced 12:41)", "4.5: Pairwise Stability in the Connections Model ", "4.6:  Externalities and the Coauthor Model ", "4.7: Network Formation and Transfers ", "4.8: Heterogeneity in Strategic Models ", "4.9: SUGMs and Strategic Network Formation (Optional/Advanced 13:47)", "4.10: Pairwise Nash Stability (Optional/Advanced 11:34)", "4.11: Dynamic Strategic Network Formation (Optional/Advanced 11:57)", "4.12: Evolution and Stochastics (Optinoal/Advanced 16:05)", "4.13: Directed Network Formation (Optional/Advanced 16:38)", "4.14: Application Structural Model (Optional/Advanced 35:06)", "4.15: Week 4 Wrap", "Quiz Week 4", "Slides from Lecture 4, with references", "OPTIONAL - Advanced Problem Set 4", "OPTIONAL - Solutions to Advanced PS 3", "Problem Set 4"], "title": "Strategic Network Formation"}, {"description": "Empirical Background, The Bass Model, Random Network Models of Contagion, The SIS model, Fitting a Simulated Model to Data.", "video": ["5.1: Diffusion", "5.2: Bass Model", "5.3: Diffusion on Random Networks ", "5.4: Giant Component Poisson Case ", "5.5: SIS Model", "5.6: Solving the SIS Model ", "5.7: Solving the SIS Model - Ordering (Optional/Advanced 24:16)", "5.8a: Fitting a Diffusion Model to Data (Optional/Advanced 22:47)", "5.8b: Application:  Financial Contagions (Optional/Advanced 12:47)", "5.8c: Application: Financial Contagions - Simulations (Optional/Advanced 13:41)", "5.9: Diffusion Summary ", "5.10: Week 5 Wrap", "Quiz Week 5", "Optional: Empirical Analysis of Network Data", "OPTIONAL - Advanced Problem Set 5", "OPTIONAL - Solutions to Advanced PS 4", "Slides from Lecture 5, with references", "Problem Set 5"], "title": "Diffusion on Networks"}, {"description": "Bayesian Learning on Networks, The DeGroot Model of Learning on a Network, Convergence of Beliefs, The Wisdom of Crowds, How Influence depends on Network Position..", "video": ["6.1: Learning", "6.2: DeGroot Model ", "6.3: Convergence in DeGroot Model ", "6.4: Proof of Convergence Theorem (Optional/Advanced 10:25)", "6.5: Influence ", "6.6: Examples of Influence ", "6.7: Information Aggregation ", "6.8: Learning Summary ", "6.9: Week 6 Wrap", "Quiz Week 6", "Slides from Lecture 6, with references", "OPTIONAL - Advanced Problem Set 6", "OPTIONAL - Solutions to Advanced PS 5", "Problem Set 6"], "title": "Learning on Networks"}, {"description": "Network Games, Peer Influences: Strategic Complements and Substitutes, the Relation between Network Structure and Behavior, A Linear Quadratic Game, Repeated Interactions and Network Structures.", "video": ["7.1: Games on Networks", "7.2: Complements and Substitutes ", "7.3: Properties of Equilibria ", "7.4: Multiple Equilibria ", "7.5: An Application ", "7.6: Beyond 0-1 Choices ", "7.7: A Linear Quadratic Model ", "7.8: RepeatedGames and Networks ", "7.9: Week 7 Wrap ", "7.9b: Course Wrap", "Quiz Week 7", "Slides from Lecture 7, with references", "OPTIONAL - Advanced Problem Set 7", "OPTIONAL - Solutions to Advanced PS 6", "OPTIONAL - Solutions to Advanced PS 7", "Problem Set 7"], "title": "Games on Networks"}, {"description": "The description goes here", "video": ["Final"], "title": "Final Exam"}], "title": "Social and Economic Networks:  Models and Analysis"}, {"course_info": "About this course: Accounting Analytics explores how financial statement data and non-financial metrics can be linked to financial performance.  In this course, taught by Wharton’s acclaimed accounting professors, you’ll learn how data is used to assess what drives financial performance and to forecast future financial scenarios. While many accounting and financial organizations deliver data, accounting analytics deploys that data to deliver insight, and this course will explore the many areas in which accounting data provides insight into other business areas including consumer behavior predictions, corporate strategy, risk management, optimization, and more. By the end of this course, you’ll understand how financial data and non-financial data interact to forecast events, optimize operations, and determine strategy. This course has been designed to help you make better business decisions about the emerging roles of accounting analytics, so that you can apply what you’ve learned to make your own business decisions and create strategy using financial data. ", "level": null, "package_name": "Business Analytics Specialization ", "created_by": "University of Pennsylvania", "package_num": "4", "teach_by": [{"name": "Brian J Bushee", "department": "Accounting"}, {"name": "Christopher D. Ittner", "department": "Accounting"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "The topic for this week is ratio analysis and forecasting. Since ratio analysis involves financial statement numbers, I’ve included two optional videos that review financial statements and sources of financial data, in case you need a review. We will do a ratio analysis of a single company during the module. First, we’ll examine the company's strategy and business model, and then we'll look at the DuPont analysis. Next, we’ll analyze profitability and turnover ratios followed by an analysis of the liquidity ratios for the company. Once we've put together all the ratios, we can use them to forecast future financial statements. (If you’re interested in learning more, I’ve included another optional video, on valuation). By the end of this week, you’ll be able to do a ratio analysis of a company to identify the sources of its competitive advantage (or red flags of potential trouble),  and then use that information to forecast its future financial statements.  ", "video": ["Module 1 Overview 1.0", "Review of Financial Statements (Optional) 1.1", "Sources for Financial Statement Information (Optional) 1.2", "Ratio Analysis: Case Overview 1.3", "Ratio Analysis: Dupont Analysis 1.4", "Ratio Analysis: Profitability and Turnover Ratios 1.5", "Ratio Analysis: Liquidity Ratios 1.6", "Forecasting 1.7", "Accounting-based Valuation (Optional) 1.8", "PDF of Lecture Slides", "Excel Files for Ratio Analysis", "Ratio Analysis and Forecasting Quiz"], "title": "Ratios and Forecasting"}, {"description": "This week we are going to examine \"earnings management\", which is the practice of trying to intentionally bias financial statements to look better than they really should look. Beginning with an overview of earnings management, we’ll cover means, motive, and opportunity: how managers actually make their earnings look better, their incentives for manipulating earnings, and how they get away with it. Then, we will investigate red flags for two different forms of revenue manipulation. Manipulating earnings through aggressive revenue recognition practices is the most common reason that companies get in trouble with government regulators for their accounting practices. Next, we will discuss red flags for manipulating earnings through aggressive expense recognition practices, which is the second most common reason that companies get in trouble for their accounting practices. By the end of this module, you’ll know how to spot earnings management and get a more accurate picture of earnings, so that you’ll be able to catch some bad guys in finance reporting!", "video": ["Module Overview: Earnings Management 2.0", "Overview of Earnings Management 2.1", "Revenue Recognition Red Flags: Revenue Before Cash Collection 2.2", "Revenue Recognition Red Flags: Revenue After Cash Collection 2.3", "Expense Recognition Red Flags: Capitalizing vs. Expensing 2.4", "Expense Recognition Red Flags: Reserve Accounts and Write-Offs 2.5", "PDFs of Lecture Slides", "Excel Files for Earnings Management", "Earnings Management"], "title": "Earnings Management"}, {"description": "This week, we’ll use big data approaches to try to detect earnings management. Specifically, we're going to use prediction models to try to predict how the financial statements would look if there were no manipulation by the manager. First, we’ll look at Discretionary Accruals Models, which try to model the non-cash portion of earnings or \"accruals,\" where managers are making estimates to calculate revenues or expenses. Next, we'll talk about Discretionary Expenditure Models, which try to model the cash portion of earnings. Then we'll look at Fraud Prediction Models, which try to directly predict what types of companies are likely to commit frauds. Finally, we’ll explore something called Benford's Law, which examines the frequency with which certain numbers appear. If certain numbers appear more often than dictated by Benford's Law, it's an indication that the financial statements were potentially manipulated. These models represent the state of the art right now, and are what academics use to try to detect and predict earnings management. By the end of this module, you'll have a very strong tool kit that will help you try to detect financial statements that may have been manipulated by managers.", "video": ["Module 3 Overview 3.0", "Discretionary Accruals: Model 3.1", "Discretionary Accruals: Cases 3.2", "Discretionary Expenditures: Models 3.3", "Discretionary Expenditures: Refinements and Cases 3.4", "Fraud Prediction Models 3.5", "Benford's Law 3.6", "PDFs of Lecture Slides", "Excel Files for Big Data and Prediction Models", "Big Data and Prediction Models"], "title": "Big Data and Prediction Models"}, {"description": "Linking non-financial metrics to financial performance is one of the most important things we do as managers, and also one of the most difficult. We need to forecast future financial performance, but we have to take non-financial actions to influence it. And we must be able to accurately predict the ultimate impact on financial performance of improving non-financial dimensions. In this module, we’ll examine how to uncover which non-financial performance measures predict financial results through asking fundamental questions, such as: of the hundreds of non-financial measures, which are the key drivers of financial success? How do you rank or weight non-financial measures which don’t share a common denominator?  What performance targets are desirable? Finally, we’ll look at some comprehensive examples of how companies have used accounting analytics to show how investments in non-financial dimensions pay off in the future, and finish with some important organizational issues that commonly arise using these models. By the end of this module, you’ll know how predictive analytics can be used to determine what you should be measuring, how to weight very, very different performance measures when trying to analyze potential financial results, how to make trade-offs between short-term and long-term objectives, and how to set performance targets for optimal financial performance.", "video": ["Introduction: Connecting Numbers to Non-financial Performance Measures 4.0", "Linking Non-financial Metrics to Financial Performance: Overview 4.1", "Steps to Linking Non-financial Metrics to Financial Performance 4.2", "Setting Targets 4.3", "Comprehensive Examples 4.4", "Incorporating Analysis Results in Financial Models 4.5", "Using Analytics to Choose Action Plans 4.6", "Organizational Issues 4.7", "PDF of Lecture Slides", "Expected Economic Value Spreadsheet", "Linking Non-financial Metrics to Financial Performance"], "title": "Linking Non-financial Metrics to Financial Performance"}], "title": "Accounting Analytics"}, {"course_info": "About this course: This course presents critical concepts and practical methods to support planning, collection, storage, and dissemination of data in clinical research.\n\nUnderstanding and implementing solid data management principles is critical for any scientific domain. Regardless of your current (or anticipated) role in the research enterprise, a strong working knowledge and skill set in data management principles and practice will increase your productivity and improve your science. Our goal is to use these modules to help you learn and practice this skill set. \n\nThis course assumes very little current knowledge of technology other than how to operate a web browser. We will focus on practical lessons, short quizzes, and hands-on exercises as we explore together best practices for data management.", "level": "Beginner", "package_name": null, "created_by": "Vanderbilt University", "package_num": null, "teach_by": [{"name": "Stephany Duda, PhD", "department": "Department of Biomedical Informatics"}, {"name": "Paul Harris, PhD", "department": "Biomedical Informatics and Biomedical Engineering"}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "This introductory module reviews the course structure and basic concepts in clinical research. We also discuss best practices for designing your clinical research data collection.", "video": ["How the Course Works", "Course Introduction", "Defining the Space", "Course Logistics", "What is your background in this area?", "Research Data Planning 1", "Research Data Planning 2", "Approaches to Data Collection", "Quiz 1", "Assignment 1 - Survey Question Critique"], "title": "Research Data Collection Strategy"}, {"description": "This module covers standards for study processes, concepts for regulatory compliance, and electronic data capture fundamentals.", "video": ["Standardization of Study Processes", "Validated Instruments", "Data Standards: What Can Standards Do for You?", "Data Standards: Basic Concepts and Overview", "What standards have you used?", "IRB, HIPAA, and FISMA", "GCP and 21 CFR Part 11", "Introduction to Electronic Data Capture (EDC)", "EDC Concepts: Data Exports, Logging, User Rights, Project Creation", "EDC Concepts: Data Imports, Scheduling, Reports, Internationalization", "Quiz 2"], "title": "Electronic Data Capture Fundamentals"}, {"description": "This module reviews the process of planning data elements for a real-world research study.", "video": ["Overview of the Study", "Study Procedures", "Baseline Data and Demographics", "Visit Data", "Review of Variables and Forms", "How have you assembled a study data management plan?", "Logging in to REDCap", "Walkthrough: Creating a Project and Adding the First Variables", "Walkthrough: Adding Fields to the Baseline Form", "Walkthrough: Adding File Fields and Formatting", "Quiz 3"], "title": "Planning a Data Strategy for a Prospective Study"}, {"description": "This week, we set up an Electronic Data Capture (EDC) instrument in REDCap for the Morphine vs. Marinol Study. We also review data processes that occur during the running of a study, including an overview of key data quality operations.", "video": ["Walkthrough: Creating Visit Forms", "Walkthrough: Copying Variables, Renaming Forms", "Walkthrough: Using the Shared Library, Longitudinal Events, Optional Modules, and User Rights", "Walkthrough: Testing the REDCap Project", "Example Study Wrap-Up", "Mid Study Activities 1", "Mid Study Activities 2", "Data Quality", "Data Quality Monitoring", "What data quality challenges have you encountered?", "Assignment 2 - First REDCap Assignment", "Quiz 4"], "title": "Practicing What We've Learned: Implementation"}, {"description": "In this week, we cover activities to wrap up your study and share data and results, as well as two lectures on other electronic sources of data that can be used in research.  In response to learner requests, we've also added several lectures on clinical data management in resource-limited settings, in collaboration with research colleagues from Indiana University. This is a long week of videos, but next week will be short on videos in exchange!", "video": ["Wrapping Up Your Study", "Sharing Your Work", "De-identifying Data", "De-identifying Dates", "Common Information Systems Used in Health Care", "Neuroimaging Data Management", "mHealth in Developing Countries", "What health care information systems have you worked with?", "Data Management for Multi-Center or Network Studies", "Resource-Limited Settings and Global Health", "Challenges of Collecting Data in Resource-Constrained Settings", "Data Privacy in Global Research", "International Data Sharing", "Quiz 5"], "title": "Post-Study Activities and Other Considerations"}, {"description": "In the final week, we cover how to collect data using surveys and review an example together. This week's assignment includes designing, distributing, and reporting on your own survey.", "video": ["Benefits, Scope, and Validated Instruments", "Survey Design", "Survey Procedures and Implementation", "Survey Testing, Administration, and Analysis", "Survey Overview and REDCap Setup", "Survey Testing and Distribution", "Share Assignment 3 survey links (optional)", "Thank you for participating!", "Week 6 Quiz", "Assignment 3 - Build, Deploy, and Report on a Survey"], "title": "Data Collection with Surveys"}], "title": "Data Management for Clinical Research"}, {"course_info": "About this course: This if the final course in the specialization which builds upon the knowledge learned in Python Programming Essentials, Python Data Representations, and Python Data Analysis.  We will learn how to install external packages for use within Python, acquire data from sources on the Web, and then we will clean, process, analyze, and visualize that data. This course will combine the skills learned throughout the specialization to enable you to write interesting, practical, and useful programs.\n\nBy the end of the course, you will be comfortable installing Python packages, analyzing existing data, and generating visualizations of that data.  This course will complete your education as a scripter, enabling you to locate, install, and use Python packages written by others. You will be able to effectively utilize tools and packages that are widely available to amplify your effectiveness and write useful programs.", "level": "Beginner", "package_name": "Introduction to Scripting in Python Specialization ", "created_by": "Rice University", "package_num": "4", "teach_by": [{"name": "Scott Rixner", "department": "Computer Science"}, {"name": "Joe Warren", "department": "Computer Science"}], "target_audience": "Who is this class for: This class is for people who have completed the other courses within the specialization and want to put the skills they have learned throughout the specialization to use in a culminating project.", "rating": "4.8", "week_data": [{"description": "This module will discuss the importance of using and writing documentation. The Python documentation is a valuable resource for learning about language features you haven't seen yet.", "video": ["Welcome!", "Class Structure", "Using Python Documentation", "Writing Documentation", "Python Built-in Modules", "Code Reuse", "Installing Packages in Thonny", "Practice Project: Drawing a USA Map in matplotlib", "Documentation"], "title": "Week 1 "}, {"description": "This module will teach you about packages and modules in Python, including how to install packages and how to create your own modules. You will also learn to use the Pygal plotting library.", "video": ["Python Packages and Modules", "Importing Your Own Code", "Line Plots with Pygal", "Installing Packages using PIP - Part 1", "Installing Packages using PIP - Part 2", "Practice Project: Extracting Data from an SVG File", "Project 1 Video", "Project Description: Creating Line Plots of GDP Data", "OwlTest: Automated Feedback and Assessment", "Project Submission History", "Project: Creating Line Plots of GDP Data"], "title": "Week 2"}, {"description": "This module will teach you about Python sets. Sets are used to hold unordered collections of data without duplicates. We will also discuss efficiency.", "video": ["Python Sets", "Hashing", "Analyzing the Efficiency of Your Code", "Comparing Two Methods for Joining CSV Files", "Practice Project: Reconciling Cancer-Risk Data with the USA Map", "Project 2 Video", "Project Description: Plotting GDP Data on a World Map - Part 1", "Project: Plotting GDP Data on World Map (Part 1)"], "title": "Week 3"}, {"description": "The final project of the specialization will enable you to demonstrate mastery of the concepts you have learned up to this point. You will also be able to understand and compare different approaches to reconciling two data sets. ", "video": ["Growing as a Scripter", "Version Control", "Practice Project: Visualizing Cancer-risk Data on the USA Map", "Project 3 Video", "Project Description: Plotting GDP Data on a World Map - Part 2", "Wrapup Video", "Project: Plotting GDP Data on World Map (Part 2)"], "title": "Week 4"}], "title": "Python Data Visualization"}, {"course_info": "About this course: This course introduces you to the basic biology of modern genomics and the experimental tools that we use to measure it. We'll introduce the Central Dogma of Molecular Biology and cover how next-generation sequencing can be used to measure DNA, RNA, and epigenetic patterns. You'll also get an introduction to the key concepts in computing and data science that you'll need to understand how data from next-generation sequencing experiments are generated and analyzed.  \n\nThis is the first course in the Genomic Data Science Specialization.", "level": null, "package_name": "Genomic Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "1", "teach_by": [{"name": "Steven Salzberg, PhD", "department": "Biomedical Engineering, Computer Science, and Biostatistics"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "In this Module, you can expect to study topics of \"Just enough molecular biology\", \"The genome\", \"Writing a DNA sequence\", \"Central dogma\", \"Transcription\", \"Translation\", and \"DNA structure and modifications\".", "video": ["Syllabus", "Pre-Course Survey", "Why Genomics?", "What Is Genomics?", "What Is Genomic Data Science?", "Just Enough Cell Biology", "Important Molecules in Molecular Biology", "The Human Genome Project", "Molecular Biology Structures", "From Genes to Phenotypes", "Quiz 1: Overview and Molecular Biology"], "title": "Overview"}, {"description": "In this module, you'll learn about polymerase chain reaction, next generation sequencing, and applications of sequencing.", "video": ["Polymerase Chain Reaction", "Next Generation Sequencing", "Applications of Sequencing", "Quiz 2: Measurement Technology"], "title": "Measurement Technology"}, {"description": "The lectures for this module cover a few basic topics in computing technology. We'll go over the foundations of computer science, algorithms, memory and data structures, efficiency, software engineering, and computational biology software.", "video": ["What Is Computer Science?", "Algorithms", "Memory and Data Structures", "Efficiency", "Software Engineering", "What is Computational Biology Software", "Quiz 3: Computing Technology"], "title": "Computing Technology"}, {"description": "In this module on Data Science Technology, we'll be covering quite a lot of information about how to handle the data produced during the sequencing process. We'll cover reproducibility, analysis, statistics, question types, the central dogma of inference, analysis code, testing, prediction, variation, experimental design, confounding, power, sample size, correlation, causation, and degrees of freedom.", "video": ["Why Care About Statistics?", "What Went Wrong?", "The Central Dogma of Statistics", "Data Sharing Plans", "Getting Help with Statistics", "Plotting Your Data", "Sample Size and Variability", "Statistical Significance", "Multiple Testing", "Study Design, Batch Effects, and Confounding", "Course Project Instructions and Reading", "Post-Course Survey", "Quiz 4: Data Science Technology", "Course Project"], "title": "Data Science Technology"}], "title": "Introduction to Genomic Technologies"}, {"course_info": "About this course: Useful quantitative models help you to make informed decisions both in situations in which the factors affecting your decision are clear, as well as in situations in which some important factors are not clear at all. In this course, you can learn how to create quantitative models to reflect complex realities, and how to include in your model elements of risk and uncertainty. You’ll also learn the methods for creating predictive models for identifying optimal choices; and how those choices change in response to changes in the model’s assumptions. You’ll also learn the basics of the measurement and management of risk. By the end of this course, you’ll be able to build your own models with your own data, so that you can begin making data-informed decisions. You’ll also be prepared for the next course in the Specialization.", "level": null, "package_name": "Business and Financial Modeling Specialization ", "created_by": "University of Pennsylvania", "package_num": "3", "teach_by": [{"name": "Sergei Savin", "department": "The Wharton School"}, {"name": "Senthil Veeraraghavan", "department": "The Wharton School"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "This module is designed to teach you how to analyze settings with low levels of uncertainty, and how to identify the best decisions in these settings. You'll explore the optimization toolkit, learn how to build an algebraic model using an advertising example, convert the algebraic model to a spreadsheet model, work with Solver to discover the best possible decision, and examine an example that introduces a simple representation of risk to the model. By the end of this module, you'll be able to build an optimization model, use Solver to uncover the optimal decision based on your data, and begin to adjust your model to account for simple elements of risk. These skills will give you the power to deal with large models as long as the actual uncertainty in the input values is not too high.", "video": ["Course Introduction", "1.1 How To Build an Optimization Model: Hudson Readers Ad Campaign", "1.2 Optimizing with Solver, and Alternative Data Inputs", "1.3 Adding Risk: Managing Investments at Epsilon Delta Capital", "PDFs of Slides for Week 1", "Excel Files for Week 1", "Week 1: Modeling in Low Uncertainty Quiz"], "title": "Week 1: Modeling Decisions in Low Uncertainty Settings"}, {"description": "What if uncertainty is the key feature of the setting you are trying to model? In this module, you'll learn how to create models for situations with a large number of variables. You'll examine high uncertainty settings, probability distributions, and risk, common scenarios for multiple random variables, how to incorporate risk reduction, how to calculate and interpret correlation values, and how to use scenarios for optimization, including sensitivity analysis and the efficient frontier. By the end of this module, you'll be able to identify and use common models of future uncertainty to build scenarios that help you optimize your business decisions when you have multiple variables and a higher degree of risk. ", "video": ["2.1 High Uncertainty Settings, Probability Distributions, Uncertainty and Risk", "2.2 Common Scenarios for Multiple Random Variables, Risk Reduction, and Calculating and Interpreting Correlation Values", "2.3 Using Scenarios for Optimizing Under High Uncertainty, Sensitivity Analysis and Efficient Frontier", "PDFs of Lecture Slides for Week 2", "Excel Files for Week 2", "Week 2: Modeling in High Uncertainty Quiz"], "title": "Week 2: Risk and Reward: Modeling High Uncertainty Settings"}, {"description": "When making business decisions, we often look to the past to make predictions for the future. In this module, you'll examine commonly used distributions of random variables to model the future and make predictions. You'll learn how to create meaningful data visualizations in Excel, how to choose the the right distribution for your data, explore the differences between discrete distributions and continuous distributions, and test your choice of model and your hypothesis for goodness of fit. By the end of this module, you'll be able to represent your data using graphs, choose the best distribution model for your data, and test your model and your hypothesis to see if they are the best fit for your data.", "video": ["3.1 Data and Visualization: Graphical Representation", "3.2, pt 1: Choosing Among Distributions: Discrete Distributions", "3.2, pt 2: Choosing Among Distributions: Continuous Distributions", "3.3 Hypothesis Testing and Goodness of Fit", "PDFs of Lecture Slides for Week 3", "Excel Files for Week 3", "Week 3: Choosing Fitting Distributions Quiz"], "title": "Week 3: Choosing Distributions that Fit Your Data"}, {"description": "This module is designed to help you use simulations to enabling compare different alternatives when continuous distributions are used to describe uncertainty. Through an in-depth examination of the simulation toolkit, you'll learn how to make decisions in high uncertainty settings where random inputs are described by continuous probability distributions. You'll also learn how to run a simulation model, analyze simulation output, and compare alternative decisions to decide on the most optimal solution.  By the end of this module, you'll be able to make decisions and manage risk using simulation, and more broadly, to make successful business decisions in an increasing complex and rapidly evolving business world.", "video": ["4.1: Modeling Uncertainty: From Scenarios to Continuous Distributions", "4.2 Connecting Random Inputs and Random Outputs in a Simulation", "4.3 Analyzing and Interpreting Simulation Output: Evaluating Alternatives Using Simulation Results", "Course Conclusion", "PDFs of Lecture Slides", "Excel files for Week 4", "Week 4: Using Simulations Quiz"], "title": "Week 4: Balancing Risk and Reward Using Simulation"}], "title": "Modeling Risk and Realities"}, {"course_info": "About this course: This course will continue the introduction to Python programming that started with Python Programming Essentials and Python Data Representations.  We'll learn about reading, storing, and processing tabular data, which are common tasks.  We will also teach you about CSV files and Python's support for reading and writing them.  CSV files are a generic, plain text file format that allows you to exchange tabular data between different programs. These concepts and skills will help you to further extend your Python programming knowledge and allow you to process more complex data.\n\nBy the end of the course, you will be comfortable working with tabular data in Python. This will extend your Python programming expertise, enabling you to write a wider range of scripts using Python.\n\nThis course uses Python 3.  While most Python programs continue to use Python 2, Python 3 is the future of the Python programming language. This course uses basic desktop Python development environments, allowing you to run Python programs directly on your computer.", "level": "Beginner", "package_name": "Introduction to Scripting in Python Specialization ", "created_by": "Rice University", "package_num": "3", "teach_by": [{"name": "Scott Rixner", "department": "Computer Science"}, {"name": "Joe Warren", "department": "Computer Science"}], "target_audience": "Who is this class for: This class is for people who have have the background from our Python Data Representations course.", "rating": "4.8", "week_data": [{"description": "This module will teach you about Python's dictionary data type and its capabilities.  Dictionaries are used to map keys to values within programs.", "video": ["Welcome!", "Class Structure", "Python Dictionaries", "Defining a Dictionary", "Dictionary Lookup and Update", "Checking Keys", "Dictionaries - Example", "Handling Dictionary Errors", "Practice Exercises for Dictionaries", "Dictionaries"], "title": "Dictionaries"}, {"description": "This module will teach you about storing tabular data within Python programs using lists and dictionaries.", "video": ["Iteration over Dictionaries", "Tabular Data", "Tabular Data as a Nested List", "Tabular Data as a Nested Dictionary", "Displaying Dictionaries", "Practice Exercises for Nested Data Structures", "Nested Representations for Tabular Data"], "title": "Tabular Data and Nested Data Structures"}, {"description": "This module will teach you the basics of CSV files and how to read them from Python programs. We will discuss the use of Python's csv module to help you access tabular data in CSV files.", "video": ["Tables and CSV Files", "CSV Files", "Parsing CSV Files", "Python's CSV Module", "CSV DictReader", "CSV Reader Options", "Experimenting with CSV Methods - Part 1", "Experimenting with CSV Methods - Part 2", "Practice Project: Loading Cancer-Risk Data", "Project Video for Part 1", "Project Description: Reading  and Writing CSV Files", "OwlTest: Automated Feedback and Assessment", "Project Submission History", "Project: Reading and Writing CSV Files"], "title": "Tabular Data and CSV Files"}, {"description": "This module will teach you how to sort data in Python. You will organize and analyze tabular data.", "video": ["Sorting", "Lambda", "Advanced Sorting", "Dictionaries vs. Lists for storing data", "Refactoring Your Code - Part 1", "Refactoring Your Code - Part 2", "Practice Project: Processing Cancer-Risk Data", "Project Video for Part 2", "Project Description: Analyzing Baseball Data", "Project: Analyzing Baseball Data"], "title": "Organizing Data"}], "title": "Python Data Analysis"}, {"course_info": "About this course: Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media such as blog articles, forum posts, product reviews, and tweets. Text data are unique in that they are usually generated directly by humans rather than a computer system or sensors, and are thus especially valuable for discovering knowledge about people’s opinions and preferences, in addition to many other kinds of knowledge that we encode in text. \n\nThis course will cover search engine technologies, which play an important role in any data mining applications involving text data for two reasons. First, while the raw data may be large for any particular problem, it is often a relatively small subset of the data that are relevant, and a search engine is an essential tool for quickly discovering a small subset of relevant text data in a large text collection. Second, search engines are needed to help analysts interpret any patterns discovered in the data by allowing them to examine the relevant original text data to make sense of any discovered pattern. You will learn the basic concepts, principles, and the major techniques in text retrieval, which is the underlying science of search engines.", "level": null, "package_name": "Data Mining  Specialization ", "created_by": "University of Illinois at Urbana-Champaign", "package_num": "2", "teach_by": [{"name": "ChengXiang Zhai", "department": "Department of Computer Science"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course.", "video": ["Welcome to Text Retrieval and Search Engines!", "Syllabus", "About the Discussion Forums", "Updating your Profile", "Social Media", "Course Errata", "Course Welcome Video", "Course Introduction Video", "Pre-Quiz", "Orientation Quiz"], "title": "Orientation"}, {"description": "During this week's lessons, you will learn of natural language processing techniques, which are the foundation for all kinds of text-processing applications, the concept of a retrieval model, and the basic idea of the vector space model. ", "video": ["Week 1 Overview", "Lesson 1.1: Natural Language Content Analysis", "Lesson 1.2: Text Access", "Lesson 1.3: Text Retrieval Problem", "Lesson 1.4: Overview of Text Retrieval Methods", "Lesson 1.5: Vector Space Model - Basic Idea", "Lesson 1.6: Vector Space Retrieval Model - Simplest Instantiation", "Week 1 Practice Quiz", "Week 1 Quiz"], "title": "Week 1"}, {"description": "In this week's lessons, you will learn how the vector space model works in detail, the major heuristics used in designing a retrieval function for ranking documents with respect to a query, and how to implement an information retrieval system (i.e., a search engine), including how to build an inverted index and how to score documents quickly for a query. ", "video": ["Week 2 Overview", "Lesson 2.1: Vector Space Model - Improved Instantiation", "Lesson 2.2: TF Transformation", "Lesson 2.3: Doc Length Normalization", "Lesson 2.4: Implementation of TR Systems", "Lesson 2.5: System Implementation - Inverted Index Construction", "Lesson 2.6: System Implementation - Fast Search", "Week 2 Practice Quiz", "Week 2 Quiz"], "title": "Week 2"}, {"description": "In this week's lessons, you will learn how to evaluate an information retrieval system (a search engine), including the basic measures for evaluating a set of retrieved results and the major measures for evaluating a ranked list, including the average precision (AP) and the normalized discounted cumulative gain (nDCG), and practical issues in evaluation, including statistical significance testing and pooling.", "video": ["Week 3 Overview", "Lesson 3.1: Evaluation of TR Systems", "Lesson 3.2: Evaluation of TR Systems - Basic Measures", "Lesson 3.3: Evaluation of TR Systems - Evaluating Ranked Lists - Part 1", "Lesson 3.4: Evaluation of TR Systems - Evaluating Ranked Lists - Part 2", "Lesson 3.5: Evaluation of TR Systems - Multi-Level Judgements", "Lesson 3.6: Evaluation of TR Systems - Practical Issues", "Week 3 Practice Quiz", "Programming Assignments Overview", "Week 3 Quiz", "Programming Assignment 1"], "title": "Week 3"}, {"description": "In this week's lessons, you will learn probabilistic retrieval models and statistical language models, particularly the detail of the query likelihood retrieval function with two specific smoothing methods, and how the query likelihood retrieval function is connected with the retrieval heuristics used in the vector space model. ", "video": ["Week 4 Overview", "Lesson 4.1: Probabilistic Retrieval Model - Basic Idea", "Lesson 4.2: Statistical Language Model", "Lesson 4.3: Query Likelihood Retrieval Function", "Lesson 4.4: Statistical Language Model - Part 1", "Lesson 4.5: Statistical Language Model - Part 2", "Lesson 4.6: Smoothing Methods - Part 1", "Lesson 4.7: Smoothing Methods - Part 2", "Week 4 Practice Quiz", "Week 4 Quiz"], "title": "Week 4"}, {"description": "In this week's lessons, you will learn feedback techniques in information retrieval, including the Rocchio feedback method for the vector space model, and a mixture model for feedback with language models. You will also learn how web search engines work, including web crawling, web indexing, and how links between web pages can be leveraged to score web pages. ", "video": ["Week 5 Overview", "Lesson 5.1: Feedback in Text Retrieval", "Lesson 5.2: Feedback in Vector Space Model - Rocchio", "Lesson 5.3: Feedback in Text Retrieval - Feedback in LM", "Lesson 5.4: Web Search: Introduction & Web Crawler", "Lesson 5.5: Web Indexing", "Lesson 5.6: Link Analysis - Part 1", "Lesson 5.7: Link Analysis - Part 2", "Lesson 5.8: Link Analysis - Part 3", "Week 5 Practice Quiz", "Week 5 Quiz"], "title": "Week 5"}, {"description": "In this week's lessons, you will learn how machine learning can be used to combine multiple scoring factors to optimize ranking of documents in web search (i.e., learning to rank), and learn techniques used in recommender systems (also called filtering systems), including content-based recommendation/filtering and collaborative filtering. You will also have a chance to review the entire course.", "video": ["Week 6 Overview", "Lesson 6.1: Learning to Rank - Part 1", "Lesson 6.2: Learning to Rank - Part 2", "Lesson 6.3: Learning to Rank - Part 3", "Lesson 6.4: Future of Web Search", "Lesson 6.5: Recommender Systems: Content-Based Filtering - Part 1", "Lesson 6.6:  Recommender Systems: Content-Based Filtering - Part 2", "Lesson 6.7: Recommender Systems: Collaborative Filtering - Part 1", "Lesson 6.8: Recommender Systems: Collaborative Filtering - Part 2", "Lesson 6.9: Recommender Systems: Collaborative Filtering - Part 3", "Lesson 6.10: Course Summary", "Week 6 Practice Quiz", "Week 6 Quiz", "Programming Assignment 2"], "title": "Week 6"}], "title": "Text Retrieval and Search Engines"}, {"course_info": "About this course: Want to understand your data network structure and how it changes under different conditions? Curious to know how to identify closely interacting clusters within a graph? Have you heard of the fast-growing area of graph analytics and want to learn more? This course gives you a broad overview of the field of graph analytics so you can learn new ways to model, store, retrieve and analyze graph-structured data.\n\nAfter completing this course, you will be able to model a problem into a graph database and perform analytical tasks over the graph in a scalable manner.  Better yet, you will be able to apply these techniques to understand the significance of your data sets for your own projects.", "level": null, "package_name": "Big Data Specialization ", "created_by": "University of California, San Diego", "package_num": "5", "teach_by": [{"name": "Amarnath Gupta", "department": "San Diego Supercomputer Center (SDSC)"}], "target_audience": null, "rating": "4.1", "week_data": [{"description": "Meet your instructor, Amarnath Gupta and learn about the course objectives.", "video": ["Welcome to Graph Analytics for Big Data"], "title": "Welcome to Graph Analytics"}, {"description": "Welcome! This week we will get a first exposure to graphs and their use in everyday life.  By the end of the module you will be able to create a graph applying core mathematical properties of graphs, and identify the kinds of analysis questions one might be able to ask of such a graph.  We hope the you will be inspired as to how graphical representations might enable you to answer new Big Data problems!", "video": ["What to learn in this module", "What is a Graph?", "Why Graphs?", "Let's Discuss: What else do you interact with that can be represented as a graph?", "Why Graphs? Example 1: Social Networking", "Why Graphs? Example 2: Biological Networks", "Why Graphs? Example 3: Human Information Network Analytics", "Why Graphs? Example 4: Smart Cities", "The Purpose of Analytics", "What are the impact of Big Data's V's on Graphs?", "Optional: What's the most interesting graph you reviewed?", "Download Slides for this Module", "Introduction to Graphs", "Graphs in Everyday Life"], "title": "Introduction to Graphs"}, {"description": "", "video": ["What to learn in this module", "Focusing On Graph Analytics Techniques", "If this module takes a little longer... that's OK!", "Download All Slides for Module 3", "Path Analytics", "The Basic Path Analytics Question: What is the Best Path?", "Applying Dijkstra's Algorithm", "Inclusion and Exclusion Constraints", "Let's Discuss: Where do you see path problems in your life?", "Connectivity Analytics", "Disconnecting a Graph", "Connectedness: Indegree and Outdegree", "Community Analytics and Local Properties", "Let's Discuss: What kind of community analytics question would you like to ask?", "Global Property: Modularity", "Centrality Analytics", "Optional Lecture 1: Bi-directional Dijkstra Algorithm", "Optional Lecture 2: Goal-directed Dijkstra Algorithm", "Optional Lecture 3: Power Law Graphs", "Optional Lecture 4: Measuring Graph Evolution", "Optional Lecture 5: Eigenvector Centrality", "Optional Lecture 6: Key Player Problems", "Graph Analytics Applications", "Connectivity, Community, and Centrality Analytics"], "title": "Graph Analytics"}, {"description": "Welcome to the 4th module in the Graph Analytics course. Last week, we got a glimpse of a number of graph properties and why they are important. This week we will use those properties for analyzing graphs using a free and powerful graph analytics tool called Neo4j. We will demonstrate how to use Cypher, the query language of Neo4j, to perform a wide range of analyses on a variety of graph networks. ", "video": ["Welcome to Graph Analytics Techniques", "About the Supplementary Resources", "Downloading, Installing, and Running Neo4j - Supplementary Resources", "Hands-On: Downloading, Installing, and Running Neo4j", "Getting Started With Neo4j - Supplementary Resources", "Hands-On: Getting Started With Neo4j", "Adding to and Modifying a Graph - Supplementary Resources", "Hands-On: Modifying a Graph With Neo4j", "Download datasets used in this Graph Analytics with Neo4j", "Importing Data Into Neo4j - Supplementary Resources", "Hands-On: Importing Data Into Neo4j", "FAQ", "Basic Queries in Neo4j With Cypher - Supplementary Resources", "Hands-On: Basic Queries in Neo4j With Cypher - Part 1", "Hands-On: Basic Queries in Neo4j With Cypher - Part 2", "Path Analytics in Neo4j With Cypher - Supplementary Resources", "Hands-On: Path Analytics in Neo4j Using Cypher - Part 1", "Hands-On: Path Analytics in Neo4j Using Cypher - Part 2", "Connectivity Analytics in Neo4j with Cypher - Supplementary Resources", "Hands-On: Connectivity Analytics in Neo4j With Cypher", "Assignment: Practicing Graph Analytics in Neo4j With Cypher", "Download All Neo4j Supplementary Resources (PDFs)", "Quiz: Graph Analytics With Neo4j", "Assessment Questions on 'Practicing Graph Analytics in Neo4j With Cypher'"], "title": "Graph Analytics Techniques"}, {"description": "In the last two modules we have learned about graph analytics and graph data management. This week we will study how they come together. There are programming models and software frameworks created specifically for graph analytics.  In this module we'll give an introductory tour of these models and frameworks.  We will learn to implement what you learned in Week 2 and build on it using GraphX and Giraph.   ", "video": ["Introduction: Large Scale Graph Processing", "A Parallel Programming Model for Graphs", "Pregel: The System That Changed Graph Processing", "Giraph and GraphX", "Beyond Single Vertex Computation", "Introduction to GraphX: Hands-On Demonstrations", "Datasets and Libraries for Example of Analytics Hands On", "Download all of the readings for this section as a PDF", "Hands On: Building a Graph", "Hands On: Building a Graph Reading", "Hands On: Building a Degree Histogram", "Hands On: Building a Degree Histogram Reading", "Hands On: Plot the Degree Histogram", "Hands On: Plot the Degree Histogram Reading", "Hands On: Network Connectedness and Clustering Components", "Hands On: Network Connectedness and Clustering Components Reading", "Hands On: Joining Graph Datasets", "Hands On: Joining Graph Datasets Reading", "Using GraphX"], "title": "Computing Platforms for Graph Analytics"}], "title": "Graph Analytics for Big Data"}, {"course_info": "About this course: This course will expose you to the data analytics practices executed in the business world. We will explore such key areas as the analytical process, how data is created, stored, accessed, and how the organization works with data and creates the environment in which analytics can flourish.\n\nWhat you learn in this course will give you a strong foundation in all the areas that support analytics and will help you to better position yourself for success within your organization. You’ll develop skills and a perspective that will make you more productive faster and allow you to become a valuable asset to your organization.\n\nThis course also provides a basis for going deeper into advanced investigative and computational methods, which you have an opportunity to explore in future courses of the Data Analytics for Business specialization.", "level": null, "package_name": "Advanced Business Analytics Specialization ", "created_by": "University of Colorado Boulder", "package_num": "1", "teach_by": [{"name": "David Torgerson", "department": null}], "target_audience": "Who is this class for: This course is designed to have broad appeal across many types of learners.  Anyone who is looking to gain an understanding of how business analytics is actually performed in real organizations will benefit.\n\nThis course is primarily aimed at professionals who have a bachelor’s degree and/or some exposure to the business world.  Those with technical degrees or more advanced business degrees like an MBA will find certain areas easier to absorb, and may get maximum value from the course.  However, even undergraduates in non-technical fields or advanced high-school students pursuing internships will be able to follow most concepts and get value from the course.  Finally, even professionals who have had deep experiences in methods will likely find value in this course. \n", "rating": "4.6", "week_data": [{"description": "Welcome to week 1! In this module we’ll learn how to think about analytical problems and examine the process by which data enables analysis & decision making. We’ll introduce a framework called the Information-Action Value chain which describes the path from events in the world to business action, and we’ll look at some of the source systems that are used to capture data. At the end of this course you will be able to: Explain the information lifecycle from events in the real world to business actions, and how to think about analytical problems in that context , Recognize the types of events and characteristics that are often used in business analytics, and explain how the data is captured by source systems and stored using both traditional and emergent technologies, Gain a high-level familiarity with relational databases and learn how to use a simple but powerful language called SQL to extract analytical data sets of interest, Appreciate the spectrum of roles involved in the data lifecycle, and gain exposure to the various ways that organizations structure analytical functions,  Summarize some of the key ideas around data quality, data governance, and data privacy", "video": ["Introduction to the Course", "Introduction to the Specialization", "0. Introduction to Data & Analysis in Real World", "Week 1 Lecture Slides", "1. Thinking about Analytical Problems", "2. Conceptual Business Models", "3. The information-Action Value Chain (Part 1)", "4. The information-Action Value Chain (Part 2)", "5. Real World Events and Characteristics", "6. Data Capture by Source Systems", "Week 1 Quiz Answer Key", "You can now start part 1 of the final assignment!", "What is more important for a data analyst to understand, the business or analytical methods?", "What are some of the challenges you might find in communicating the results of your analysis? ", "Week 1 Quiz"], "title": "Data and Analysis in the Real World"}, {"description": "In this module we’ll learn about the technologies that enable analytical work.  We’ll examine data storage and databases, including the relational database.  We’ll talk about Big Data and Cloud technologies and ideas like federation, virtualization, and in-memory computing.  We’ll also walk through a landscape of some of the more common tool classes and learn how these tools support common analytical tasks.\n", "video": ["0. Introduction - Analytical Technologies", "Week 2 Lecture Slides", "1. Data Storage and Databases", "2. Big Data & the Cloud", "3. Virtualization, Federation, and In-Memory Computing", "4. The Relational Database", "5. Data Tools Landscape", "6. The Tools of the Data Analyst", "Week 2 Quiz Answer Key", "You can now start part 2 of the final assignment!", "Information tools used at different organizations", "Different approaches to building a relational database", "Week 2 Quiz"], "title": "Analytical Tools"}, {"description": "In this module we’ll learn how to extract data from a relational database using Structured Query Language, or SQL.  We’ll cover all the basic SQL commands and learn how to combine and stack data from different tables.  We’ll also learn how to expand the power of our queries using operators and handle additional complexity using subqueries.", "video": ["0. Introduction - Data Extraction Using SQL", "Week 3 Lecture Slides", "1. Introduction to SQL", "2. Aggregating and Sorting Data in SQL", "3. Extracting Data from Multiple Tables", "4. Stacking Data with UNION Command", "5. Extending SQL Queries Using Operators", "6. Using SQL Subqueries", "Week 3 SQL Assignment Answer Key", "You can now start part 3 of the final assignment!", "Advantages and shortcomings of accessing data using SQL", "SQL Coding Assignment"], "title": "Data Extraction Using SQL"}, {"description": "In this module we focus on the people and organizations that work with data and actually execute analytics.  We’ll discuss who does what and see how organizational structures can influence efficiency and effectiveness.  We’ll also look at the supporting rules & processes that help an analytical organization run smoothly, like Data Governance, Data Privacy, and Data Quality.", "video": ["0. Introduction to Real World Analytical Orgs", "Week 4 Lecture Slides", "1. Analytical Organizations – Roles", "2. Analytical Organizations – Structures", "3. Data Governance", "4. Data Privacy", "5. Data Quality", "Week 4 Quiz Answer Key", "Issues with roles and organizational structures around analytics", "Data privacy issues", "Week 4 Quiz", "Final Course Assignment"], "title": "Real World Analytical Organizations"}], "title": "Introduction to Data Analytics for Business"}, {"course_info": "About this course: In this course, you will analyze and apply essential design principles to your Tableau visualizations. This course assumes you understand the tools within Tableau and have some knowledge of the fundamental concepts of data visualization. You will define and examine the similarities and differences of exploratory and explanatory analysis as well as begin to ask the right questions about what’s needed in a visualization. You will assess how data and design work together, including how to choose the appropriate visual representation for your data, and the difference between effective and ineffective visuals. You will apply effective best practice design principles to your data visualizations and be able to illustrate examples of strategic use of contrast to highlight important elements. You will evaluate pre-attentive attributes and why they are important in visualizations. You will exam the importance of using the \"right\" amount of color and in the right place and be able to apply design principles to de-clutter your data visualization.", "level": "Beginner", "package_name": "Data Visualization with Tableau Specialization ", "created_by": "University of California, Davis", "package_num": "2", "teach_by": [{"name": "Govind Acharya", "department": "Budget and Institutional Analysis"}, {"name": "Hunter Whitney", "department": "Design Strategy and Data Visualization"}], "target_audience": "Who is this class for: This course is primarily aimed at individuals with some fundamental data visualization knowledge, who are familiar with the basics of Tableau, and want to begin applying key design principles to their visualizations. We consider this course still meant for the beginner who wants to gain experience in presenting data visually, but also it is appropriate for those who can analyze data and want to gain more exposure to visualization design concepts.", "rating": "4.4", "week_data": [{"description": "Welcome to this first module where we are going to start you off with background information about how the human brain perceives the world and then you will discover effective and ineffective visuals. By the end of this module, you will be able to recognize how the brain relates to visual design. You will know the difference between cognitive versus perceptual design. You will learn the various visualization options offered by Tableau and some of their advantages and disadvantages. You will discuss why how good ethical practices play in designing visualizations. You will also start to examine ineffective visualizations and learn how to improve them.", "video": ["Course Introduction", "The Human Brain and Data Visualization", "Cognitive vs Perceptual Design Distinction", "Tableau Desktop vs. Tableau Public", "Getting Your Tableau Desktop License", "Introduction of Effective and Ineffective Visuals", "Types of Visualizations", "Pros and Cons of Pie Charts", "Examples of the Types of Visualizations in Tableau", "Practicing Good Ethics in Data Visualization", "Ineffective Visuals and How to Improve Them", "Ineffective Visuals (Activity)", "Module 1 Quiz"], "title": "Getting Started in Effective and Ineffective Visuals"}, {"description": "Welcome to this second module. This module will explore specific data visualization concepts that apply the concepts you learned about how the human brain works from the last module. In this module, you will be able to define cognitive load and what clutter means from a visualization perspective. You will be able to visually illustrate the principles of visual perception and use contrast to enhance your visualizations. You will be able to define and use pre-attentive attributes like color to make effective visualizations.", "video": ["Visual Perception and Cognitive Principles Introduction", "Cognitive Load and Clutter", "Clutter and Clarity", "Principles of Visual Perception", "Gestalt Principle (Activity)", "Strategic Use of Contrast", "Pre-Attentive Attributes of Visualizations", "Color as a Pre-Attentive Attribute", "De-Cluttering Exercise", "Decluttering a Visual (Activity)", "Module 2 Quiz"], "title": "Visual Perception and Cognitive Load"}, {"description": "In this module, we revisit some of the concepts introduced from the previous module. You will be able to apply Gestalt Principles and leverage pre-attentive attributes in your visualizations. You will examine the role of accessibility and aesthetics play in your creations. Also, you will be able to define the ideas of exploratory and explanatory analysis and be able to normalize your data and identify outliers. Finally, you will be introduced to a challenging concept and construct a control chart to set you up to perform more advanced exploratory analysis.", "video": ["Design Best Practices Introduction", "Gestalt Principle: Proximity", "Leveraging Pre-Attentive Attributes", "Accessible Visualizations", "Missed Opportunities and Graphical Fails", "Aesthetics", "The Challenger: An Information Disaster", "Design and Exploratory Analysis Introduction", "What is Exploratory and Explanatory Analysis?", "Case Study: Anscombe's Quartet", "Identifying Outliers", "Explanation of Outliers", "Constructing a Control Chart", "Control Charts", "Module 3 Quiz"], "title": "Design Best Practices and Exploratory Analysis"}, {"description": "Making sense of large, multi-dimensional data sets can be a challenge for anyone. Your task as a designer is to make good decisions about encoding, arranging, and presenting data to reveal meaningful patterns and stories for your audiences. After completing this module, you will be able to design your visualizations for a target audience and with purpose. You will be able to identify the connection to between data, relationships and good visual design. You will implement additional design tools and tips into your visualizations. \n", "video": ["Design For Understanding Introduction", "Know Your Audience(s)", "Design For Purpose", "Data, Relationships, and Design", "Static Versus Interactive Visualizations", "Multiple, Connected View", "Language, Labeling, and Scales", "Visual Lies and Cognitive Bias", "Blogs and Articles About Data Visualization", "Selected Books About Data Visualization, Data Analysis, and Perception/Cognition", "Final Thoughts", "Reflection", "Designing a Visualization for Your Manager", "Module 4 Quiz"], "title": "Design for Understanding"}], "title": "Essential Design Principles for Tableau"}, {"course_info": "About this course: This course builds on the theory and foundations of marketing analytics and focuses on practical application by demystifying the use of data in marketing and helping you realize the power of visualizing data with artful use of numbers found in the digital space.\n\nThis course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.", "level": null, "package_name": "Digital Marketing Specialization ", "created_by": "University of Illinois at Urbana-Champaign", "package_num": "3", "teach_by": [{"name": "Kevin Hartman", "department": "College of Business"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "In this module, you will become familiar with the course, your instructor, your classmates, and our learning environment. The orientation also helps you obtain the technical skills required for the course. Lesson 1 will lay out the Marketing Analysis Process (MAP), a step-by-step procedure for conducting thorough and insightful digital data analyses. Each step of MAP will be introduced with key activities.", "video": ["Welcome to Marketing Analytics in Practice", "About Prof Kevin Hartman", "Syllabus", "About the Discussion Forums", "About the Rubric for Peer Assessment", "Glossary", "Brand Descriptions", "Social Media", "Getting to Know Your Classmates", "Orientation Quiz", "Module 1 Overview", "Module 1 Readings", "Lesson 1: Following the MAP, Part 1", "Lesson 1: Following the MAP, Part 2", "Lesson 1 Practice Quiz", "Module 1 Discussion", "Module 1 Quiz", "Module 1 Peer Review"], "title": "Course Overview and Marketing Analytics Process"}, {"description": "In this module, you will learn the steps of the Marketing Analytics Process, unstructured data, three primary methods for collecting web-based data and tools analysis can use to collect, store, and analyze data. As a result of these activities, you will learn extensively about planning for and collecting data for your analysis projects.", "video": ["Module 2 Overview", "Module 2 Readings", "Lesson 2: Preparing for the Analysis Journey, Part 1", "Lesson 2: Preparing for the Analysis Journey, Part 2", "Lesson 2 Practice Quiz", "Lesson 3: Data Collection Part I: Unstructured Data", "Comments by Prof Rhiannon Clifton", "Lesson 3 Practice Quiz", "Module 2 Discussion", "Module 2 Quiz", "Module 2 Peer Review"], "title": "Data Collection"}, {"description": "In this module, you will learn about data collection, data review, data biases, and data management tools. As a result of this module’s activities, you will have an in-depth understanding of structured data and know how to ensure your data is error-free and ready for analysis.", "video": ["Module 3 Overview", "Module 3 Readings", "Lesson 4: Data Collection Part II: Structured Data", "Lesson 4 Practice Quiz", "Lesson 5: Data Management in Practice, Part 1", "Lesson 5: Data Management in Practice, Part 2", "Lesson 5 Practice Quiz", "Module 3 Discussion", "Module 3 Quiz", "Module 3 Peer Review"], "title": "Data Analysis"}, {"description": "In this module, you will learn how to coax insights from data that can be used to direct marketing decisions and the effective communication of data insights to stakeholders. As a result of this module’s activities, you will gain hands-on experience in data analysis and learn to make more memorable presentations and data visualizations.", "video": ["Module 4 Overview", "Module 4 Readings", "Lesson 6: Insights from Digital Data Pt.1", "Lesson 6: Insights from Digital Data Pt.2", "Lesson 6 Practice Quiz", "Lesson 7: Pictures You See with Your Brain and Feel with Your Heart, Part 1", "Lesson 7: Pictures You See with Your Brain and Feel with Your Heart, Part 2", "Lesson 7 Practice Quiz", "Module 4 Discussion", "Module 4 Quiz", "Module 4 Peer Review"], "title": "Data Visualization"}], "title": "Digital Analytics for Marketing Professionals: Marketing Analytics in Practice"}, {"course_info": "About this course: Data analysis has replaced data acquisition as the bottleneck to evidence-based decision making --- we are drowning in it.  Extracting knowledge from large, heterogeneous, and noisy datasets requires not only powerful computing resources, but the programming abstractions to use them effectively.  The abstractions that emerged in the last decade blend ideas from parallel databases, distributed systems, and programming languages to create a new class of scalable data analytics platforms that form the foundation for data science at realistic scales.\n\nIn this course, you will learn the landscape of relevant systems, the principles on which they rely, their tradeoffs, and how to evaluate their utility against your requirements. You will learn how practical systems were derived from the frontier of research in computer science and what systems are coming on the horizon.   Cloud computing, SQL and NoSQL databases, MapReduce and the ecosystem it spawned, Spark and its contemporaries, and specialized systems for graphs and arrays will be covered.\n\nYou will also learn the history and context of data science, the skills, challenges, and methodologies the term implies, and how to structure a data science project.  At the end of this course, you will be able to:\n\nLearning Goals: \n1. Describe common patterns, challenges, and approaches associated with data science projects, and what makes them different from projects in related fields.\n2. Identify and use the programming models associated with scalable data manipulation, including relational algebra, mapreduce, and other data flow models.\n3. Use database technology adapted for large-scale analytics, including the concepts driving parallel databases, parallel query processing, and in-database analytics\n4. Evaluate key-value stores and NoSQL systems, describe their tradeoffs with comparable systems, the details of important examples in the space, and future trends.\n5. “Think” in MapReduce to effectively write algorithms for systems including Hadoop and Spark.  You will understand their limitations, design details, their relationship to databases, and their associated ecosystem of algorithms, extensions, and languages.\nwrite programs in Spark\n6. Describe the landscape of specialized Big Data systems for graphs, arrays, and streams", "level": null, "package_name": "Data Science at Scale Specialization ", "created_by": "University of Washington", "package_num": "1", "teach_by": [{"name": "Bill Howe", "department": "Scalable Data Analytics"}], "target_audience": null, "rating": "4.3", "week_data": [{"description": "Understand the terminology and recurring principles associated with data science, and understand the structure of data science projects and emerging methodologies to approach them.    Why does this emerging field exist?  How does it relate to other fields?  How does this course distinguish itself?  What do data science projects look like, and how should they be approached?  What are some examples of data science projects?  ", "video": ["Appetite Whetting: Politics", "Appetite Whetting: Extreme Weather", "Appetite Whetting: Digital Humanities", "Appetite Whetting: Bibliometrics", "Appetite Whetting: Food, Music, Public Health", "Appetite Whetting: Public Health cont'd, Earthquakes, Legal", "Characterizing Data Science", "Characterizing Data Science, cont'd", "Distinguishing Data Science from Related Topics", "Four Dimensions of Data Science", "Tools vs. Abstractions", "Desktop Scale vs. Cloud Scale", "Hackers vs. Analysts", "Structs vs. Stats", "Structs vs. Stats cont'd", "A Fourth Paradigm of Science", "Data-Intensive Science Examples", "Big Data and the 3 Vs", "Big Data Definitions", "Big Data Sources", "Supplementary: Three-Course Reading List", "Supplementary: Resources for Learning Python", "Course Logistics", "Supplementary: Class Virtual Machine", "Supplementary: Github Instructions", "Twitter Assignment: Getting Started", "Twitter Sentiment Analysis"], "title": "Data Science Context and Concepts"}, {"description": "Relational Databases are the workhouse of large-scale data management.  Although originally motivated by problems in enterprise operations, they have proven remarkably capable for analytics as well.  But most importantly, the principles underlying relational databases are universal in managing, manipulating, and analyzing data at scale.  Even as the landscape of large-scale data systems has expanded dramatically in the last decade, relational models and languages have remained a unifying concept.  For working with large-scale data, there is no more important programming model to learn.", "video": ["Data Models, Terminology", "From Data Models to Databases", "Pre-Relational Databases", "Motivating Relational Databases", "Relational Databases: Key Ideas", "Algebraic Optimization Overview", "Relational Algebra Overview", "Relational Algebra Operators: Union, Difference, Selection", "Relational Algebra Operators: Projection, Cross Product", "Relational Algebra Operators: Cross Product cont'd, Join", "Relational Algebra Operators: Outer Join", "Relational Algebra Operators: Theta-Join", "From SQL to RA", "Thinking in RA: Logical Query Plans", "Practical SQL: Binning Timeseries", "Practical SQL: Genomic Intervals", "User-Defined Functions", "Support for User-Defined Functions", "Optimization: Physical Query Plans", "Optimization: Choosing Physical Plans", "Declarative Languages", "Declarative Languages: More Examples", "Views: Logical Data Independence", "Indexes", "SQL for Data Science Assignment"], "title": "Relational Databases and the Relational Algebra"}, {"description": "The MapReduce programming model (as distinct from its implementations) was proposed as a simplifying abstraction for parallel manipulation of massive datasets, and remains an important concept to know when using and evaluating modern big data platforms.  ", "video": ["What Does Scalable Mean?", "A Sketch of Algorithmic Complexity", "A Sketch of Data-Parallel Algorithms", "\"Pleasingly Parallel\" Algorithms", "More General Distributed Algorithms", "MapReduce Abstraction", "MapReduce Data Model", "Map and Reduce Functions", "MapReduce Simple Example", "MapReduce Simple Example cont'd", "MapReduce Example: Word Length Histogram", "MapReduce Examples: Inverted Index, Join", "Relational Join: Map Phase", "Relational Join: Reduce Phase", "Simple Social Network Analysis: Counting Friends", "Matrix Multiply Overview", "Matrix Multiply Illustrated", "Shared Nothing Computing", "MapReduce Implementation", "MapReduce Phases", "A Design Space for Large-Scale Data Systems", "Parallel and Distributed Query Processing", "Teradata Example, MR Extensions", "RDBMS vs. MapReduce: Features", "RDBMS vs. Hadoop: Grep", "RDBMS vs. Hadoop: Select, Aggregate, Join", "Thinking in MapReduce"], "title": "MapReduce and Parallel Dataflow Programming"}, {"description": "NoSQL systems are purely about scale rather than analytics, and are arguably less relevant for the practicing data scientist.  However, they occupy an important place in many practical big data platform architectures, and data scientists need to understand their limitations and strengths to use them effectively.", "video": ["NoSQL Context and Roadmap", "NoSQL Roundup", "Relaxing Consistency Guarantees", "Two-Phase Commit and Consensus Protocols", "Eventual Consistency", "CAP Theorem", "Types of NoSQL Systems", "ACID, Major Impact Systems", "Memcached: Consistent Hashing", "Consistent Hashing, cont'd", "DynamoDB: Vector Clocks", "Vector Clocks, cont'd", "CouchDB Overview", "CouchB Views", "BigTable Overview", "BigTable Implementation", "HBase, Megastore", "Spanner", "Spanner cont'd, Google Systems", "MapReduce-based Systems", "Bringing Back Joins", "NoSQL Rebuttal", "Almost SQL: Pig", "Pig Architecture and Performance", "Data Model", "Load, Filter, Group", "Group, Distinct, Foreach, Flatten", "CoGroup, Join", "Join Algorithms", "Skew", "Other Commands", "Evaluation Walkthrough", "Review", "Context", "Spark Examples", "RDDs, Benefits"], "title": "NoSQL: Systems and Concepts"}, {"description": "Graph-structured data are increasingly common in data science contexts due to their ubiquity in modeling the communication between entities: people (social networks), computers (Internet communication), cities and countries (transportation networks), or corporations (financial transactions).  Learn the common algorithms for extracting information from graph data and how to scale them up. ", "video": ["Graph Overview", "Structural Analysis", "Degree Histograms, Structure of the Web", "Connectivity and Centrality", "PageRank", "PageRank in more Detail", "Traversal Tasks: Spanning Trees and Circuits", "Traversal Tasks: Maximum Flow", "Pattern Matching", "Querying Edge Tables", "Relational Algebra and Datalog for Graphs", "Querying Hybrid Graph/Relational Data", "Graph Query Example: NSA", "Graph Query Example: Recursion", "Evaluation of Recursive Programs", "Recursive Queries in MapReduce", "The End-Game Problem", "Representation: Edge Table, Adjacency List", "Representation: Adjacency Matrix", "PageRank in MapReduce", "PageRank in Pregel"], "title": "Graph Analytics"}], "title": "Data Manipulation at Scale: Systems and Algorithms"}, {"course_info": "About this course: This course covers advanced topics in R programming that are necessary for developing powerful, robust, and reusable data science tools. Topics covered include functional programming in R, robust error handling, object oriented programming, profiling and benchmarking, debugging, and proper design of functions. Upon completing this course you will be able to identify and abstract common data analysis tasks and to encapsulate them in user-facing functions. Because every data science environment encounters unique data challenges, there is always a need to develop custom software specific to your organization’s mission. You will also be able to define new data types in R and to develop a universe of functionality specific to those data types to enable cleaner execution of data science tasks and stronger reusability within a team.", "level": "Intermediate", "package_name": "Mastering Software Development in R Specialization ", "created_by": "Johns Hopkins University", "package_num": "2", "teach_by": [{"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Brooke Anderson", "department": "Colorado State University"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "This course covers advanced topics in R programming that are necessary for developing powerful, robust, and reusable data science tools. Topics covered include functional programming in R, robust error handling, object oriented programming, profiling and benchmarking, debugging, and proper design of functions. Upon completing this course you will be able to identify and abstract common data analysis tasks and to encapsulate them in user-facing functions. Because every data science environment encounters unique data challenges, there is always a need to develop custom software specific to your organization’s mission. You will also be able to define new data types in R and to develop a universe of functionality specific to those data types to enable cleaner execution of data science tasks and stronger reusability within a team.", "video": ["Welcome to Advanced R Programming", "Syllabus", "Course Textbook: Mastering Software Development in R", "swirl Assignments"], "title": "Welcome to Advanced R Programming"}, {"description": "This module begins with control structures in R for controlling the logical flow of an R program. We then move on to functions, their role in R programming, and some guidelines for writing good functions.", "video": ["Control Structures Overview", "if-else", "for Loops", "Nested for loops", "next, break", "Summary", "Functions Overview", "Code", "Function interface", "Default values", "Re-factoring code", "Dependency Checking", "Vectorization", "Argument Checking", "R package", "When Should I Write a Function?", "Summary", "Swirl Lesson"], "title": "Functions"}, {"description": "Functional programming is a key aspect of R and is one of R's differentiating factors as a data analysis language. Understanding the concepts of functional programming will help you to become a better data science software developer. In addition, we cover error and exception handling in R for writing robust code.", "video": ["What is Functional Programming?", "Core Functional Programming Functions", "Map", "Reduce", "Search", "Filter", "Compose", "Partial Application", "Side Effects", "Recursion", "Summary", "Expressions", "Environments", "Execution Environments", "What is an error?", "Generating Errors", "When to generate errors or warnings", "How should errors be handled?", "Summary", "Swirl Lesson"], "title": "Functional Programming"}, {"description": "Debugging tools are useful for analyzing your code when it exhibits unexpected behavior. We go through the various debugging tools in R and how they can be used to identify problems in code. Profiling tools allow you to see where your code spends its time and to optimize your code for maximum efficiency.", "video": ["Debugging Overview", "traceback()", "Browsing a Function Environment", "Tracing Functions", "Using debug() and debugonce()", "recover()", "Final Thoughts on Debugging", "Summary", "Profiling Overview", "microbenchmark", "profvis", "Find out more", "Summary", "Non-standard evaluation", "Summary", "Debugging and Profiling"], "title": "Debugging and Profiling"}, {"description": "Object oriented programming allows you to define custom data types or classes and a set of functions for handling that data type in a way that you define. R has a three different methods for implementing object oriented programming and we will cover them in this section.", "video": ["OOP Overview", "Object Oriented Principles", "S3", "S4", "Reference Classes", "Summary", "Overview", "Reuse existing data structures", "Compose simple functions with the pipe", "Embrace functional programming", "Design for humans", "Functional and Object-Oriented Programming"], "title": "Object-Oriented Programming"}], "title": "Advanced R Programming"}, {"course_info": "About this course: Process mining is the missing link between model-based process analysis and data-oriented analysis techniques. Through concrete data sets and easy to use software the course provides data science knowledge that can be applied directly to analyze and improve processes in a variety of domains.\n\nData science is the profession of the future, because organizations that are unable to use (big) data in a smart way will not survive. It is not sufficient to focus on data storage and data analysis. The data scientist also needs to relate data to process analysis. Process mining bridges the gap between traditional model-based process analysis (e.g., simulation and other business process management techniques) and data-centric analysis techniques such as machine learning and data mining. Process mining seeks the confrontation between event data (i.e., observed behavior) and process models (hand-made or discovered automatically). This technology has become available only recently, but it can be applied to any type of operational processes (organizations and systems). Example applications include: analyzing treatment processes in hospitals, improving customer service processes in a multinational, understanding the browsing behavior of customers using booking site, analyzing failures of a baggage handling system, and improving the user interface of an X-ray machine. All of these applications have in common that dynamic behavior needs to be related to process models. Hence, we refer to this as \"data science in action\".\n\nThe course explains the key analysis techniques in process mining. Participants will learn various process discovery algorithms. These can be used to automatically learn process models from raw event data. Various other process analysis techniques that use event data will be presented. Moreover, the course will provide easy-to-use software, real-life data sets, and practical skills to directly apply the theory in a variety of application domains.\n\nThis course starts with an overview of approaches and technologies that use event data to support decision making and business process (re)design. Then the course focuses on process mining as a bridge between data mining and business process modeling. The course is at an introductory level with various practical assignments.\n\nThe course covers the three main types of process mining.\n\n1. The first type of process mining is discovery. A discovery technique takes an event log and produces a process model without using any a-priori information. An example is the Alpha-algorithm that takes an event log and produces a process model (a Petri net) explaining the behavior recorded in the log.\n\n2. The second type of process mining is conformance. Here, an existing process model is compared with an event log of the same process. Conformance checking can be used to check if reality, as recorded in the log, conforms to the model and vice versa.\n\n3. The third type of process mining is enhancement. Here, the idea is to extend or improve an existing process model using information about the actual process recorded in some event log. Whereas conformance checking measures the alignment between model and reality, this third type of process mining aims at changing or extending the a-priori model. An example is the extension of a process model with performance information, e.g., showing bottlenecks. Process mining techniques can be used in an offline, but also online setting. The latter is known as operational support. An example is the detection of non-conformance at the moment the deviation actually takes place. Another example is time prediction for running cases, i.e., given a partially executed case the remaining processing time is estimated based on historic information of similar cases.\n\nProcess mining provides not only a bridge between data mining and business process management; it also helps to address the classical divide between \"business\" and \"IT\". Evidence-based business process management based on process mining helps to create a common ground for business process improvement and information systems development.\n\nThe course uses many examples using real-life event logs to illustrate the concepts and algorithms. After taking this course, one is able to run process mining projects and have a good understanding of the Business Process Intelligence field.\n\nAfter taking this course you should:\n- have a good understanding of Business Process Intelligence techniques (in particular process mining),\n- understand the role of Big Data in today’s society,\n- be able to relate process mining techniques to other analysis techniques such as simulation, business intelligence, data mining, machine learning, and verification,\n- be able to apply basic process discovery techniques to learn a process model from an event log (both manually and using tools),\n- be able to apply basic conformance checking techniques to compare event logs and process models (both manually and using tools),\n- be able to extend a process model with information extracted from the event log (e.g., show bottlenecks),\n- have a good understanding of the data needed to start a process mining project,\n- be able to characterize the questions that can be answered based on such event data,\n- explain how process mining can also be used for operational support (prediction and recommendation), and\n- be able to conduct process mining projects in a structured manner.", "level": "Intermediate", "package_name": null, "created_by": "Eindhoven University of Technology", "package_num": null, "teach_by": [{"name": "Wil van der Aalst", "department": "Department of Mathematics & Computer Science"}], "target_audience": "Who is this class for: This course is aimed at both students and professionals.\n\nA basic understanding of logic, sets, and statistics (at the undergraduate level) is assumed. Basic computer skills are required to use the software provided with the course (but no programming experience is needed). Participants are also expected to have an interest in process modeling and data mining but no specific prior knowledge is assumed as these concepts are introduced in the course.", "rating": "4.7", "week_data": [{"description": "This first module contains general course information (syllabus, grading information) as well as the first lectures introducing data mining and process mining.", "video": ["Welcome to Process Mining: Data Science in Action", "Course Background and Practical Information", "The Forum is your (Extended) Classroom", "Process Mining: Data Science in Action Getting Started!", "1.1: Data Science and Big Data", "1.2: Different Types of Process Mining", "1.3: How Process Mining Relates to Data Mining", "1.4: Learning Decision Trees", "1.5: Applying Decision Trees", "1.6: Association Rule Learning", "[Extra] The data used in the lectures", "How is Process Mining Different from Data Mining?", "1.7: Cluster Analysis", "1.8: Evaluating Mining Results", "Quick Note Regarding Quizzes in this Course", "Process Mining Software", "Introducing Fluxicon & Disco", "Real-life Process Mining Session", "Real Life Session 01: The Demo Scenario (7 min.)", "Real Life Session 02: Process Discovery and Simplification (11 min.)", "Real Life Session 03: Statistics, Cases and Variants (8 min.)", "Real Life Session 04: Bottleneck Analysis (7 min.)", "Real Life Session 05: Compliance Analysis (6 min.)", "Real Life Session 06: Tip 1 - Keep Copies of your Analyses (4 min.)", "Real Life Session 07: Tip 2 - Take Different Views on your Process (7 min.)", "Real Life Session 08: Tip 3 - Exporting Results (4 min.)", "Real-life Process Mining Session Quiz (Not for points)", "Quiz 1"], "title": "Introduction and Data Mining"}, {"description": "In this module we introduce process models and the key feature of process mining: discovering process models from event data.", "video": ["Using Event Data to Tear Down the Towers of Babel in Process Management", "2.1: Event Logs and Process Models", "2.2: Petri Nets (1/2)", "2.3: Petri Nets (2/2)", "2.4: Transition Systems and Petri Net Properties", "2.5: Workflow Nets and Soundness", "2.6: Alpha Algorithm: A Process Discovery Algorithm", "2.7: Alpha Algorithm: Limitations", "2.8: Introducing ProM and Disco", "Quiz 2", "Tool Quiz"], "title": "Process Models and Process Discovery"}, {"description": "Now that you know the basics of process mining, it is time to dive a little bit deeper and show you other ways of discovering a process model from event data.", "video": ["3.1: Four Quality Criteria For Process Discovery", "3.2: On The Representational Bias of Process Mining", "3.3: Business Process Model and Notation (BPMN)", "Process Mining in the Large: Smart Data Scientists Are More Important Than Big Computers!!", "3.4: Dependency Graphs and Causal Nets", "3.5: Learning Dependency Graphs", "3.6: Learning Causal nets and Annotating Them", "3.7: Learning Transition Systems", "3.8: Using Regions to Discover Concurrency", "Quiz 3"], "title": "Different Types of Process Models"}, {"description": "In this module we conclude process discovery by discussing alternative approaches. We also introduce how to check the conformance of the event data and the process model.", "video": ["4.1: Two-Phase Process Discovery And Its Limitations", "4.2: Alternative Process Discovery Techniques", "Conformance Checking: Positive and Negative Deviants", "4.3: Introduction to Conformance Checking", "4.4: Conformance Checking Using Causal Footprints", "4.5: Conformance Checking Using Token-Based Replay", "4.6: Token Based Replay: Some Examples", "4.7: Aligning Observed and Modeled Behavior", "4.8: Exploring Event Data", "Quiz 4", "Applying Process Mining on Real Data"], "title": "Process Discovery Techniques and Conformance Checking"}, {"description": "In this module we focus on enriching process models. We can for instance add the data aspect to process models, show bottlenecks on the process model and analyse the social aspects of the process.", "video": ["5.1: About the Last Two Weeks of This Course", "5.2: Mining Decision Points", "5.3: Discovering Data Aware Petri Nets", "Holistic Process Mining: Integrating Different Perspectives", "5.4: Mining Bottlenecks", "5.5: Mining Social Networks", "5.6: Organizational Mining", "5.7: Combining Different Perspectives", "5.8: Comparative Process Mining Using Process Cubes", "5.9: Refined Process Mining Framework", "Quiz 5"], "title": "Enrichment of Process Models"}, {"description": "In this final module we discuss how process mining can be applied on running processes. We also address how to get the (right) event data, process mining software, and how to get from data to results.", "video": ["6.1: Operational Support: Detect, Predict and Recommend", "Process models are like maps:  Which one is best depends on the questions that need to be answered!", "6.2: Getting the Right Event Data", "6.3: Guidelines for Logging", "6.4: Process Mining Software", "Overview: Process Mining Software", "Process Mining Software", "6.5: How to Conduct a Process Mining Project", "6.6: Mining Lasagna Processes", "6.7: Mining Spaghetti Processes", "6.8: Process Models as Maps", "6.9: Data Science in Action", "Quiz 6", "Final Quiz"], "title": "Operational Support and Conclusion"}], "title": "Process Mining: Data science in Action"}, {"course_info": "About this course: Data science is a team sport. As a data science executive it is your job to recruit, organize, and manage the team to success. In this one-week course, we will cover how you can find the right people to fill out your data science team, how to organize them to give them the best chance to feel empowered and successful, and how to manage your team as it grows. \n\nThis is a focused course designed to rapidly get you up to speed on the process of building and managing a data science team. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward.\n\nAfter completing this course you will know.\n\n1. The different roles in the data science team including data scientist and data engineer\n2. How the data science team relates to other teams in an organization\n3. What are the expected qualifications of different data science team members\n4. Relevant questions for interviewing data scientists\n5. How to manage the onboarding process for the team\n6. How to guide data science teams to success\n7. How to encourage and empower data science teams\n\nCommitment: 1 week of study, 4-6 hours\n\nCourse cover image by JaredZammit. Creative Commons BY-SA. https://flic.kr/p/5vuWZz", "level": null, "package_name": "Executive Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "2", "teach_by": [{"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "Welcome to Building a Data Science Team! This course is one module, intended to be taken in one week.  the course works best if you follow along with the material in the order it is presented. Each lecture consists of videos and reading materials and every lecture has a 5 question quiz. You need to get 4 out of 5 or better on the quiz to pass. Overall the quizzes are worth 17% of your grade each, with the exception of the last quiz, which is worth 15%. I'm excited to have you in the class and look forward to your contributions to the learning community. Click Discussions to see forums where you can discuss the course material with fellow students taking the class. Be sure to introduce yourself to everyone in the Meet and Greet forum.If you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center.Good luck as you get started, and I hope you enjoy the course!  -Jeff", "video": ["About your instructor", "The Data Team", "When Do You Need Data Science?", "Lecture materials and related reading", "Data Engineer: Qualifications & Skills", "Data Scientist: Qualifications & Skills", "Data Science Manager: Qualifications & Skills", "Lecture Materials and Related Reading", "Where To Find The Data Team", "Interviewing For Data Science", "Lecture Materials and Related Reading", "Onboarding the Data Science Team", "Managing the Data Science Team", "Evaluating Success of the Team", "Lecture Materials and Related Reading", "Embedded Teams vs. Dedicated Groups", "How Does Data Science Interact with Other Groups", "Empowering Others to Use Data", "Lecture Materials and Related Reading", "Common Interaction Difficulties", "Common Internal Difficulties", "Lecture Materials and Related Reading", "Wrap-Up", "Lecture Materials and Related Reading", "Post-Course Survey", "Defining the data science team", "Data team qualifications", "Assembling the team", "Management strategies", "Working with other teams", "Common difficulties"], "title": "Building a Data Science Team"}], "title": "Building a Data Science Team"}, {"course_info": "About this course: Whether being used to customize advertising to millions of website visitors or streamline inventory ordering at a small restaurant, data is becoming more integral to success. Too often, we’re not sure how use data to find answers to the questions that will make us more successful in what we do. In this course, you will discover what data is and think about what questions you have that can be answered by the data – even if you’ve never thought about data before. Based on existing data, you will learn to develop a research question, describe the variables and their relationships, calculate basic statistics, and present your results clearly. By the end of the course, you will be able to use powerful data analysis tools – either SAS or Python – to manage and visualize your data, including how to deal with missing data, variable groups, and graphs. Throughout the course, you will share your progress with others to gain valuable feedback, while also learning how your peers use data to answer their own questions.", "level": null, "package_name": "Data Analysis and Interpretation Specialization ", "created_by": "Wesleyan University", "package_num": "1", "teach_by": [{"name": "Lisa Dierker", "department": "Psychology"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "We would like to welcome you to Wesleyan University's Data Analysis and Interpretation Specialization. In this session, we will discuss the basics of data analysis. Your task will be to select a data set that you would like to work with and to review available code books that help you develop your own research question. You will also set up a Tumblr blog that will allow you to reflect on these experiences, submit assignments and share your work with others throughout the course. First, you may want to check out the welcome video", "video": ["Welcome Video", "A few quick questions to get started", "Video Lesson - Steps in data analysis", "Video Lesson - What do we mean by data?", "Video Lesson - Datasets and codebooks", "Video Lesson - Developing a research question", "Course codebooks", "Course Data Sets", "Getting Set Up for the Assignments", "Tumblr Instructions", "Troubleshooting Your Tumblr Assignment Blog Link", "Getting Your Research Project Started"], "title": "Selecting a research question"}, {"description": "In this session, we will discuss how to write a basic program that allows you to load a data set and examine frequency distributions. Your task will be to write a program that helps you to explore the variables you have selected for your own research question. You may choose either Python or SAS. Both are made freely available, and we have created a helpful guide to support you in making the decision. Once you have selected your platform, just follow the instructions in the appropriate \"GETTING STARTED....\" file, and then check out this week's video lessons aimed at helping you write and run your first program. You need only view the lessons for one of the statistical platforms (SAS or Python).\n", "video": ["Choosing SAS or Python", "Getting Started with SAS", "Getting Started with Python", "Codebook for Video Examples", "Program for Video Examples", "Uploading Your Own Data to SAS", "SAS Lesson 1 - Defining exploratory data analysis", "SAS Lesson 2 - SAS coding conventions", "SAS Lesson 3 - Running your program and examining frequency distribution", "SAS Lesson 4 - Refining your research question by selecting rows", "Python Lesson 1 - Defining Exploratory Data Analysis", "Python Lesson 2 - Python Coding Conventions", "Python Lesson 3 - Running your program and examining frequency distributions", "Python Lesson 4 - Refining your research question by selecting rows", "Assignment Sample", "Running Your First Program"], "title": "Writing your first program - SAS or Python"}, {"description": "In this session, we will help you to make and implement even more decisions with data. Statisticians often call this task 'data management', while computer scientists like the term 'data munging'. Whatever you call it, it is a vital and ongoing process when working with data. Your task will be to write a program that manages the variables you have selected for your own research question. ", "video": ["Program for Video Examples", "Codebook for Video Examples", "SAS Lesson 1 - Setting aside missing data", "SAS Lesson 2 - Coding in valid data and recoding values", "SAS Lesson 3  - Creating secondary variables", "SAS Lesson 4 - Grouping variables within individual variables", "Python Lesson 1 - Setting aside missing data", "Python Lesson 2  - Coding valid data and recoding values", "Python Lesson 3 - Creating secondary variables", "Python Lesson 4 - Grouping values within individual variables", "Assignment Sample", "Making Data Management Decisions"], "title": "Managing Data"}, {"description": "In this session we will discuss descriptive statistics and get you visualizing your newly data managed variables individually and as graphs showing the relationships between them.  ", "video": ["Graphing Decisions Flowchart", "Programs for Video Examples", "Codebooks for Video Examples", "SAS Lesson 1 - Graphing individual variables", "SAS Lesson 2 - Describing distributions visually", "SAS Lesson 3 - Measures of center and spread", "SAS Lesson 4 - Designing the role each of your variables will play", "SAS Lesson 5 - Graphing decisions: categorical response variables", "SAS Lesson 6 - Graphing decisions: quantitative response variable", "Python Lesson 1 - Graphing individual variables", "Python Lesson 2 - Describing distributions visually", "Python Lesson 3 - Measures of Center and Spread", "Python Lesson 4 - Designing the role each of your variables will play", "Python Lesson 5 - Graphing Decisions: Categorical response variables", "Python Lesson 6 - Graphing decisions: quantitative response variable", "Assignment Sample", "Creating graphs for your data"], "title": "Visualizing Data"}, {"description": "", "video": ["How to Write a Literature Review", "Translation Code", "Acknowledgments"], "title": "Supplemental Materials (All Weeks)"}], "title": "Data Management and Visualization"}, {"course_info": "About this course: Have you ever had the perfect data science experience? The data pull went perfectly. There were no merging errors or missing data. Hypotheses were clearly defined prior to analyses. Randomization was performed for the treatment of interest. The analytic plan was outlined prior to analysis and followed exactly. The conclusions were clear and actionable decisions were obvious. Has that every happened to you? Of course not. Data analysis in real life is messy. How does one manage a team facing real data analyses? In this one-week course, we contrast the ideal with what happens in real life. By contrasting the ideal, you will learn key concepts that will help you manage real life analyses. \n\nThis is a focused course designed to rapidly get you up to speed on doing data science in real life. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward.\n\nAfter completing this course you will know how to:\n\n1, Describe the “perfect” data science experience\n2. Identify strengths and weaknesses in experimental designs\n3. Describe possible pitfalls when pulling / assembling data and learn solutions for managing data pulls.\n4. Challenge statistical modeling assumptions and drive feedback to data analysts\n5. Describe common pitfalls in communicating data analyses\n6. Get a glimpse into a day in the life of a data analysis manager.\n\nThe course will be taught at a conceptual level for active managers of data scientists and statisticians.  Some key concepts being discussed include:\n1. Experimental design, randomization, A/B testing\n2. Causal inference, counterfactuals, \n3. Strategies for managing data quality.\n4. Bias and confounding\n5. Contrasting machine learning versus classical statistical inference\n\nCourse promo:\nhttps://www.youtube.com/watch?v=9BIYmw5wnBI\n\nCourse cover image by Jonathan Gross. Creative Commons BY-ND https://flic.kr/p/q1vudb", "level": null, "package_name": "Executive Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "4", "teach_by": [{"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "This course is one module, intended to be taken in one week. Please do the course roughly in the order presented. Each lecture has reading and videos. Except for the introductory lecture, every lecture has a 5 question quiz; get 4 out of 5 or better on the quiz.", "video": ["Just for fun, course promotional video", "Pre-Course Survey", "Course structure", "Grading", "Data science in the ideal versus real life Part 1", "Data science in the ideal versus real life Part 2", "Examples", "Machine Learning vs. Traditional Statistics Part 1", "Machine Learning vs. Traditional Statistics Part 2", "The data pull is clean", "Managing the Data Pull", "The experiment is carefully designed", "Experimental design and observational analysis", "Causality part 1", "Causality Part 2", "What Can Go Wrong?: Confounding", "The experiment is carefully designed, things to do", "A/B Testing", "Sampling bias and random sampling", "Blocking and adjustment", "Results of analyses are clear", "Multiplicity", "Effect size, significance, & modeling", "Comparison with benchmark effects", "Negative controls", "The decision is obvious", "Non-significance", "Estimation Target is Relevant", "The analysis product is awesome", "Report writing", "Version control", "Post-Course Survey", "The Data Pull is Clean", "The experiment is carefully designed principles", "The experiment is carefully designed, things to do", "Results of analyses are clear", "The Decision is Obvious", "The analysis product is awesome"], "title": "Introduction, the perfect data science experience"}], "title": "Data Science in Real Life"}, {"course_info": "About this course: This 1-week, accelerated course builds upon previous courses in the Data Engineering on Google Cloud Platform specialization. Through a combination of video lectures, demonstrations, and hands-on labs, you'll learn how to create and manage computing clusters to run Hadoop, Spark, Pig and/or Hive jobs on Google Cloud Platform.  You will also learn how to access various cloud storage options from their compute clusters and integrate Google’s machine learning capabilities into their analytics programs.  \n\nIn the hands-on labs, you will create and manage Dataproc Clusters using the Web Console and the CLI, and use cluster to run Spark and Pig jobs. You will then create iPython notebooks that integrate with BigQuery and storage and utilize Spark. Finally, you integrate the machine learning APIs into your data analysis.\n\nPre-requisites\n• Google Cloud Platform Big Data & Machine Learning Fundamentals (or equivalent experience)\n• Some knowledge of Python", "level": "Intermediate", "package_name": "Data Engineering on Google Cloud Platform Specialization ", "created_by": "Google Cloud", "package_num": "2", "teach_by": [{"name": "Google Cloud Training", "department": null}], "target_audience": "Who is this class for: This class is intended for data analysts, data scientists and programmers who want to migrate Hadoop and Spark workloads onto Google Cloud Platform and also learn how leverage Google’s infrastructure and machine learning API’s to enhance the power of big data processing applications.", "rating": "4.4", "week_data": [{"description": "", "video": ["Why Unstructured Data?", "What Data Do Enterprises Analyze?", "Even Google Skipped Unstructured Data", "Considering Counting Problems", "Why Cloud Dataproc?", "Cluster Provisioning Considerations", "Imagine Your Cluster Provisioning", "Dataproc Eases Hadoop Management", "Create a Cluster from the Web", "Cluster Configurations and Preemptible Workers", "Customizing a Dataproc Cluster", "Lab Overview for creating a Dataproc Cluster", "Lab Objectives for creating a Dataproc Cluster", "Codelab – Leveraging Unstructured Data, Part 1", "Lab \"Creating a Dataproc Cluster \" Review", "Creating Custom Machine Types", "Module 1 Quiz"], "title": "Module 1: Introduction to Cloud Dataproc"}, {"description": "", "video": ["Overview of Running Dataproc Jobs", "Why SSH into a Cluster?", "Lab Overview", "Lab Objectives", "Codelab – Leveraging Unstructured Data, Part 2", "Lab Review", "Separation of Storage and Compute", "Problem of Compute and Storage Rigidity: Scenario 1", "Problem of Compute and Storage Rigidity - Scenario 2", "Moving to a Serverless World", "Submitting Jobs with Dataproc and Cloud Shell", "Lab Overview", "Lab Objectives", "Codelab – Leveraging Unstructured Data, Part 3", "Lab Review", "Module 2 Quiz"], "title": "Module 2: Running Dataproc jobs"}, {"description": "", "video": ["Introduction to Leveraging GCP", "Leveraging Google Cloud Platform Pt. 1", "Leveraging Google Cloud Platform Pt. 2", "Dataproc vs. Metadata Server", "Lab Intro", "Lab Objectives", "Codelab – Leveraging Unstructured Data, Part 4", "Lab Review", "Why Process BigQuery Data in Spark?", "BigQuery Support", "Tips for Interacting with BigQuery", "Module 3 Quiz"], "title": "Module 3: Leveraging GCP"}, {"description": "", "video": ["Introduction to Analyzing Unstructured Data", "Infuse Your Business with Machine Learning", "Lab Objectives", "Codelab – Leveraging Unstructured Data, Part 5", "Lab Review", "Module 4 Quiz"], "title": "Module 4: Analyzing Unstructured Data"}], "title": "Leveraging Unstructured Data with Cloud Dataproc on Google Cloud Platform"}, {"course_info": "About this course: Everyday across the world, thousands of businesses are victimized by fraud.  Who commits these bad acts?  Why? And, how? In this course we are going to help you answer the questions: who commits fraud, why and how.  We’ll also help you develop skills for catching them.", "level": null, "package_name": null, "created_by": "West Virginia University", "package_num": null, "teach_by": [{"name": "Dr. Richard Riley, Ph.D", "department": "Accounting"}, {"name": "Dr. Richard Dull, Ph.D", "department": "Accounting"}, {"name": "John Gill, J.D., CFE", "department": "Association of Certified Fraud Examiners "}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "Who are \"accidental\" fraudsters?  Learn who accidental fraudsters are, the basic elements of fraud, and how devastating the costs of fraud are.", "video": ["Syllabus", "Getting the Most from this Course", "The Elements of Fraud", "Why People Commit Fraud", "Fraudster Demographics", "Randy Pierce - The Accidental Fraudster", "Discussion on Bernie Madoff", "Week 1 Quiz"], "title": "The Accidental Fraudster"}, {"description": "What is a \"preditor\" fraudster? How do you protect your organization against a preditor fraudster?  Learn how internal control concepts and other techniques can help you detect and prevent preditor fraudsters.", "video": ["The Predator Fraudster", "Attributes of a Predator", "Internal Control Concepts", "How to Detect and Prevent Financial Statement Fraud - Crazy Eddie", "Discussion on Lance Armstrong", "Week 2 Quiz"], "title": "The Predator Fraudster"}, {"description": "What is \"big data\"?  Learn how data analysis, Benford analysis and other tools can help you identify fraudulent activities.", "video": ["Using Data and Technology for Fraud Detection and Investigation", "Data Analysis Tools", "Benford Analysis", "Availability of Data and Text Analysis", "Making Crime Pay: How to Locate Hidden Assets - Steve Comisar", "Discussion on Edward Snowden", "Week 3 Quiz"], "title": " Big Data, Benford's Law and Financial Analytics"}, {"description": "Money laundering.  This week’s session will introduce you to the objectives and \n\nstages of money laundering as well as the basic techniques used. ", "video": ["Money Laundering", "Money Laundering Basics", "Money Laundering Schemes", "Making Crime Pay: How to Locate Hidden Assets - Money Laundering", "Discussion on combating cross-jurisdictional crime", "Week 4 Quiz"], "title": "Cyber-Crime and Money Laundering: Contemporary Tools and Techniques."}, {"description": "Everything you need to know about whistleblowing.  Learn the importance of \n\nwhistleblowing and the difficulty of being a whistleblower.  ", "video": ["Whistleblowing", "WorldCom Fraud", "The Choice to be a Whistleblower", "ZZZZ Best and Barry Minkow", "Final Thoughts", "Discussion on Cynthia Cooper & Betty Vinson", "Week 5 Quiz", "Final Exam"], "title": "Whistleblowing"}], "title": "Forensic Accounting and Fraud Examination"}, {"course_info": "About this course: We have all heard the phrase “correlation does not equal causation.”  What, then, does equal causation?  This course aims to answer that question and more!  \n\nOver a period of 5 weeks, you will learn how causal effects are defined, what assumptions about your data and models are necessary, and how to implement and interpret some popular statistical methods.  Learners will have the opportunity to apply these methods to example data in R (free statistical software environment).\n\nAt the end of the course, learners should be able to:\n1.  Define causal effects using potential outcomes\n2.  Describe the difference between association and causation\n3.  Express assumptions with causal graphs\n4.  Implement several types of causal inference methods (e.g. matching, instrumental variables, inverse probability of treatment weighting)\n5.  Identify which causal assumptions are necessary for each type of statistical method\n\nSo join us.... and discover for yourself why modern statistical methods for estimating causal effects are indispensable in so many fields of study!", "level": "Intermediate", "package_name": null, "created_by": "University of Pennsylvania", "package_num": null, "teach_by": [{"name": "Jason A. Roy, Ph.D. ", "department": "Department of Biostatistics, Epidemiology, and Informatics"}], "target_audience": "Who is this class for: Familiarity with traditional statistical methods, such as regression models, and basic probability recommended. Familiarity with free statistical environment R recommended.  Learners should successfully download R before starting the course.", "rating": "4.9", "week_data": [{"description": "This module focuses on defining causal effects using potential outcomes. A key distinction is made between setting/manipulating values and conditioning on variables. Key causal identifying assumptions are also introduced.", "video": ["Welcome to \"A Crash Course in Causality\"", "Confusion over causality", "Potential outcomes and counterfactuals", "Hypothetical interventions", "Causal effects", "Practice Quiz", "Causal assumptions", "Stratification", "Practice Quiz", "Incident user and active comparator designs", "Causal effects"], "title": "Welcome and Introduction to Causal Effects"}, {"description": "This module introduces directed acyclic graphs. By understanding various rules about these graphs, learners can identify whether a set of variables is sufficient to control for confounding.", "video": ["Confounding", "Causal graphs", "Relationship between DAGs and probability distributions", "Paths and associations", "Conditional independence (d-separation)", "Practice Quiz", "Confounding revisited", "Backdoor path criterion", "Disjunctive cause criterion", "Identify from DAGs sufficient sets of confounders"], "title": "Confounding and Directed Acyclic Graphs (DAGs)"}, {"description": "An overview of matching methods for estimating causal effects is presented, including matching directly on confounders and matching on the propensity score. The ideas are illustrated with data analysis examples in R.", "video": ["Observational studies", "Overview of matching", "Matching directly on confounders", "Practice Quiz", "Greedy (nearest-neighbor) matching", "Optimal matching", "Assessing balance", "Analyzing data after matching", "Practice Quiz", "Sensitivity analysis", "Data example in R", "Propensity scores", "Propensity score matching", "Propensity score matching in R", "Matching", "Propensity score matching", "Data analysis project - analyze data in R using propensity score matching"], "title": "Matching and Propensity Scores"}, {"description": "Inverse probability of treatment weighting, as a method to estimate causal effects, is introduced. The ideas are illustrated with an IPTW data analysis in R.", "video": ["Intuition for Inverse Probability of Treatment Weighting (IPTW)", "More intuition for IPTW estimation", "Marginal structural models", "IPTW estimation", "Assessing balance", "Practice Quiz", "Distribution of weights", "Remedies for large weights", "Doubly robust estimators", "Data example in R", "IPTW", "Data analysis project - carry out an IPTW causal analysis"], "title": "Inverse Probability of Treatment Weighting (IPTW)"}, {"description": "This module focuses on causal effect estimation using instrumental variables in both randomized trials with non-compliance and in observational studies. The ideas are illustrated with an instrumental variables analysis in R.", "video": ["Introduction to instrumental variables", "Randomized trials with noncompliance", "Compliance classes", "Assumptions", "Practice Quiz", "Causal effect identification and estimation", "IVs in observational studies", "Two stage least squares", "Weak instruments", "Practice Quiz", "IV analysis in R", "Instrumental variables / Causal effects in randomized trials with non-compliance"], "title": "Instrumental Variables Methods"}], "title": "A Crash Course in Causality:  Inferring Causal Effects from Observational Data"}, {"course_info": "About this course: This course will continue the introduction to Python programming that started with Python Programming Essentials.  We'll learn about different data representations, including strings, lists, and tuples, that form the core of all Python programs.  We will also teach you how to access files, which will allow you to store and retrieve data within your programs. These concepts and skills will help you to manipulate data and write more complex Python programs.\n\nBy the end of the course, you will be able to write Python programs that can manipulate data stored in files.  This will extend your Python programming expertise, enabling you to write a wide range of scripts using Python\n\nThis course uses Python 3.  While most Python programs continue to use Python 2, Python 3 is the future of the Python programming language. This course introduces basic desktop Python development environments, allowing you to run Python programs directly on your computer. This choice enables a smooth transition from online development environments.", "level": "Beginner", "package_name": "Introduction to Scripting in Python Specialization ", "created_by": "Rice University", "package_num": "2", "teach_by": [{"name": "Scott Rixner", "department": "Computer Science"}, {"name": "Joe Warren", "department": "Computer Science"}], "target_audience": "Who is this class for: This class is for people who have have the background from our Python Programming Essentials course.", "rating": "4.9", "week_data": [{"description": "This module will teach you about Python's string data type and its capabilities. Strings are used to represent text within programs.", "video": ["Welcome!", "Class Structure", "Python Strings", "Indexing Strings", "Searching Strings", "Slicing Strings", "Formatting Strings", "Answering Common Python Questions", "Practice Exercises for Strings", "Strings"], "title": "Strings"}, {"description": "This module will teach you the basics of Python's list data type. Lists are used to hold a sequence of data within programs.", "video": ["Lists", "List Indexing and Slicing", "Defining and Accessing Lists", "Splitting and Joining Strings", "List Searching", "Iteration over Lists", "Iteration", "Using Thonny", "Using Thonny's Debugger", "Debugging an Example Program", "Practice Exercises for Lists", "Lists"], "title": "Basics of Lists"}, {"description": "This module will dive further into the use of lists. You will learn how about mutating the contents of a list and the implications of doing so.", "video": ["List Mutation", "List Processing Example", "Tuples", "Objects and References", "Understanding List References", "Visualizing Objects and References", "Diagnosing List Reference Issues", "Practice Exercises for List Manipulation", "List Manipulation"], "title": "List Manipulation"}, {"description": "This module will teach you how to access files in Python.", "video": ["Introduction to Files", "Opening and Reading Files", "Reading Files using Iteration", "Writing Files", "Understanding File Systems and Paths", "Working with File Paths", "Practice Project: Updating the CodeSkulptor Docs", "Project Video", "Project Description: File Differences", "OwlTest: Automated Feedback and Assessment", "Project Submission History", "Project: File Differences"], "title": "File Access"}], "title": "Python Data Representations"}, {"course_info": "About this course: In this third course of the specialization, we’ll drill deeper into the tools Tableau offers in the areas of charting, dates, table calculations and mapping. We’ll explore the best choices for charts, based on the type of data you are using. We’ll look at specific types of charts including scatter plots, Gantt charts, histograms, bullet charts and several others, and we’ll address charting guidelines. We’ll define discrete and continuous dates, and examine when to use each one to explain your data.  You’ll learn how to create custom and quick table calculations and how to create parameters. We’ll also introduce mapping and explore how Tableau can use different types of geographic data, how to connect to multiple data sources and how to create custom maps.", "level": "Beginner", "package_name": "Data Visualization with Tableau Specialization ", "created_by": "University of California, Davis", "package_num": "3", "teach_by": [{"name": "Suk S. Brar, M.B.A.", "department": "Blue Shield of California"}], "target_audience": "Who is this class for: This course is primarily aimed at individuals with some fundamental data visualization knowledge, who are familiar with Tableau, and want to increase their knowledge in explanatory analysis. This course is intended for those taking this course as part of the Specialization, and for those who have some experience working with data and data sets.", "rating": "4.4", "week_data": [{"description": "In this module, you will explore the topic of charting in Tableau. By now you should already be well versed in how to change colors, shapes, and sizes of charts, so we are going to practice and demonstrate that skill more. You will be able to explain what the Tableau Tooltip does and when to use it. You will be able to discuss the various guidelines for choosing the right chart for your data. You will also create a chart using Tableau.", "video": ["Visual Analytics with Tableau Introduction", "Save Workbooks to Tableau Public", "Introduction to Charting", "Tableau Charts", "Colors, Shapes, and Sizes", "Dual Line Charts", "Tableau Tooltip", "Charting Guidelines: Bar Charts, Line Graphs, Pie Charts", "Charting Guidelines: Maps, Scatter Plots, Gantt Charts, Bubble Charts", "Charting Guidelines: Histograms, Bullet Charts, Heat Maps and Highlight Tables", "Charting Guidelines: Treemaps and Box-and-Whisker Plots", "Customer Scatterplot", "Module 1: Visual Analytics with Tableau - Charting"], "title": "Getting Started and Charting"}, {"description": "This module highlights the important topic of dates within Tableau. You will be able to differentiate between discrete and continuous dates and when to use each. You will be able to use date hierarchies and use the date field to better customize your charts. You will be able to convert between discrete and continuous dates and know when and why you want to switch from one to the other. You will create dates using calculated fields.", "video": ["Introduction to Dates", "Discrete vs. Continuous Dates", "Date Hierarchies", "Converting Discrete and Continuous Dates", "Tableau Calculated Fields", "Shipping Details", "Module 2: Visual Analytics with Tableau - Dates"], "title": "Dates"}, {"description": "In this module, you will focus on table calculations. You will be able to create new calculated fields to allow you to compare fields, apply aggregations, and more. You will be able use quick table calculations and create new calculated fields. You will be able to customize them and apply filters and parameters to your table calculations. ", "video": ["Introduction to Table Calculations", "Calculated Fields", "Quick Table Calculations", "Custom Table Calculations", "Filters", "Parameters", "Sales Spotlight", "Module 3: Visual Analytics with Tableau - Table Calculations"], "title": "Table Calculations"}, {"description": "In this final module, we will go more in depths about maps within Tableau. You will be able to connect to a different data sources and customize your maps by changing colors, shapes, and sizes. You will be able to custom geocode a map and create Tableau maps with geographic data that is not recognized by Tableau. You will also be able to create dual layer maps and showcase how to overlay maps on top of one another.", "video": ["Introduction to Mapping", "Working with Geographic Data", "Shapes, Colors, and Sizes", "Custom Mapping Techniques", "Custom Geocoding", "Dual Layer Mapping", "Course Summary", "Course Reflection", "Dual Layer Maps", "Module 4: Visual Analytics with Tableau - Mapping"], "title": "Mapping"}], "title": "Visual Analytics with Tableau"}, {"course_info": "About this course: In this course you will be introduced to the basic ideas behind the qualitative research in social science. You will learn about data collection, description, analysis and interpretation in qualitative research. Qualitative research often involves an iterative process. We will focus on the ingredients required for this process: data collection and analysis.\nYou won't learn how to use qualitative methods by just watching video's, so we put much stress on collecting data through observation and interviewing and on analysing and interpreting the collected data in other assignments.\nObviously, the most important concepts in qualitative research will be discussed, just as we will discuss quality criteria, good practices, ethics, writing some methods of analysis, and mixing methods.\nWe hope to take away some prejudice, and enthuse many students for qualitative research.", "level": "Intermediate", "package_name": "Methods and Statistics in Social Sciences Specialization ", "created_by": "University of Amsterdam", "package_num": "2", "teach_by": [{"name": "Gerben Moerman", "department": "Faculty of Social and Behavioural Sciences"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "Welcome to the first week of the course. We start with an introduction, followed by two lessons on the  Philosophy of Qualitative Research.", "video": ["A very warm welcome to Qualitative Research Methods!", "1.0 Introduction", "Course description", "Prerequisite knowledge needed", "Course Structure", "Standing on shoulders of giants", "Team", "On Elephants", "1.1 On Elephants", "1.2 Ontology for social actors", "1.3 Ontology for social researchers", "1.4 Epistemology", "Slides", "References", "How to never emerge from the library again...", "1.5 Hermeneutics", "1.6 Phenomenology", "1.7 Pragmatism", "Slides", "References", "Assignment A: Is Silverman right?"], "title": "Philosophy of Qualitative Research"}, {"description": "In the first module we discussed the philosophy of qualitative research, explaining some basic notions and general philosophical approaches. In this second module we'll discuss observation as an important method within qualitative research. What types of observation are there? How do we observe? And how do we analyse and describe our data? ", "video": ["Background of observation as a method", "2.1 Ethnography", "2.2 Participant Observation", "2.3 Views on Observation", "Slides", "References", "Focus and the assignment", "2.4 Observational Focus", "2.5 Cyclists in Amsterdam", "2.6 Privatizing Public Space", "Slides", "References", "Note taking and Interpreting", "2.7 Field Notes", "2.8 Fieldwork organisation", "2.9 Observation and Interpretation", "Slides", "References", "Peer Review: Assignment B: instructions", "Assignment B: Observing on waiting around the world: Privatising Public Space"], "title": "Observation"}, {"description": "What makes qualitative research 'good' is a rather difficult question. Different criteria are suggested, but within the field of qualitative research there is not much agreement on these criteria. However, there is quite some agreement on what good practices of qualitative research are. In this module we will start in lesson 1 with a  discussion of  good practices of qualitative research.", "video": ["Good practices", "3.1 Flexibility", "3.2 Triangulation", "3.3 Abduction", "3.4 Focus on Details and Context", "3.5 Look for Contradiction", "3.6 Reflexivity", "Slides", "References", "Selecting Cases", "3.7 How Many Cases Are Enough", "3.8 Theoretical Saturation", "Slides", "References", "Criteria for Research Quality", "3.9 Criteria For Research Quality", "Slides", "References", "Practice quiz", "Midterm exam"], "title": "Good Practices & Criteria"}, {"description": "In this module we'll look at what a qualitative interview entails by trying to define it and by discussing different forms of interviewing behaviour. ", "video": ["What is a qualitative interview?", "4.1 Visions of Interviewing", "4.2 Typologies of Interviews", "Slides", "References", "Interviewing Behaviour", "4.3 Rapport in interviewing", "4.4 Powergame or Roleplaying", "Slides", "References", "Probing", "4.5 Probing and prompting", "4.6 Probing tactics", "4.7 Interviewing, a final thought", "Slides", "References", "Assignment C Interviewing about Happiness around the world"], "title": "Qualitative Interviewing"}, {"description": "In previous modules we discussed how you should observe a social situation or conduct a qualitative interview. Now we will focus on what to do with your data, by discussing qualitative analysis. In this module you will try to do a qualitative analysis by interpreting your observed data and try to code it. ", "video": ["Qualitative Analysis", "5.1 Qualitative Analysis", "Slides", "References", "Deductive Approaches", "5.2 Code and Retrieve", "5.3 Framework Analysis", "Slides", "References", "Inductive Approaches", "5.4 Analytic Induction", "5.5 Grounded Theory", "5.6 Versions of Grounded Theory", "Slides", "References", "Optional: Interview with Kathy Charmaz", "Assignment D: Analysis of Privatising Public Space around the world"], "title": "Qualitative Analysis"}, {"description": "In this module I will discuss ideas on writing in qualitative research, I will discuss mixing methods  and talk about the ethical issues you should consider. ", "video": ["Writing", "6.1 Qualitative writing", "Slides", "References", "Mixing", "6.2 Mixing Methods", "6.3 How to mix", "Slides", "References", "Ethics", "6.4 Five Focal Points in Ethics", "6.5 Ethics - Visions & Practices", "6.6 Ethical Boards & Qualitative Research", "Slides", "References", "Fraud", "6.7 Fraud", "Slides", "References", "Assignment E: Writing about Happiness around the world"], "title": "Writing, mixing & ethics"}, {"description": "In this module there's no new material. The only requirement in this module is that you finish up the final peer review assignment.", "video": ["Catch up? Or do some more!"], "title": "Catch up week"}, {"description": "This is the final module, where you can apply everything you've learned up until now in the final exam. Please note that you can only take the final exam once every day, so make sure you are fully prepared to take the test. Please follow the honor code and do not communicate or confer with others taking this exam. Good luck! Once you've taken the exam, please consider doing the other courses in our specialisation track. I hope it was an enjoyable experience. If it was, please consider joining in with the Massive Open Online Research by my colleague Christian Bröer. Thanks for all your hard work, feedback and interpretations, the course team and your fellow learners really appreciate it!", "video": ["Final Exam"], "title": "Exam week"}], "title": "Qualitative Research Methods"}, {"course_info": "About this course: This course is designed to show you how use quantitative models to transform data into better business decisions. You’ll learn both how to use models to facilitate decision-making and also how to structure decision-making for optimum results. Two of Wharton’s most acclaimed professors will show you the step-by-step processes of modeling common business and financial scenarios, so you can significantly improve your ability to structure complex problems and derive useful insights about alternatives. Once you’ve created models of existing realities, possible risks, and alternative scenarios, you can determine the best solution for your business or enterprise, using the decision-making tools and techniques you’ve learned in this course.", "level": null, "package_name": "Business and Financial Modeling Specialization ", "created_by": "University of Pennsylvania", "package_num": "4", "teach_by": [{"name": "Richard Lambert", "department": "Accounting- Wharton School"}, {"name": "Robert W. Holthausen", "department": "Accounting"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "This module was designed to introduce you to the many potential criteria for selecting investment projects, and to explore the most effective of these criteria: Net Present Value (NPV). Through the use of concrete examples, you'll learn the key components of Net Present Value, including the time value of money and the cost of capital, the main utility of NPV, and why it is ultimately more accurate and useful for evaluating projects than other commonly used criteria. By the end of this module, you'll be able to explain why net present value analysis is the appropriate criteria for choosing whether to accept or reject a project, and why other criteria, such as IRR, payback, ROI, etc. may not lead to decisions which maximize value.", "video": ["Course Introduction & Overview", "1.1 Introduction: Criteria for Evaluating Projects", "1.2 Time Value of Money", "1.3 NPV Analysis of Projects", "1.4 Other Evaluation Techniques", "1.5 The Cost of Capital", "PDFs of Lecture Slides", "Excel Spreadsheets: Module 1", "Evaluation Criteria: Module 1 Quiz"], "title": "Evaluation Criteria: Net Present Value"}, {"description": "In this module, you'll learn how to evaluate a project with emphasis on analyzing the incremental after-tax cash flows associated with the project. You'll work through a concrete example using alternative scenarios to test the effectiveness of this method. You'll also learn why only future cash flows are relevant, why to ignore financial costs, include all incidental effects, remember working capital requirements, consider the effect of taxes, forget sunk costs, remember opportunity costs, use expected cash flows, and perform sensitivity analysis.  By the end of this module, you'll be able to evaluate projects more thoroughly and effectively, with emphasis on how to model the change in the company’s after-tax cash flows, so that you can make more profitable decisions.", "video": ["2.1 Introduction and Analyzing Incremental After-Tax Cash Flows - Initial Investment Phase", "2.2 Analyzing Incremental After-Tax Flows - Operating Phase", "2.3 Analyzing Incremental After-Tax Flows - Terminal Phase", "2.4 Example: New Production Machine", "2.5 Key Considerations in Evaluations", "PDFs of Lecture Slides", "How to Evaluate Projects: Module 2 Quiz"], "title": "Evaluating Projects"}, {"description": "This module was designed to give you the opportunity to learn how business activities, transactions and events are translated into financial statements, including balance sheets, income statements, and cash flow statements. You'll also learn how these three statements are linked to each other, and how balance sheets and income statements can help forecast the future cash flow statements. By the end of this module, you'll be able to explain how accounting systems translate business activities into financial terms, and how to use this to better forecast future cash flows, so that you can express your business strategies in these financial terms, and show \"the bottom line\" for your proposed plan of action.", "video": ["3.1 Introduction to Financial Statements", "3.2 Balance Sheets and Income Statements", "3.3 Cash Flow Statements", "PDFs of Lecture Slides", "Financial Statements and Forecasting: Module 3 Quiz"], "title": "Expressing Business Strategies in Financial Terms"}, {"description": "In this module, you'll apply what you’ve been learning to an analysis of a new product venture. You’ll learn how to map out a plan of the business activities, transactions and events that need to happen to implement the new venture, including their timing. You'll also learn how to set up a spreadsheet to help with forecasts, and to re-calculate things automatically as we re-think our plans. You'll see how to forecast out the implied financial statements, and calculate the Net Present Value (NPV). By the end of this module, you'll be able to use spreadsheets to explore different risks a venture may face, and analyze the implications of these scenarios for NPV, so that you can make the most profitable, data-driven decision possible.", "video": ["4.1 Introduction and Speadsheet Setup", "4.2 Forecasting Future Cash Flows", "4.3 NPV and IRR Calculations", "4.4 Formulation and Evaluation of Alternative Scenarios", "4.5 Expanding Beyond the Time Horizon", "Course Conclusion", "PDFs of Lecture Slides", "Excel Spreadsheets: Module 4", "Calculating Value: Module 4 Quiz"], "title": "New Product Value"}], "title": "Decision-Making and Scenarios"}, {"course_info": "About this course: The ability to understand and apply Business Statistics is becoming increasingly important in the industry. A good understanding of Business Statistics is a requirement to make correct and relevant interpretations of data. Lack of knowledge could lead to erroneous decisions which could potentially have negative consequences for a firm. This course is designed to introduce you to Business Statistics. We begin with the notion of descriptive statistics, which is summarizing data using a few numbers. Different categories of descriptive measures are introduced and discussed along with the Excel functions to calculate them. The notion of probability or uncertainty is introduced along with the concept of a sample and population data using relevant business examples. This leads us to various statistical distributions along with their Excel functions which are then used to model or approximate business processes. You get to apply these descriptive measures of data and various statistical distributions using easy-to-follow Excel based examples which are demonstrated throughout the course.\n\nTo successfully complete course assignments, students must have access to Microsoft Excel. \n________________________________________\nWEEK 1\nModule 1: Basic Data Descriptors\nIn this module you will get to understand, calculate and interpret various descriptive or summary measures of data. These descriptive measures summarize and present data using a few numbers. Appropriate Excel functions to do these calculations are introduced and demonstrated.\n\nTopics covered include:\n•\tCategories of descriptive data\n•\tMeasures of central tendency, the mean, median, mode, and their interpretations and calculations\n•\tMeasures of spread-in-data, the range, interquartile-range, standard deviation and variance\n•\tBox plots\n•\tInterpreting the standard deviation measure using the rule-of-thumb and Chebyshev’s theorem\n________________________________________\nWEEK 2\nModule 2: Descriptive Measures of Association, Probability, and Statistical Distributions\nThis module presents the covariance and correlation measures and their respective Excel functions. You get to understand the notion of causation versus correlation. The module then introduces the notion of probability and random variables and starts introducing statistical distributions.\n\nTopics covered include:\n•\tMeasures of association, the covariance and correlation measures; causation versus correlation\n•\tProbability and random variables; discrete versus continuous data\n•\tIntroduction to statistical distributions\n________________________________________\nWEEK 3\nModule 3: The Normal Distribution\nThis module introduces the Normal distribution and the Excel function to calculate probabilities and various outcomes from the distribution. \n\nTopics covered include:\n•\tProbability density function and area under the curve as a measure of probability\n•\tThe Normal distribution (bell curve), NORM.DIST, NORM.INV functions in Excel\n________________________________________\nWEEK 4\nModule 4: Working with Distributions, Normal, Binomial, Poisson\nIn this module, you'll see various applications of the Normal distribution. You will also get introduced to the Binomial and Poisson distributions. The Central Limit Theorem is introduced and explained in the context of understanding sample data versus population data and the link between the two.\n\nTopics covered include:\n•\tVarious applications of the Normal distribution\n•\tThe Binomial and Poisson distributions\n•\tSample versus population data; the Central Limit Theorem", "level": null, "package_name": "Business Statistics and Analysis Specialization ", "created_by": "Rice University", "package_num": "2", "teach_by": [{"name": "Sharad Borle", "department": "Jones Graduate School of Business"}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "", "video": ["Meet the Professor", "Course FAQs", "Pre-Course Survey", "Descriptive Statistics", "Slides, Lesson 1", "Descriptive Statistics", "Descriptive Statistics Continued", "Slides, Lesson 2", "Descriptive Statistics Continued", "Introduction to the Box Plot and Standard Deviation", "Slides, Lesson 3", "Introduction to the Box Plot and the Standard Deviation", "The Standard Deviation \"Rule of Thumb\"", "Slides, Lesson 4", "The Standard Deviation “Rule of Thumb”", "Testing the \"Rule of Thumb\"", "Slides, Lesson 5", "Testing the “Rule of Thumb”", "Chebyshev's Theorem", "Slides, Lesson 6", "Chebyshev’s Theorem", "Week 1 Recap", "Basic Data Descriptors and Data Distributions"], "title": "Basic Data Descriptors"}, {"description": "", "video": ["Covariance", "Slides, Lesson 1", "Covariance", "Correlation", "Slides, Lesson 2", "Correlation", "Causation", "Slides, Lesson 3", "Causation", "Probability and Random variables", "Slides, Lesson 4", "Probability", "Statistical Distributions", "Slides, Lesson 5", "Statistical Distributions", "Week 2 Recap", "Descriptive Measures of Association, Probability, and Data Distributions"], "title": "Descriptive Measures of Association, Probability, and Statistical Distributions"}, {"description": "", "video": ["Probability Density Function and Area Under the Curve", "Slides, Lesson 1", "PDF and PMF", "The Normal Distribution", "Slides, Lesson 2", "The Normal Distribution", "The NORM.DIST Function", "Slides, Lesson 3", "The NORM.DIST Function", "The NORM.DIST Function Continued", "Slides, Lesson 4", "The NORM.DIST Function Continued", "The NORM.INV Function", "Slides, Lesson 5", "The NORM.INV Function", "The Normal Distribution"], "title": "The Normal Distribution"}, {"description": "", "video": ["Applying the Normal Distribution, Standard Normal Distribution", "Slides, Lesson 1", "Applying the Normal Distribution, Standard Distribution", "Population and Sample data", "Slides, Lesson 2", "Population and Sample data", "Central Limit Theorem", "Slides, Lesson 3", "Central Limit Theorem", "The Binomial Distribution", "Slides, Lesson 4", "The Binomial Distribution", "Business Application of the Binomial Distribution", "Slides, Lesson 5", "Business Application of the Binomial Distribution", "Poisson Distribution", "Slides, Lesson 6", "Poisson Distribution", "Weeks 3 and 4 Recap", "End-of-Course Survey", "Working with Distributions (Normal, Binomial, Poisson), Population and Sample Data"], "title": "Working with Distributions (Normal, Binomial, Poisson), Population and Sample Data"}], "title": "Basic Data Descriptors, Statistical Distributions, and Application to Business Decisions"}, {"course_info": "About this course: Leveraging the visualizations you created in the previous course, Visual Analytics with Tableau, you will create dashboards that help you identify the story within your data, and you will discover how to use Storypoints to create a powerful story to leave a lasting impression with your audience.\n\nYou will balance the goals of your stakeholders with the needs of your end-users, and be able to structure and organize your story for maximum impact. Throughout the course you will apply more advanced functions within Tableau, such as hierarchies, actions and parameters to guide user interactions.  For your final project, you will create a compelling narrative to be delivered in a meeting, as a static report, or in an interactive display online.", "level": "Beginner", "package_name": "Data Visualization with Tableau Specialization ", "created_by": "University of California, Davis", "package_num": "4", "teach_by": [{"name": "Govind Acharya", "department": "Budget and Institutional Analysis"}, {"name": "Hunter Whitney", "department": "Design Strategy and Data Visualization"}], "target_audience": "Who is this class for: This course is primarily aimed at individuals with some fundamental data visualization knowledge, who are familiar with Tableau, and want to increase their knowledge in explanatory analysis, applying key design principles and data storytelling. This is the fourth course in a series designed for the beginner who wants to gain experience in presenting data visually. It is also appropriate for those who can analyze data and want to gain more exposure to visualization design concepts.", "rating": "4.6", "week_data": [{"description": "Welcome to the first module of this course! In the following modules, you will learn and work with concepts, tips, and techniques to help you explore data, identify meaningful findings, and then explain them through the power of data visualization and storytelling. In this module, you will be able to determine the who, what, why, and how of the story and discover the importance of planning before you start. You will be able to interview your stakeholders and assess your audience to find the right story in the data. By the end of this module, you will be able to define what a story is and build a basic framework for presenting your story. Let's get started!", "video": ["Course Introduction", "What is a Story?", "Save Workbooks to Tableau Public", "Introduction to Planning and Preproduction for Visualizations", "A Design Checklist of the Who, What, Why, and How", "Start with Your Stakeholders", "Addressing Your Audience and Their Context", "Finding the Story", "Prioritizing, Optimizing, and Designing the Data Story", "Journalistic Data Stories", "Case Study - \"The Tale of 100 Entrepreneurs\" (Part 1)", "Case Study - \"The Tale of 100 Entrepreneurs\" (Part 2)", "Case Study - \"The Tale of 100 Entrepreneurs\" (Part 3)", "Additional Readings", "The Who, What, Why, and How", "Project Brief", "Sylvia and Terrence Personas", "Sample - SuperStore Dataset", "Part 1: Create a Design Checklist", "Module 1"], "title": "Planning and Preproduction: Aligning your Audience, Stakeholders, and Data"}, {"description": "Welcome to Module 2. In this module, you will identify the key metrics that will provide the answers to your business question. You will develop an understanding of the types of ways KPIs can be visualized. You will create calculated fields for KPIs to build a figure that will be used to measure progress in the data.  By the end of this module, you should be able to set thresholds and create alerts to trigger a decision.  We will also discuss the topic of quality and constraints of the data.", "video": ["Introduction to Key Metrics, Indicators, and Decision Triggers", "Key Performance Indicators (KPIs)", "Tableau Calculated Fields and KPIs", "Creating Complex KPIs Using Tableau", "Thresholds and Alerts", "Data Quality", "Part 2: Key Performance Indicators (KPIs)", "Module 2 Quiz"], "title": "Key Metrics, Indicators, and Decision Triggers"}, {"description": "Welcome to Module 3.  In this module, you will go from learning Tableau's six best practices for dashboard design. By the end of this module, you should be able to apply hierarchies, actions, filters, and parameters within Tableau. You will also review five videos associated with the topic and discover how to uncover the story in the data and be able to frame your story.      ", "video": ["Pre-Reading: Tableau Whitepaper", "Best Practices for Dashboard Design", "Dashboard Design Principles 1, 2, & 3", "Dashboard Design Principles 4, 5, & 6", "Hierarchies, Actions, Filters, and Parameters (Parts 1 & 2)", "Hierarchies, Actions, Filters, and Parameters (Part 3)", "Hierarchies, Actions, Filters, and Parameters (Part 4)", "Important Considerations for Creating Dashboards", "Recommended Readings", "Part 3: Narrative Arc & the 3C's", "Creating a Dashboard", "Module 3"], "title": "Dashboard and Storytelling with Data"}, {"description": "Welcome to Module 4. Although this course focuses on Tableau, we will look at a wider range of examples and techniques to help you become a better data storyteller. By the end of this module, you should be able to leverage concepts and techniques designed to help you become a more focused and compelling storyteller with data as the foundation. We will discuss ways to avoid unintentionally creating false narratives with good data. You will also learn about what neuroscience research tells us about stories, audience engagement, and decision drivers. You will use structural story elements to help improve the relatability of the story and explore ways design and textual elements can affect the emotional tone of a story.  Lastly, you will learn how to frame and format the data story based on your design checklist.  Let's get started!              ", "video": ["Pre-Reading: Tableau Whitepaper and Training Video", "Tell the Story of Your Data Introduction", "The Art of Storytelling (Past, Present, and Future)", "Storytelling and the Human Brain", "Bringing Data to Life: Emotions and Data Storytelling", "Examples of Emotions in Data Storytelling", "Emotion Modulators: Color, Language, and Other Design Elements", "Framing and Format", "Framing, Conventions, and Priming", "False Narratives and Data Storytelling", "Telling the Stories within the Story (Part 1)", "Telling the Stories within the Story (Part 2)", "Story Considerations", "Preparation of the Story Points", "Setting Up the Story", "Creating Points in the Story", "Interactive Story", "Create a Data Story in a Static Presentation", "Additional Readings", "Single-Frame Visualization (Optional Activity)", "Course Summary", "Course Reflection", "Part 4: Story Points Presentation", "Module 4"], "title": "Tell the Story of Your Data"}], "title": "Creating Dashboards and Storytelling with Tableau"}, {"course_info": "About this course: This is the second course in the Data Warehousing for Business Intelligence specialization. Ideally, the courses should be taken in sequence.\n\nIn this course, you will learn exciting concepts and skills for designing data warehouses and creating data integration workflows. \nThese are fundamental skills for data warehouse developers and administrators. You will have hands-on experience for data warehouse design and use open source products for manipulating pivot tables and creating data integration workflows.You will also gain conceptual background about maturity models, architectures, multidimensional models, and management practices, providing an organizational perspective about data warehouse development. If you are currently a business or information technology professional and want to become a data warehouse designer or administrator, this course will give you the knowledge and skills to do that. By the end of the course, you will have the design experience, software background, and organizational context that prepares you to succeed with data warehouse development projects.  \n\nIn this course, you will create data warehouse designs and data integration workflows that satisfy the business intelligence needs of organizations. When you’re done with this course, you’ll be able to:\n   * Evaluate an organization for data warehouse maturity and business architecture alignment;\n   * Create a data warehouse design and reflect on alternative design methodologies and design goals;\n   * Create data integration workflows using prominent open source software;\n   * Reflect on the role of change data, refresh constraints, refresh frequency trade-offs, and data quality goals in data integration process design; and\n   * Perform operations on pivot tables to satisfy typical business analysis requests using prominent open source software", "level": null, "package_name": "Data Warehousing for Business Intelligence Specialization ", "created_by": "University of Colorado System", "package_num": "2", "teach_by": [{"name": "Michael Mannino", "department": "Business School, University of Colorado Denver"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "Module 1 introduces the course and covers concepts that provide a context for the remainder of this course. In the first two lessons, you’ll understand the objectives for the course and know what topics and assignments to expect. In the remaining lessons, you will learn about historical reasons for development of data warehouse technology, learning effects, business architectures, maturity models, project management issues, market trends, and employment opportunities. This informational module will ensure that you have the background for success in later modules that emphasize details and hands-on skills.You should also read about the software requirements in the lesson at the end of module 1. I recommend that you try to install the software this week before assignments begin in week 2.", "video": ["Course introduction video lecture", "Course objectives video lecture", "Powerpoint lecture notes for lesson 1", "Course topics and assignments video lecture", "Optional textbook", "Powerpoint lecture notes for lesson 2", "Motivation and characteristics video lecture", "Powerpoint lecture notes for lesson 3", "Learning effects for data warehouse development video lecture", "Powerpoint lecture notes for lesson 4", "Data warehouse architectures and maturity video lecture", "Powerpoint lecture notes for lesson 5", "Applications and market trends video lecture", "Powerpoint lecture notes for lesson 6", "Employment opportunities video lecture", "Powerpoint lecture notes for lesson 7", "Overview of software requirements", "Pivot4J installation", "Pentaho Data Integration installation", "Overview of database software installation", "Oracle installation notes", "Making connections to a local Oracle database", "Optional textbook reading material", "Module 1 quiz"], "title": "Data Warehouse Concepts and Architectures"}, {"description": "Now that you have the informational context for data warehouse development, you’ll start using data warehouse tools! In module 2, you will learn about the multidimensional representation of a data warehouse used by business analysts. You’ll apply what you’ve learned in practice and graded problems using Pivot4J, an open source tool for manipulating pivot tables. At the end of this module, you will have solid background to communicate and assist business analysts who use a multidimensional representation of a data warehouse.", "video": ["Data cube representation video lecture", "Powerpoint lecture notes for lesson 1", "Data cube operators video lecture", "Powerpoint lecture notes for lesson 2", "Overview of Microsoft MDX video lecture", "Powerpoint lecture notes for lesson 3", "Microsoft MDX statements video lecture", "Powerpoint lecture notes for lesson 4", "Overview of Pivot4J video lecture", "Powerpoint lecture notes for lesson 5", "Overview of WebPivotTable video lecture", "Powerpoint lecture notes for lesson 6", "Pivot4J software demonstration video lecture", "Optional textbook reading material", "Pentaho Pivot4J tutorial", "Module 2 quiz", "Assignment for module 2  ", "Quiz for module 2 assignment"], "title": "Multidimensional Data Representation and Manipulation"}, {"description": "This module emphasizes data warehouse design skills. Now that you understand the multidimensional representation used by business analysts, you are ready to learn about data warehouse design using a relational database. In practice, the multidimensional representation used by business analysts must be derived from a data warehouse design using a relational DBMS.You will learn about design patterns, summarizability problems, and design methodologies. You will apply these concepts to mini case studies about data warehouse design. At the end of the module, you will have created data warehouse designs based on data sources and business needs of hypothetical organizations.", "video": ["Relational database concepts for multidimensional data video lecture", "Powerpoint lecture notes for lesson 1", "Table design patterns video lecture", "Powerpoint lecture notes for lesson 2", "Summarizability patterns for dimension tables video lecture", "Powerpoint lecture notes for lesson 3", "Summarizability patterns for dimension-fact relationships video lecture", "Powerpoint lecture notes for lesson 4", "Mini case for data warehouse design video lecture", "Powerpoint lecture notes for lesson 5", "Data warehouse design methodologies video lecture", "Powerpoint lecture notes for lesson 6", "Practice problems for module 3", "Optional textbook reading material", "Module 3 quiz", "Assignment for module 3"], "title": "Data Warehouse Design Practices and Methodologies"}, {"description": "Module 4 extends your background about data warehouse development. After learning about schema design concepts and practices, you are ready to learn about data integration processing to populate and refresh a data warehouse. The informational background in module 4 covers concepts about data sources, data integration processes, and techniques for pattern matching and inexact matching of text. Module 4 provides a context for the software skills that you will learn in module 5. ", "video": ["Concepts of data integration processes video lecture", "Powerpoint lecture notes for lesson 1", "Change data concepts video lecture", "Powerpoint lecture notes for lesson 2", "Data cleaning tasks video lecture", "Powerpoint lecture notes for lesson 3", "Pattern matching with regular expressions video lecture", "Powerpoint lecture notes for lesson 4", "Matching and consolidation video lecture", "Powerpoint lecture notes for lesson 5", "Quasi identifiers and distance functions for entity matching video lecture", "Powerpoint lecture notes for lesson 6", "Optional reading material", "Module 4 quiz"], "title": "Data Integration Concepts, Processes,\u000band Techniques "}, {"description": "Module 5 extends your background about data integration from module 4. Module 5 covers architectures, features, and details about data integration tools to complement the conceptual background in module 4. You will learn about the features of two open source data integration tools, Talend Open Studio and Pentaho Data Integration. You will use Pentaho Data Integration in guided tutorial in preparation for a graded assignment involving Pentaho Data Integration.", "video": ["Architectures and marketplace video lecture", "Powerpoint lecture notes for lesson 1", "Common features of data Integration tools video lecture", "Powerpoint lecture notes for lesson 2", "Talend Open Studio video lecture", "Powerpoint lecture notes for lesson 3", "Pentaho Data Integration video lecture", "Powerpoint lecture notes for lesson 4", "Software video demonstration for Pentaho Data Integration", "Optional reading material", "Guided tutorial for Pentaho Data Integration", "Documents for the module 5 assignment", "Course conclusion video lecture", "Module 5 quiz", "Assignment for module 5", "Quiz for module 5 assignment"], "title": "Architectures, Features, and \u000bDetails of Data Integration Tools"}], "title": "Data Warehouse Concepts, Design, and Data Integration"}, {"course_info": "About this course: Discover the basic concepts of cluster analysis, and then study a set of typical clustering methodologies, algorithms, and applications. This includes partitioning methods such as k-means, hierarchical methods such as BIRCH, and density-based methods such as DBSCAN/OPTICS. Moreover, learn methods for clustering validation and evaluation of clustering quality. Finally, see examples of cluster analysis in applications.", "level": null, "package_name": "Data Mining  Specialization ", "created_by": "University of Illinois at Urbana-Champaign", "package_num": "5", "teach_by": [{"name": "Jiawei Han", "department": "Department of Computer Science"}], "target_audience": null, "rating": "4.2", "week_data": [{"description": "You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course.", "video": ["Course Introduction", "Syllabus", "About the Discussion Forums", "Getting to Know Your Classmates", "Social Media", "Orientation Quiz"], "title": "Course Orientation"}, {"description": "", "video": ["Lesson 1 Overview", "1.1. What is Cluster Analysis", "1.2. Applications of Cluster Analysis", "1.3 Requirements and Challenges", "1.4 A Multi-Dimensional Categorization", "1.5 An Overview of Typical Clustering Methodologies", "1.6 An Overview of Clustering Different Types of Data", "1.7 An Overview of User Insights and Clustering", "Lesson 2 Overview", "2.1 Basic Concepts: Measuring Similarity between Objects", "2.2 Distance on Numeric Data Minkowski Distance", "2.3 Proximity Measure for Symetric vs Asymmetric Binary Variables", "2.4 Distance between Categorical Attributes Ordinal Attributes and Mixed Types", "2.5 Proximity Measure between Two Vectors Cosine Similarity", "2.6 Correlation Measures between Two variables Covariance and Correlation Coefficient", "Lesson 1 Quiz", "Lesson 2 Quiz"], "title": "Module 1"}, {"description": "", "video": ["Lesson 3 Overview", "3.1 Partitioning-Based Clustering Methods", "3.2 K-Means Clustering Method", "3.3 Initialization of K-Means Clustering", "3.4 The K-Medoids Clustering Method", "3.5 The K-Medians and K-Modes Clustering Methods", "3.6 Kernel K-Means Clustering", "Lesson 4 Part 1 Overview", "4.1 Hierarchical Clustering Methods", "4.2 Agglomerative Clustering Algorithms", "4.3 Divisive Clustering Algorithms", "4.4 Extensions to Hierarchical Clustering", "4.5 BIRCH: A Micro-Clustering-Based Approach", "ClusterEnG Introduction", "ClusterEnG Overview", "ClusterEnG: K-Means and K-Medoids", "ClusterEnG Application: AGNES", "ClusterEnG Application: DBSCAN", "Lesson 3 Quiz", "Implementing the K-means Clustering Algorithm"], "title": "Week 2"}, {"description": "", "video": ["Lesson 4 Part 2 Overview", "4.6 CURE: Clustering Using Well-Scattered Representatives (Optional)", "4.7 CHAMELEON: Graph Partitioning on the KNN Graph of the Data", "4.8 Probabilistic Hierarchical Clustering", "Lesson 5 Overview", "5.1 Density-Based and Grid-Based Clustering Methods", "5.2 DBSCAN: A Density-Based Clustering Algorithm", "5.3 OPTICS: Ordering Points To Identify Clustering Structure", "5.4 Grid-Based Clustering Methods", "5.5 STING: A Statistical Information Grid Approach", "5.6 CLIQUE: Grid-Based Subspace Clustering", "Lesson 4 Quiz", "Lesson 5 Quiz"], "title": "Week 3"}, {"description": "", "video": ["Lesson 6 Overview", "6.1 Methods for Clustering Validation", "6.2 Clustering Evaluation Measuring Clustering Quality", "6.3 Constraint-Based Clustering", "6.4 External Measures 1: Matching-Based Measures", "6.5 External Measure 2: Entropy-Based Measures", "6.6 External Measure 3: Pairwise Measures", "6.7 Internal Measures for Clustering Validation", "6.8 Relative Measures", "6.9 Cluster Stability", "6.10 Clustering Tendency", "Lesson 6 Quiz", "Implementing Clustering Validation Measures"], "title": "Week 4"}, {"description": "In the course conclusion, feel free to share any thoughts you have on this course experience.", "video": ["Final Reflections"], "title": "Course Conclusion"}], "title": "Cluster Analysis in Data Mining"}, {"course_info": "About this course: This 1-week, accelerated on-demand course builds upon Google Cloud Platform Big Data and Machine Learning Fundamentals. Through a combination of video lectures, demonstrations, and hands-on labs, you'll learn how to build streaming data pipelines using Google Cloud Pub/Sub and Dataflow to enable real-time decision making. You will also learn how to build dashboards to render tailored output for various stakeholder audience.\n\nPrerequisites:\n• Google Cloud Platform Big Data and Machine Learning Fundamentals (or equivalent experience)\n• Some knowledge of Java\n\nObjectives:\n• Understand use-cases for real-time streaming analytics\n• Use Google Cloud PubSub asynchronous messaging service to manage data events\n• Write streaming pipelines and run transformations where necessary\n• Get familiar with both sides of a streaming pipeline: production and consumption\n• Interoperate Dataflow, BigQuery and Cloud Pub/Sub for real-time streaming and analysis", "level": "Intermediate", "package_name": "Data Engineering on Google Cloud Platform Specialization ", "created_by": "Google Cloud", "package_num": "5", "teach_by": [{"name": "Google Cloud Training", "department": null}], "target_audience": "Who is this class for: This class is intended for data analysts, data scientists and programmers who want to build for out-of-the-ordinary scenarios such as high availability, resiliency, high-throughput, real-time streaming analytics on leveraging Google Cloud Platform. The typical audience member has experience analyzing and visualizing big data, implementing cloud-based big data solutions, and transforming/processing datasets.", "rating": "4.8", "week_data": [{"description": "", "video": ["Module Agenda", "Streaming => data processing for unbounded data sets", "Unbounded datasets are quite common", "The need for fast decisions leads to streaming", "The Three Vs of Big Data", "Stream data processing: Challenges", "Variable volumes makes it possible to derive real-time insights from growing data", "Tightly coupled services propagate failures", "Loosely-coupled systems scale better", "Latency is to be expected", "Latency happens for a variety of reasons", "Beam/Dataflow model provides exactly once processing of events", "Challenge #3: Need instant insights", "BigQuery lets you ingest streaming data and run queries as the data arrives", "Need to use unified language when querying historic and streaming data for seamless integration", "Stream processing on GCP", "Discuss some streaming scenarios", "Lab Worksheet", "Module 1 Quiz"], "title": "Module 1: Architecture of Streaming Analytics Pipelines"}, {"description": "", "video": ["Module Agenda", "Managed stream data processing: A common configuration", "Cloud Pub/Sub connects applications and services through a messaging infrastructure", "Pub/Sub simplifies event distribution", "Pub/Sub features", "How It Works: Topics and Subscriptions", "Create topic and publish message", "Other publishing options", "Push vs. Pull delivery flows", "Creating subscription, pull messages", "Codelab – Publish streaming data into Pub/Sub", "Lab Review", "Module 2 Quiz"], "title": "Module 2: Ingesting Variable Volumes"}, {"description": "", "video": ["Module Intro and Agenda", "Element-wise stream processing is easy. Aggregation and composite on unbounded data is hard", "We could split it into processing time windows...", "A programming model for both batch AND stream", "Challenges in stream processing", "Need to process variable amounts of data that will grow over time", "The Beam unified model is very powerful and handles different processing paradigms", "For example, batch and window in same pipeline", "Dataflow resources are deployed on demand, per job, and work is constantly rebalanced across resources", "Build a stream processing pipeline for live traffic data", "Dataflow is a great way to work with Pub/Sub", "Can enforce only-once handling in Dataflow even if your publisher might retry publishes", "To compute average speed on streaming data, we need to bound the computation within time-windows", "Did we use triggers? What did we do with late data?", "Handle late data: watermarks, triggers, accumulation", "Heuristic/guarantee of completeness (Watermark)", "Windows + Watermarks + Triggers collectively help you handle data arriving late and out-of-order", "How Dataflow handles streaming data while balancing tradeoffs", "Watermark is the system's notion of when all data in a certain window can be expected to have arrived", "Streaming with speculative + late data", "In your DoFn, can get information about Window, Triggers", "Codelab – Streaming Data Pipelines", "Lab Review", "Module 3 Quiz"], "title": "Module 3: Implementing Streaming Pipelines"}, {"description": "", "video": ["Module Intro and Agenda", "What is BigQuery?", "Streaming data into BigQuery", "Data Studio lets you build dashboards and reports", "Pipeline to detect accidents", "Codelab – Streaming Analytics & Dashboards", "Lab Review", "Module 4 Quiz"], "title": "Module 4: Streaming analytics and dashboards"}, {"description": "", "video": ["Module Intro and Agenda", "Choosing where to store data on GCP", "How do you use Cloud Spanner?", "Bigtable: big, fast, autoscaling NoSQL", "Designing for Bigtable", "Rows are sorted lexicographically by row key, from lowest to highest byte string", "Store related entities in adjacent rows", "Distribute your writes and reads across rows", "Distribute the writing load between tablets while allowing common queries to return consecutive rows", "Ingesting into Bigtable", "Lab Overview", "Codelab – Streaming into Bigtable at Low-latency", "Lab Review", "Performance considerations", "Rebalance strategy: distribute storage", "Summary of Data Engineering on GCP Specialization", "Module 5 Quiz"], "title": "Module 5: Handling Throughput and Latency Requirements"}], "title": "Building Resilient Streaming Systems on Google Cloud Platform"}, {"course_info": "About this course: Are you interested in predicting future outcomes using your data? This course helps you do just that! Machine learning is the process of developing, testing, and applying predictive algorithms to achieve this goal. Make sure to familiarize yourself with course 3 of this specialization before diving into these machine learning concepts. Building on Course 3, which introduces students to integral supervised machine learning concepts, this course will provide an overview of many additional concepts, techniques, and algorithms in machine learning, from basic classification to decision trees and clustering. By completing this course, you will learn how to apply, test, and interpret machine learning algorithms as alternative methods for addressing your research questions.", "level": null, "package_name": "Data Analysis and Interpretation Specialization ", "created_by": "Wesleyan University", "package_num": "4", "teach_by": [{"name": "Jen Rose", "department": "Psychology"}, {"name": "Lisa Dierker", "department": "Psychology"}], "target_audience": null, "rating": "4.1", "week_data": [{"description": "In this session, you will learn about decision trees, a type of data mining algorithm that can select from among a large number of variables those and their interactions that are most important in predicting the target or response variable to be explained. Decision trees create segmentations or subgroups in the data, by applying a series of simple rules or criteria over and over again, which choose variable constellations that best predict the target variable.", "video": ["Some Guidance for Learners New to the Specialization", "SAS or Python - Which to Choose?", "Getting Started with SAS", "Getting Started with Python", "Course Codebooks", "Course Data Sets", "Uploading Your Own Data to SAS", "Data Set for Decision Tree Videos (tree_addhealth.csv)", "What Is Machine Learning?", "Machine Learning and the Bias Variance Trade-Off", "What Is a Decision Tree?", "What is the Process of Growing a Decision Tree?", "SAS Code: Decision Trees", "CART Paper - Prevention Science", "Building a Decision Tree with SAS", "Strengths and Weaknesses of Decision Trees in SAS", "Python Code: Decision Trees", "Building a Decision Tree with Python", "Installing Graphviz and pydotplus", "Getting Set up for Assignments", "Tumblr Instructions", "Assignment Example", "Running a Classification Tree"], "title": "Decision Trees"}, {"description": "In this session, you will learn about random forests, a type of data mining algorithm that can select from among a large number of variables those that are most important in determining the target or response variable to be explained. Unlike decision trees, the results of random forests generalize well to new data.", "video": ["What Is A Random Forest and How Is It \"Grown\"?", "SAS code: Random Forests", "The HPForest Procedure in SAS", "Building a Random Forest with SAS", "Python Code: Random Forests", "Building a Random Forest with Python", "Validation and Cross-Validation", "Assignment Example", "Running a Random Forest"], "title": "Random Forests"}, {"description": "Lasso regression analysis is a shrinkage and variable selection method for linear regression models. The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable. The lasso does this by imposing a constraint on the model parameters that causes regression coefficients for some variables to shrink toward zero. Variables with a regression coefficient equal to zero after the shrinkage process are excluded from the model. Variables with non-zero regression coefficients variables are most strongly associated with the response variable. Explanatory variables can be either quantitative, categorical or both. In this session, you will apply and interpret a lasso regression analysis. You will also develop experience using k-fold cross validation to select the best fitting model and obtain a more accurate estimate of your model’s test error rate. \nTo test a lasso regression model, you will need to identify a quantitative response variable from your data set if you haven’t already done so, and choose a few additional quantitative and categorical predictor (i.e. explanatory) variables to develop a larger pool of predictors.  Having a larger pool of predictors to test will maximize your experience with lasso regression analysis. Remember that lasso regression is a machine learning method, so your choice of additional predictors does not necessarily need to depend on a research hypothesis or theory. Take some chances, and try some new variables. The lasso regression analysis will help you determine which of your predictors are most important. Note also that if you are working with a relatively small data set, you do not need to split your data into training and test data sets. The cross-validation method you apply is designed to eliminate the need to split your data when you have a limited number of observations. ", "video": ["What is Lasso Regression?", "SAS Code: Lasso Regression", "Testing a Lasso Regression with SAS", "Data Management for Lasso Regression in Python", "Testing a Lasso Regression Model in Python", "Python Code: Lasso Regression", "Lasso Regression Limitations", "Assignment Example", "Running a Lasso Regression Analysis"], "title": "Lasso Regression"}, {"description": "Cluster analysis is an unsupervised machine learning method that partitions the observations in a data set into a smaller set of clusters where each observation belongs to only one cluster. The goal of cluster analysis is to group, or cluster, observations into subsets based on their similarity of responses on multiple variables. Clustering variables should be primarily quantitative variables, but binary variables may also be included. In this session, we will show you how to use k-means cluster analysis to identify clusters of observations in your data set. You will gain experience in interpreting cluster analysis results by using graphing methods to help you determine the number of clusters to interpret, and examining clustering variable means to evaluate the cluster profiles. Finally, you will get the opportunity to validate your cluster solution by examining differences between clusters on a variable not included in your cluster analysis.  \nYou can use the same variables that you have used in past weeks as clustering variables. If most or all of your previous explanatory variables are categorical, you should identify some additional quantitative clustering variables from your data set. Ideally, most of your clustering variables will be quantitative, although you may also include some binary variables. In addition, you will need to identify a quantitative or binary response variable from your data set that you will not include in your cluster analysis. You will use this variable to validate your clusters by evaluating whether your clusters differ significantly on this response variable using statistical methods, such as analysis of variance or chi-square analysis, which you learned about in Course 2 of the specialization (Data Analysis Tools). Note also that if you are working with a relatively small data set, you do not need to split your data into training and test data sets. \n", "video": ["What Is a k-Means Cluster Analysis?", "Running a k-Means Cluster Analysis in SAS, pt. 1", "Running a k-Means Cluster Analysis in SAS, pt. 2", "SAS Code: k-Means Cluster Analysis", "Python Code: k-Means Cluster Analysis", "Running a k-Means Cluster Analysis in Python, pt. 1", "Running a k-Means Cluster Analysis in Python, pt. 2", "k-Means Cluster Analysis Limitations", "Assignment Example", "Running a k-means Cluster Analysis"], "title": "K-Means Cluster Analysis"}], "title": "Machine Learning for Data Analysis"}, {"course_info": "About this course: Learner Outcomes: After taking this course, you will be able to:\n- Utilize various Application Programming Interface (API) services to collect data from different social media sources such as YouTube, Twitter, and Flickr.\n- Process the collected data - primarily structured - using methods involving correlation, regression, and classification to derive insights about the sources and people who generated that data.\n- Analyze unstructured data - primarily textual comments - for sentiments expressed in them.\n- Use different tools for collecting, analyzing, and exploring social media data for research and development purposes.\n\nSample Learner Story: Data analyst wanting to leverage social media data.\nIsabella is a Data Analyst working as a consultant for a multinational corporation. She has experience working with Web analysis tools as well as marketing data. She wants to now expand into social media arena, trying to leverage the vast amounts of data available through various social media channels. Specifically, she wants to see how their clients, partners, and competitors view their products/services and talk about them. She hopes to build a new workflow of data analytics that incorporates traditional data processing using Web and marketing tools, as well as newer methods of using social media data.\n\nSample Job Roles requiring these skills: \n- Social Media Analyst\n- Web Analyst\n- Data Analyst\n- Marketing and Public Relations \n\nFinal Project Deliverable/ Artifact: The course will have a series of small assignments or mini-projects that involve data collection, analysis, and presentation involving various social media sources using the techniques learned in the class.", "level": "Intermediate", "package_name": null, "created_by": "Rutgers the State University of New Jersey", "package_num": null, "teach_by": [{"name": "Chirag Shah", "department": "Information and Computer Science"}], "target_audience": "Who is this class for: This course is for students who have at least some beginning training in technologies, including programming and databases. Specifically, it is expected that the student has done some programming before such as C, Java, Python, PHP, Perl, Pascal, Cobol, and JavaScript. The minimal training from any of such programming language expected is the understanding of variable declaration (x=10), condition checking (if x==10), and simple loops (while x<10). The student may or may not have some knowledge of statistics for data analysis - an optional statistics module will be included for reference.", "rating": "4.0", "week_data": [{"description": "In this first unit of the course, several concepts related to social media data and data analytics are introduced. We start by first discussing two kinds of data - structured and unstructured. Then look at how structured data, the primary focus of this course, is analyzed and what one could gain by doing such analysis. Finally, we briefly cover some of the visualizations for exploring and presenting data.Make sure to go through the material for this unit in the sequence it's provided. First, watch the four short videos, then take the practice test, followed by the two quizzes. Finally, read the documents about installation and configuration of Python and R. This is very important - before proceeding to the next units, make sure you have installed necessary tools, and also learned how to install new packages/libraries for them. The course expects students to have programming experience in Python and R.", "video": ["Video-1: Introduction", "Video-2: Structured vs. Unstructured Data", "Video-3: Analyzing Structured Data", "Video-4: Visualization of Data", "Anaconda Installation", "Python installation, configuration, and usage", "R installation", "R/RStudio Setup Guide (on Windows)", "Installation and Configuration of Development Environment", "Quiz-1", "Quiz-2"], "title": "Introduction to Data Analytics"}, {"description": "In this unit we will see how to collect data from Twitter and YouTube. The unit will start with an introduction to Python programming. Then we will use a Python script, with a little editing, to extract data from Twitter. A similar exercise will then be done with YouTube. In both the cases, we will also see how to create developer accounts and what information to obtain to use the data collection APIs.\n\nOnce again, make sure to go item-by-item in the order provided. Before beginning this unit, ensure that you have all the right tools (Python, R, Anaconda) ready and configured. The lessons depend on them and also your ability to install required packages.", "video": ["Video-1: Introduction", "Errata: please read this first", "Video-2: Introduction to Python Programming", "Python Packages Installation", "(Optional) Introduction to Python for Econometrics, Statistics and Data Analysis", "Video-3: Using Python to Extract Data from Twitter", "Script: twitter_search.py", "Twitter libraries", "Video-4: Using Python to Extract Data from YouTube", "Script: youtube_search.py", "Python Programming Exercise", "Twitter data download using Python", "YouTube data download using Python"], "title": "Collecting and Extracting Social Media Data"}, {"description": "In this unit, we will focus on analyzing and visualizing the data from various social media services. We will first use the data collected before from YouTube to do various statistics analyses such as correlation and regression. We will then introduce R - a platform for doing statistical analysis. Using R, then we will analyze a much larger dataset obtained from Yelp.\n\nMake sure you have covered the material in the previous units before proceeding with this. That means, having all the tools (Anaconda, Python, and R) as well as various packages installed. We will also need new packages this time, so make sure you know how to install them to your Python or R. If needed, please review some basic concepts in statistics - specifically, correlation and regression - before or during working on this unit.", "video": ["Video-1: Introduction", "Video-2: Analyzing Social Media Data Using Python", "Script: twitter_process.py", "Statistical Analysis with Twitter Data", "Video-3: Introduction to R", "Data: iqsize.csv", "R Installation Guide", "Installing R Packages", "Statistical Analysis with R", "Read this first", "Video-4: Social Media Data Analysis with R", "Scripts for converting json to csv", "Data Visualization with ggplot2 (R) - Cheat Sheet", "Data Visualization using R", "Statistical Analysis with Twitter Data", "Data Visualization using R"], "title": "Data Analysis, Visualization, and Exploration"}, {"description": "In the final unit of this course, we will work on two case studies - both using Twitter and focusing on unstructured data (in this case, text). The first case study will involve doing sentiment analysis with Python. The second case study will take us through basic text mining application using R. We wrap up the unit with a conclusion of what we did in this course and where to go next for further learning and exploration.", "video": ["Video-1: Introduction", "Video-2: Sentiment Analysis with Twitter Data", "Script: twitter_sentiments.py", "NLTK", "Sentiment Analysis with Twitter Data", "Video-3: Text Mining of Twitter Data", "Script: text_mining_twitter.r", "An Introduction to Network Analysis with R and statnet", "Video-4: Conclusion", "Sentiment Analysis with Twitter", "Text Mining with Twitter"], "title": "Case Studies"}], "title": "Social Media Data Analytics"}, {"course_info": "About this course: Inferential statistics are concerned with making inferences based on relations found in the sample, to relations in the population. Inferential statistics help us decide, for example, whether the differences between groups that we see in our data are strong enough to provide support for our hypothesis that group differences exist in general, in the entire population.\n\nWe will start by considering the basic principles of significance testing: the sampling and test statistic distribution, p-value, significance level, power and type I and type II errors. Then we will consider a large number of statistical tests and techniques that help us make inferences for different types of data and different types of research designs. For each individual statistical test we will consider how it works, for what data and design it is appropriate and how results should be interpreted. You will also learn how to perform these tests using freely available software. \n\nFor those who are already familiar with statistical testing: We will look at z-tests for 1 and 2 proportions,  McNemar's test for dependent proportions, t-tests for 1 mean (paired differences) and 2 means, the Chi-square test for independence, Fisher’s exact test, simple regression (linear and exponential) and multiple regression (linear and logistic), one way and factorial analysis of variance, and non-parametric tests (Wilcoxon, Kruskal-Wallis, sign test,  signed-rank test, runs test).", "level": null, "package_name": "Methods and Statistics in Social Sciences Specialization ", "created_by": "University of Amsterdam", "package_num": "4", "teach_by": [{"name": "Annemarie Zand Scholten", "department": "Economics and Business"}, {"name": "Emiel van Loon", "department": "Institute for Biodiversity and Ecosystem Dynamics"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "[formatted text here]", "video": ["Hi there", "Welcome to Inferential Statistics!", "How to navigate this course", "How to contribute", "General info - What will I learn in this course?", "Course format - How is this course structured?", "Requirements - What resources do I need?", "Grading - How do I pass this course?", "Team - Who created this course?", "Honor Code - Integrity in this course", "Useful literature and documents"], "title": "Before we get started..."}, {"description": "In this second module of week 1 we dive right in with a quick refresher on statistical hypothesis testing. Since we're assuming you just completed the course Basic Statistics, our treatment is a little more abstract and we go really fast! We provide the relevant Basic Statistics videos in case you need a gentler introduction. After the refresher we discuss methods to compare two groups on a categorical or quantitative dependent variable. We use different test for independent and dependent groups.", "video": ["Comparing two groups - Drawing inferences", "1.01 Null hypothesis testing", "1.02 P-values", "1.03 Confidence intervals and two-sided tests", "1.04 Power", "Comparing two groups - Independent groups", "1.05 Two independent proportions", "1.06 Two independent means", "Comparing two groups - Dependent groups", "1.07 Two dependent proportions", "1.08 Two dependent means", "Comparing two groups - Controlling for other variables", "1.09 Controlling for other variables", "Comparing two groups - Transcripts", "R lab - Getting started (part1)", "R lab - Getting started (part 2)", "Comparing two groups", "R lab - Comparing two groups"], "title": "Comparing two groups"}, {"description": "In this module we tackle categorical association. We'll mainly discuss the Chi-squared test that allows us to decide whether two categorical variables are related in the population. If two categorical variables are unrelated you would expect that categories of these variables don't 'go together'. You would expect the number of cases in each category of one variable to be proportionally similar at each level of the other variable. The Chi-squared test helps us to compare the actual number of cases for each combination of categories (the joint frequencies) to the expected number of cases if the variables are unrelated.", "video": ["Categorical association - Chi-squared test for association", "2.01 Categorical association and independence", "2.02  The Chi-squared test", "2.03 Interpreting the Chi-squared test", "Categorical association - Chi-squared test for goodness of fit", "2.04 Chi-squared as goodness-of-fit", "Categorical association - Sidenotes and an alternative to the Chi-squared test", "2.05 The Chi-squared test - sidenotes", "2.06 Fisher's exact test", "Categorical association - Transcripts", "Categorical association", "R lab - Categorical association"], "title": "Categorical association"}, {"description": "In this module we’ll see how to describe the association between two quantitative variables using simple (linear) regression analysis. Regression analysis allows us to model the relation between two quantitative variables and - based on our sample -decide whether a 'real' relation exists in the population. Regression analysis is more useful than just calculating a correlation coefficient, since it allows us assess how well our regression line fits the data, it helps us to identify outliers and to predict scores on the dependent variable for new cases.", "video": ["Simple regression - Describing quantitative association", "3.01 The regression line", "3.02 The regression equation", "3.03 The regression model", "3.04 Predictive power", "Simple regression - Drawing inferences", "3.05 Pitfalls in regression", "3.06 Testing the model", "3.07 Checking assumptions", "3.08 CI and PI for predicted values", "Simple regression - Exponential regression", "3.09 Exponential regression", "Simple regression - Transcripts", "Simple regression", "R lab - Simple regression"], "title": "Simple regression"}, {"description": "In this module we’ll see how we can use more than one predictor to describe or predict a quantitative outcome variable. In the social sciences relations between psychological and social variables are generally not very strong, since outcomes are generally influences by complex processes involving many variables. So it really helps to be able to describe an outcome variable with several predictors, not just to increase the fit of the model, but also to assess the individual contribution of each predictor, while controlling for the others. ", "video": ["Multiple regression - Model", "4.01 Regression model", "4.02 R and R-squared", "Multiple regression - Tests", "4.03 Overall test", "4.04 Individual tests", "4.05 Checking assumptions", "Multiple regression - Categorical predictors, categorical response variable and example", "4.06 Categorical predictors", "4.07 Categorical response variable", "4.08 Interpreting results", "Multiple regression - Transcripts", "Multiple regression", "R lab - Multiple regression"], "title": "Multiple regression"}, {"description": "In this module we'll discuss analysis of variance, a very popular technique that allows us to compare more than two groups on a quantitative dependent variable. The reason we call it analysis of variance is because we compare two estimates of the variance in the population. If the group means differ in the population then these variance estimates differ. Just like in multiple regression, factorial analysis of variance allows us to investigate the influence of several independent variables.", "video": ["Analysis of variance - Basics and one-way ANOVA", "5.01 One-way ANOVA", "5.02 One-way ANOVA - Assumptions and F-test", "5.03 One-way ANOVA - Post-hoc t-tests", "Analysis of variance - Factorial ANOVA and regression", "5.04 Factorial ANOVA", "5.05  Factorial ANOVA - Assumptions and tests", "5.06 ANOVA and regression", "Analysis of variance - Transcripts", "Analysis of variance", "R lab - Analysis of variance"], "title": "Analysis of variance"}, {"description": "In this module we'll discuss the last topic of this course: Non-parametric tests. Until now we've mostly considered tests that require assumptions about the shape of the distribution (z-tests, t-tests and F-tests). Sometimes those assumptions don't hold. Non-parametric tests require fewer of those assumptions. There are several non-parametric tests that correspond to the parametric z-, t- and F-tests. These tests also come in handy when the response variable is an ordered categorical variable as opposed to a quantitative variable. There are also non-parametric equivalents to the correlation coefficient and some tests that have no parametric-counterparts.", "video": ["Non-parametric tests - The basics", "6.01 Non-parametric tests - Why and when", "6.02 The sign test", "Non-parametric tests - Comparing groups with respect to mean rank", "6.03 One sample - Wilcoxon signed rank test", "6.04 Two samples - Wilcoxon/Mann-Whitney test", "6.05 Several samples - Kruskal-Wallis test", "a note about test-names", "Non-parametric tests - Rank-based correlation & randomness", "6.06 Spearman correlation", "6.07 The runs test", "Non-parametric tests - Transcripts", "Non-parametric tests", "R lab - Non-parametric tests"], "title": "Non-parametric tests"}, {"description": "In this final module there's no new material to study. We advise you to take some extra time to review the material from the previous modules and to practice for the final exam. We've provided a practice exam that you can take as many times as you like. The final exam is structured exactly like the practice exam, so you know what to expect. Please note that you can only take the final exam twice every seven days, so make sure you are fully prepared. Please follow the honor code and do not communicate or confer with others while taking this exam or after. In the open questions of the exam (i.e. those that are not multiple choice) you should report your answers to 3 decimal places, and use 5 decimal places in your calculations. Good luck!", "video": ["Practice exam", "Final exam"], "title": "Exam time!"}], "title": "Inferential Statistics"}, {"course_info": "About this course: Learn the general concepts of data mining along with basic methodologies and applications. Then dive into one subfield in data mining: pattern discovery. Learn in-depth concepts, methods, and applications of pattern discovery in data mining. We will also introduce methods for data-driven phrase mining and some interesting applications of pattern discovery. This course provides you the opportunity to learn skills and content to practice and engage in scalable pattern discovery methods on massive transactional data, discuss pattern evaluation measures, and study methods for mining diverse kinds of patterns, sequential patterns, and sub-graph patterns.", "level": null, "package_name": "Data Mining  Specialization ", "created_by": "University of Illinois at Urbana-Champaign", "package_num": "4", "teach_by": [{"name": "Jiawei Han", "department": "Department of Computer Science"}], "target_audience": null, "rating": "4.2", "week_data": [{"description": "The course orientation will get you familiar with the course, your instructor, your classmates, and our learning environment.", "video": ["Course Introduction", "Syllabus", "About the Discussion Forums", "Getting to Know Your Classmates", "Social Media", "Orientation Quiz"], "title": "Course Orientation"}, {"description": "Module 1 consists of two lessons. Lesson 1 covers the general concepts of pattern discovery. This includes the basic concepts of frequent patterns, closed patterns, max-patterns, and association rules. Lesson 2 covers three major approaches for mining frequent patterns. We will learn the downward closure (or Apriori) property of frequent patterns and three major categories of methods for mining frequent patterns: the Apriori algorithm, the method that explores vertical data format, and the pattern-growth approach.  We will also discuss how to directly mine the set of closed patterns.", "video": ["Lesson 1 Overview", "1.1. What Is Pattern Discovery? Why Is It Important?", "1.2. Frequent Patterns and Association Rules", "1.3. Compressed Representation: Closed Patterns and Max-Patterns", "Lesson 2 Overview", "2.1. The Downward Closure Property of Frequent Patterns", "2.2. The Apriori Algorithm", "2.3. Extensions or Improvements of Apriori", "2.4. Mining Frequent Patterns by Exploring Vertical Data Format", "2.5. FPGrowth: A Pattern Growth Approach", "2.6. Mining Closed Patterns", "Lesson 1 Quiz", "Lesson 2 Quiz", "Frequent Itemset Mining Using Apriori"], "title": "Module 1"}, {"description": "Module 2 covers two lessons: Lessons 3 and 4.  In Lesson 3, we discuss pattern evaluation and learn what kind of interesting measures should be used in pattern analysis. We show that the support-confidence framework is inadequate for pattern evaluation, and even the popularly used lift and chi-square measures may not be good under certain situations. We introduce the concept of null-invariance and introduce a new null-invariant measure for pattern evaluation. In Lesson 4, we examine the issues on mining a diverse spectrum of patterns. We learn the concepts of and mining methods for multiple-level associations, multi-dimensional associations, quantitative associations, negative correlations, compressed patterns, and redundancy-aware patterns.", "video": ["Lesson 3 Overview", "3.1. Limitation of the Support-Confidence Framework", "3.2. Interestingness Measures: Lift and χ2", "3.3. Null Invariance Measures", "3.4. Comparison of Null-Invariant Measures", "Lesson 4 Overview", "4.1. Mining Multi-Level Associations", "4.2. Mining Multi-Dimensional Associations", "4.3. Mining Quantitative Associations", "4.4. Mining Negative Correlations", "4.5. Mining Compressed Patterns", "Lesson 3 Quiz", "Lesson 4 Quiz"], "title": "Module 2"}, {"description": "Module 3 consists of two lessons: Lessons 5 and 6.   In Lesson 5, we discuss mining sequential patterns.   We will learn several popular and efficient sequential pattern mining methods, including an Apriori-based sequential pattern mining method, GSP; a vertical data format-based sequential pattern method, SPADE; and a pattern-growth-based sequential pattern mining method, PrefixSpan. We will also learn how to directly mine closed sequential patterns. In Lesson 6, we will study concepts and methods for mining spatiotemporal and trajectory patterns as one kind of pattern mining applications. We will introduce a few popular kinds of patterns and their mining methods, including mining spatial associations, mining spatial colocation patterns, mining and aggregating patterns over multiple trajectories, mining semantics-rich movement patterns, and mining periodic movement patterns.", "video": ["Lesson 5 Overview", "5.1. Sequential Pattern and Sequential Pattern Mining", "5.2. GSP: Apriori-Based Sequential Pattern Mining", "5.3. SPADE—Sequential Pattern Mining in Vertical Data Format", "5.4. PrefixSpan—Sequential Pattern Mining by Pattern-Growth", "5.5. CloSpan—Mining Closed Sequential Patterns", "Lesson 6 Overview", "6.1. Mining Spatial Associations", "6.2. Mining Spatial Colocation Patterns", "6.3. Mining and Aggregating Patterns over Multiple Trajectories", "6.4. Mining Semantics-Rich Movement Patterns", "6.5. Mining Periodic Movement Patterns", "Lesson 5 Quiz", "Lesson 6 Quiz"], "title": "Module 3"}, {"description": "Module 4 consists of two lessons: Lessons 7 and 8.   In Lesson 7, we study mining quality phrases from text data as the second kind of pattern mining application. We will mainly introduce two newer methods for phrase mining: ToPMine and SegPhrase, and show frequent pattern mining may be an important role for mining quality phrases in massive text data. In Lesson 8, we will learn several advanced topics on pattern discovery, including mining frequent patterns in data streams, pattern discovery for software bug mining, pattern discovery for image analysis, and pattern discovery and society: privacy-preserving pattern mining.  Finally, we look forward to the future of pattern mining research and application exploration.", "video": ["Lesson 7 Overview", "7.1. From Frequent Pattern Mining to Phrase Mining", "7.2. Previous Phrase Mining Methods", "7.3. ToPMine: Phrase Mining without Training Data", "7.4. SegPhrase: Phrase Mining with Tiny Training Sets", "Mining Contiguous Sequential Patterns in Text", "Lesson 8 Overview", "8.1. Frequent Pattern Mining in Data Streams", "8.2. Pattern Discovery for Software Bug Mining", "8.3. Pattern Discovery for Image Analysis", "8.4. Advanced Topics on Pattern Discovery: Pattern Mining and Society—Privacy Issue", "8.5. Advanced Topics on Pattern Discovery: Looking Forward", "Lesson 7 Quiz", "Lesson 8 Quiz"], "title": "Week 4"}], "title": "Pattern Discovery in Data Mining"}, {"course_info": "About this course: Science is undergoing a data explosion, and astronomy is leading the way. Modern telescopes produce terabytes of data per observation, and the simulations required to model our observable Universe push supercomputers to their limits. To analyse this data scientists need to be able to think computationally to solve problems. In this course you will investigate the challenges of working with large datasets: how to implement algorithms that work; how to use databases to manage your data; and how to learn from your data with machine learning tools. The focus is on practical skills - all the activities will be done in Python 3, a modern programming language used throughout astronomy.\n\nRegardless of whether you’re already a scientist, studying to become one, or just interested in how modern astronomy works ‘under the bonnet’, this course will help you explore astronomy: from planets, to pulsars to black holes.\n\nCourse outline:\nWeek 1: Thinking about data\n- Principles of computational thinking\n- Discovering pulsars in radio images\n\nWeek 2: Big data makes things slow\n- How to work out the time complexity of algorithms\n- Exploring the black holes at the centres of massive galaxies\n\nWeek 3: Querying data using SQL\n- How to use databases to analyse your data\n- Investigating exoplanets in other solar systems\n\nWeek 4: Managing your data\n- How to set up databases to manage your data\n- Exploring the lifecycle of stars in our Galaxy\n\nWeek 5: Learning from data: regression\n- Using machine learning tools to investigate your data\n- Calculating the redshifts of distant galaxies\n\nWeek 6: Learning from data: classification\n- Using machine learning tools to classify your data\n- Investigating different types of galaxies\n\nEach week will also have an interview with a data-driven astronomy expert.\n\nNote that some knowledge of Python is assumed, including variables, control structures, data structures, functions, and working with files.", "level": "Intermediate", "package_name": null, "created_by": "The University of Sydney", "package_num": null, "teach_by": [{"name": "Tara Murphy", "department": "School of Physics"}, {"name": "Simon Murphy", "department": "School of Physics"}], "target_audience": "Who is this class for: This course is aimed at science students with an interest in computational approaches to problem solving, people with an interest in astronomy who would like to learn current research methods, or people who would like to improve their programming by applying it to astronomy examples. ", "rating": "4.9", "week_data": [{"description": "This module introduces the idea of computational thinking, and how big data can make simple problems quite challenging to solve. We use the example of calculating the median and mean stack of a set of radio astronomy images to illustrate some of the issues you encounter when working with large datasets. ", "video": ["Thinking about data", "Introduce yourself (optional)", "Course overview", "Pulsars", "Diving in: imaging stacking", "Challenge: the median doesn't scale", "How could you improve  the algorithm?", "The solution: improving your method", "Calculating the median stack", "Module summary", "Further reading", "Interview with Aris Karastergiou", "Set up your online assessment", "Pulsars: test your understanding", "Calculating the mean stack"], "title": "Thinking about data"}, {"description": "In this module we explore the idea of scaling your code. Some algorithms scale well as your dataset increases, but others become impossibly slow. We look at some of the reason for this, and use the example of cross-matching astronomical catalogues to demonstrate what kind of improvements you can make. ", "video": ["Big data makes things slow", "What scaling problems have you encountered?", "Supermassive black holes", "What is cross-matching?", "Evaluating time complexity", "A (much) faster algorithm", "Crossmatching with k-d trees", "Module summary", "Interview with Brendon Brewer", "Supermassive black holes: test your understanding", "A naive cross-matcher"], "title": "Big data makes things slow"}, {"description": "Most large astronomy projects use databases to manage their data. In this module we introduce SQL - the language most commonly used to query databases. We use SQL to query the NASA Exoplanet database and investigate the habitability of planets in other solar systems.", "video": ["Organising your data", "Do you use databases in your work?", "Exoplanets", "Querying database with SQL", "More advanced SQL", "Joining tables in SQL", "Joining tables with SQL", "Module summary", "Interview with Jon Jenkins", "Exoplanets - test your understanding", "Writing your own SQL queries"], "title": "Querying your data"}, {"description": "This module introduces the basic principles of setting up databases. We look at how to set up new tables, and then how to combine Python and SQL to get the best out of both approaches. We use these tools to explore the life of stars in a stellar cluster.\n", "video": ["Managing your big datasets", "The lifecycle of stars", "Setting up your own database", "Exploring a star cluster", "Combining SQL and Python", "Module summary", "Interview with Emily Petroff", "Stars - test your understanding", "Setting up your own database"], "title": "Managing your data"}, {"description": "This module introduces the idea of machine learning. We look at standard methodology for running machine learning experiments, and then apply this to calculating redshifts of distant galaxies using decision trees for regression. ", "video": ["Learning from data", "The cosmological distance scale", "What is machine learning?", "Decision tree classifiers", "Estimating redshifts using regression", "Improving and evaluating our classifier", "Summary", "Interview with Ashish Mahabal", "Cosmological distances - test your understanding", "Building a regression classifier"], "title": "Learning from data: regression"}, {"description": "In this final module we explore the limitations of decision tree classifiers. We then look at ensemble classifiers, using the random forest algorithm to classify images of galaxies into different types.", "video": ["Classifying your data", "Types of galaxies", "Morphological classification of galaxies", "Limitations of decision tree classifiers", "Classify some galaxies by hand!", "Reflection on galaxy classification", "Improving our results with ensemble classifiers", "Module summary", "Interview with Karen Masters", "Galaxies - test your understanding", "Exploring machine learning classification"], "title": "Learning from data: classification"}], "title": "Data-driven Astronomy"}, {"course_info": "About this course: Welcome to this course on Data Analytics for Lean Six Sigma. \n\nIn this course you will learn data analytics techniques that are typically useful within Lean Six Sigma improvement projects. At the end of this course you are able to analyse and interpret data gathered within such a project. You will be able to use Minitab to analyse the data. I will also briefly explain what Lean Six Sigma is.\n\nI will emphasize on use of data analytics tools and the interpretation of the outcome. I will use many different examples from actual Lean Six Sigma projects to illustrate all tools. I will not discuss any mathematical background. \n\nThe setting we chose for our data example is a Lean Six Sigma improvement project. However data analytics tools are very widely applicable. So you will find that you will learn techniques that you can use in a broader setting apart from improvement projects. \n\nI hope that you enjoy this course and good luck!\nDr. Inez Zwetsloot & the IBIS UvA team", "level": "Beginner", "package_name": null, "created_by": "University of Amsterdam", "package_num": null, "teach_by": [{"name": "Inez Zwetsloot", "department": "Amsterdam Business School, Economics & Business"}], "target_audience": "Who is this class for: This course is aimed at anyone who would like to learn more about data analytics tools and Lean Six Sigma. No prior knowledge (data analytics or Lean Six Sigma) is required. ", "rating": "4.8", "week_data": [{"description": "This module introduces Lean Six Sigma and shows you where data and data analytics have their place within the DMAIC framework.  It also introduces the software package Minitab. This package is used throughout the videos for data analytics. It is not mandatory to use this package. I just really like it!", "video": ["Introduction to Data Analytics for Lean Six Sigma", "Let me introduce myself!", "Let me introduce IBIS UvA", "M1-V1 Introduction to Lean Six Sigma", "M1-V2 Data and DMAIC", "M1-V3 Selecting CTQs", "M1-V4 Units and operational definition", "M1-V5 Sampling", "M1-V6 Organizing your data", "Overview videos", "Minitab: what is it?", "M1-V7 Installing Minitab", "M1-V8 Introduction to Minitab", "M1-V9 Loading data into Minitab", "Explanation quizzes", "Practice quiz - Data and Lean Six Sigma", "Graded quiz - Data and Lean Six Sigma"], "title": "Data and Lean Six Sigma"}, {"description": "This module explains how to visualize data. It discusses visualizing single variables as well as visualizing two variables. You will learn to select the appropriate graph. For this it is essential to first learn the distinction between numerical and categorical data. ", "video": ["M2-V1 Numerical and categorical data", "Data needed for the next videos!", "M2-V2 Descriptive statistics", "M2-V3 Visualizing numerical data", "M2-V4 Visualizing categorical data", "M2-V5 Pareto analysis", "M2-V6 Visualizing two variables", "Video exercises", "M2-V7 Exercise - Investigation time", "M2-V8 Exercise - Coffee batch", "Explanation quizzes", "Practice quiz - Understanding and visualizing data", "Graded quiz - Understanding and visualizing data"], "title": "Understanding and visualizing data"}, {"description": "In  this module on using probability distributions, you will learn how to quantify uncertainty. Furthermore you will learn to answer an important business question:  “what percentage of products or cases meet our specifications?\".", "video": ["M3-V1 Population versus sampling", "M3-V2 Estimation and confidence intervals", "M3-V3 Normal, Lognormal and Weibull distribution", "M3-V4 Probability plot", "M3-V5 Empirical CDF", "M3-V6 Properties of the normal distribution", "M3-V7 Exercise - Length of Stay", "Explanation quizzes", "Practice quiz - Using probability distributions", "Graded quiz - Using probability distributions"], "title": "Using probability distributions"}, {"description": "You will learn to model your CTQ and influence factor(s) and to use a decision tree to select the appropriate tool for data based testing of this model. Furthermore, causality is introduced.", "video": ["M4-V1 Introduction to data analysis", "M4-V2 Hypothesis testing", "M4-V3 Causality", "Explanation quizzes", "Practice quiz - Introduction to testing"], "title": "Introduction to testing"}, {"description": "In this module on statistical testing, you will learn how to establish relationship between a numerical Y variable (the CTQ) and categorical influence factors (the X variables). ", "video": ["M5-V1 Introduction to ANOVA", "M5-V2 ANOVA analysis", "M5-V3 ANOVA residual analysis", "M5-V4 Kruskal-Wallis test", "M5-V5 Two sample t-test", "Note on video M5-V6", "M5-V6 Test for equality of variances", "M5-V7 Exercise - Productivity", "M5-V8 Exercise - Department", "Explanation quizzes", "Practice quiz - Testing: numerical Y and categorical X", "Graded quiz - Introduction to testing & Testing: numerical Y and categorical X"], "title": "Testing: numerical Y and categorical X"}, {"description": "What is the relation between the length of stay and the age of a patient? In this module you will learn to answers these types of questions using statistical tests to relate a numerical CTQ (the Y variable) to a numerical influence factor (the X variable).", "video": ["M6-V1 Correlation", "M6-V2 Introduction to regression", "M6-V3 Regression analysis", "M6-V4 Regression residual analysis", "M6-V5 Regression prediction interval", "M6-V6 Quadratic regression", "M6-V7 Exercise - picking", "Explanation quizzes", "Practice quiz - Testing: numerical Y and numerical X"], "title": "Testing: numerical Y and numerical Y"}, {"description": "Finally you will learn how to test a relationship between a Y and a X variable whenever your Y variable (the CTQ) is a categorical variable. ", "video": ["M7-V1 Chi-square analysis", "M7-V2 Logistic regression", "M7-V3 Exercise - printers", "M7-V4 Exercise - students", "Explanation quizzes", "Practice quiz - Testing: categorical Y", "Graded quiz - Testing: numerical Y and numerical X & Testing: categorical Y"], "title": "Testing: categorical Y"}], "title": "Data Analytics for Lean Six Sigma"}, {"course_info": "About this course: This course covers the design, acquisition, and analysis of Functional Magnetic Resonance Imaging (fMRI) data.\n\nA book related to the class can be found here: https://leanpub.com/principlesoffmri", "level": null, "package_name": null, "created_by": "Johns Hopkins University, University of Colorado Boulder", "package_num": null, "teach_by": [{"name": "Martin Lindquist, PhD, MSc", "department": "Bloomberg School of Public Health | Johns Hopkins University"}, {"name": "Tor Wager", "department": "Department of Psychology and Neuroscience, The Institute of Cognitive Science | University of Colorado at Boulder"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "This week we will introduce fMRI, and talk about data acquisition and reconstruction. ", "video": ["Module 1: Introduction and Ground Rules", "Syllabus", "Principles of fMRI Book", "Module 2: Goals of fMRI Analysis", "Module 3: fMRI Data Structure", "Module 4.1: Psychological Inference Part 1", "Module 4.2: Psychological Inference Part 2", "Module 5: Basic Understanding of MR Physics", "Module 6: Forming an Image", "Module 7: K Space", "Quiz 1"], "title": "Week 1"}, {"description": "This week we will discuss the fMRI signal, experimental design and pre-processing. ", "video": ["Module 8: Signal, Noise, and Bold Physiology", "Module 9: fMRI Artifacts and Types of Noise", "Module 10.1: Spatial and Temporal Resolution of Bold Part 1", "Module 10.2: Spatial and Temporal Resolution of Bold Part 2 ", "Module 11: Experimental Design", "Module 12.1: Kinds of Designs Part 1", "Module 12.2: Kinds of Designs Part 2 ", "Module 13: Pre-Processing of fMRI Data", "Module 14: Pre-Processing (continued)", "Quiz 2"], "title": "Week 2"}, {"description": "This week we will discuss the General Linear Model (GLM).", "video": ["Module 15: General Linear Model", "Module 16: Applying GLM to fMRI Data", "Module 17: Details of Building GLM Models", "Module 18: Linear Basis Sets", "Module 19: Filtering a Nuisance Covariance", "Module 20: GLM Estimation", "Module 21: Noise Models - AR Models", "Module 22: Inference - Contrasts and T-tests", "Quiz 3"], "title": "Week 3"}, {"description": "The description goes here", "video": ["Module 23: Group-level Analysis I", "Module 24: Group-level Analysis II", "Module 25: Group-level Analysis III", "Module 26: Multiple Comparison Problem in fMRI", "Module 27: FWER Correction", "Module 28: FDR Correction", "Module 29: Pitfalls and Multiple Comparisons", "Quiz 4"], "title": "Week 4"}], "title": "Principles of fMRI 1"}, {"course_info": "About this course: In this second MOOC in the Social Marketing Specialization - \"The Importance of Listening\" - you will go deep into the Big Data of social and gain a more complete picture of what can be learned from interactions on social sites. You will be amazed at just how much information can be extracted from a single post, picture, or video. In this MOOC, guest speakers from Social Gist, BroadReader, Lexalytics, Semantria, Radian6, and IBM's Bluemix and Social Media Analytics Tools (SMA) will join Professor Hlavac to take you through the full range of analytics tools and options available to you and how to get the most from them. The best part, most of them will be available to you through the MOOC for free! Those purchasing the MOOC will receive special tools, templates, and videos to enhance your learning experience. In completing this course you will develop a fuller understanding of the data and will be able to increase the effectiveness of your content strategy by making better decisions and spotting crises before they happen! MOOC 2 bonus content in the paid toolkit includes access to Semantria's analytics engine to extract some data on the markets you are developing and have it analyzed. \n\nAs a student in this course, you are being provided the opportunity to access IBM Bluemix® platform-as-a-service trial for up to six months at no-charge with no credit card (up to a $1500 value).\n\nNOTE: By enrolling in this course, given access to IBM's Bluemix technology for one month for free as well as Lexalytics' Semantria tool. For those earning a Course Certificate, you will be given an additional five months of Bluemix and three months of Semantria at no cost with a special key code. By enrolling for a Course Certificate for this MOOC, you are acknowledging that your email will be shared with Lexalytics for the sole and express purpose of generating your individual key code. After the key code has been generated, Lexalytics will delete your email from its records. \n\nAdditional MOOC 2 faculty include: \n* Steve Dodd (SVP Business Development, Effyis - dba BoardReader and Socialgist - Global Social Media Content Access) \n* Seth Redmore (CMO, Lexalytics, Inc.)\n* Chris Gruber (Social Media Analytics Solution Architect, IBM)\n* Russell Beardall (Cloud Architect, IBM) \n* Tom Collinger (Executive Director Spiegel Research Center and Senior Director Distance Learning, Medill Integrated Marketing Communications, Northwestern)\n* Tressie Lieberman (VP Digital Innovation, Taco Bell)", "level": null, "package_name": "Social Media Marketing Specialization ", "created_by": "Northwestern University", "package_num": "2", "teach_by": [{"name": "Randy Hlavac", "department": null}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "In this module, you will learn how big data is collected, standardized, and deployed by organizations into big insights. ", "video": ["Welcome to MOOC 2!", "Meet Your Instructors", "Grading and Logistics", "Using Social Data", "Social Data Flows from a Single Source", "Data Extraction from Social Documents", "Accuracy of Text Analysis", "Data Extraction from Photos and Videos", "Social Marketing Toolkits", "Course Bookstore", "A Note on the Frequency of Capstone Offerings", "Special Tools and Information for You", "Step 1 to Sign Up for IBM Bluemix", "Step 2 to Sign Up for IBM Bluemix", "Step 3 - Getting your 6 month upgrade for IBM Bluemix", "Step 3 - Link to Upgrade your IBM Bluemix free trial to 6 months", "Step 4 to Upgrade your IBM Bluemix Account to 6 months", "Signing Up for IBM WASM [Watson Analytics for Social Media]", "Sign up for Lexalytics Semantria", "Having Problems with IBM and Lexalytics?", "Quiz 1"], "title": "Big Data"}, {"description": "In this module, you will learn how to use key social information to drive your social strategy using state of the art analytics systems.  ", "video": ["The How-to Section", "Boardreader - Another Free Social Monitoring Tool", "Hashtagify.me - Another Free Social Monitoring System -", "Social Mention - Free Social Monitoring Tool - Currently Offline", "Klout, BuzzSumo, FollowerWonk & Birdsong Analytics - more Free Systems", "Alltop - Free Content Analysis Tool", "Social Monitoring", "Radian6", "Analyzing YOUR markets using Watson Analytics for Social Media [WASM] 1", "Analyzing YOUR markets using Watson Analytics for Social Media [WASM] 2", "Lexalytics Links", "Social Marketing Toolkit Pt. 1b - Lexalytics Worksheets", "Social Marketing Toolkit Pt. 1c - Analyzing Content", "Quiz 2"], "title": "Big Information"}, {"description": "In this module, you will learn how data is transformed into actionable insights for your social marketing programs. ", "video": ["Social Analytics", "Transforming Data to Information", "IBM Bluemix", "Question for Discussion", "New Bitly Instructions", "IBM Bluemix Insights from Twitter discontinued", "IBM Bluemix Personality Insights", "IBM Bluemix Visual Recognition System", "IBM Bluemix Tone Analyzer", "IBM Bluemix - Celebrity Analyzer", "Peer Review Assignment Step by Step", "Hearing Your Target Market through Social Tools", "Quiz 3", "Hearing Your Target Market through Social Tools: Finding influencers, topics, and communities to develop your professional brand"], "title": "Big Insights"}, {"description": "In this module, you will learn how big data and big insights are being used by global businesses and organizations to drive their content strategies and prevent crises from happening. ", "video": ["A Visit to Taco Bell Headquarters", "How Are Big Insights Being Used Today?", "Engagement Strategies", "Building Social Relationships", "Taco Bell War Room", "Crisis Management", "Social Insights for Content Strategies", "Download the Lecture Video Slides", "What Comes Next?", "Quiz 4"], "title": "Real-Time in Action"}], "title": "The Importance of Listening"}, {"course_info": "About this course: Statistical experiment design and analytics are at the heart of data science.  In this course you will design statistical experiments and analyze the results using modern methods.  You will also explore the common pitfalls in interpreting statistical arguments, especially those associated with big data.  Collectively, this course will help you internalize a core set of practical and effective machine learning methods and concepts, and apply them to solve some real world problems.\n\n  \nLearning Goals: After completing this course, you will be able to:\n1. Design effective experiments and analyze the results\n2. Use resampling methods to make clear and bulletproof statistical arguments without invoking esoteric notation\n3. Explain and apply a core set of classification methods of increasing complexity (rules, trees, random forests), and associated optimization methods (gradient descent and variants)\n4. Explain and apply a set of unsupervised learning concepts and methods\n5. Describe the common idioms of large-scale graph analytics, including structural query, traversals and recursive queries, PageRank, and community detection", "level": null, "package_name": "Data Science at Scale Specialization ", "created_by": "University of Washington", "package_num": "2", "teach_by": [{"name": "Bill Howe", "department": "Scalable Data Analytics"}], "target_audience": null, "rating": "4.1", "week_data": [{"description": "Learn the basics of statistical inference, comparing classical methods with resampling methods that allow you to use a simple program to make a rigorous statistical argument.  Motivate your study with current topics at the foundations of science: publication bias and reproducibility.", "video": ["Appetite Whetting: Bad Science", "Hypothesis Testing", "Significance Tests and P-Values", "Example: Difference of Means", "Deriving the Sampling Distribution", "Shuffle Test for Significance", "Comparing Classical and Resampling Methods", "Bootstrap", "Resampling Caveats", "Outliers and Rank Transformation", "Example: Chi-Squared Test", "Bad Science Revisited: Publication Bias", "Effect Size", "Meta-analysis", "Fraud and Benford's Law", "Intuition for Benford's Law", "Benford's Law Explained Visually", "Multiple Hypothesis Testing: Bonferroni and Sidak Corrections", "Multiple Hypothesis Testing: False Discovery Rate", "Multiple Hypothesis Testing: Benjamini-Hochberg Procedure", "Big Data and Spurious Correlations", "Spurious Correlations: Stock Price Example", "How is Big Data Different?", "Bayesian vs. Frequentist", "Motivation for Bayesian Approaches", "Bayes' Theorem", "Applying Bayes' Theorem", "Naive Bayes: Spam Filtering"], "title": "Practical Statistical Inference"}, {"description": "Follow a tour through the important methods, algorithms, and techniques in machine learning.  You will learn how these methods build upon each other and can be combined into practical algorithms that perform well on a variety of tasks.  Learn how to evaluate machine learning methods and the pitfalls to avoid.", "video": ["Statistics vs. Machine Learning", "Simple Examples", "Structure of a Machine Learning Problem", "Classification with Simple Rules", "Learning Rules", "Rules: Sequential Covering", "Rules Recap", "From Rules to Trees", "Entropy", "Measuring Entropy", "Using Information Gain to Build Trees", "Building Trees: ID3 Algorithm", "Building Trees: C.45 Algorithm", "Rules and Trees Recap", "Overfitting", "Evaluation: Leave One Out Cross Validation", "Evaluation: Accuracy and ROC Curves", "Bootstrap Revisited", "Ensembles, Bagging, Boosting", "Boosting Walkthrough", "Random Forests", "Random Forests: Variable Importance", "Summary: Trees and Forests", "Nearest Neighbor", "Nearest Neighbor: Similarity Functions", "Nearest Neighbor: Curse of Dimensionality", "R Assignment: Classification of Ocean Microbes", "R Assignment: Classification of Ocean Microbes"], "title": "Supervised Learning"}, {"description": "You will learn how to optimize a cost function using gradient descent, including popular variants that use randomization and parallelization to improve performance.  You will gain an intuition for popular methods used in practice and see how similar they are fundamentally. ", "video": ["Optimization by Gradient Descent", "Gradient Descent Visually", "Gradient Descent in Detail", "Gradient Descent: Questions to Consider", "Intuition for Logistic Regression", "Intuition for Support Vector Machines", "Support Vector Machine Example", "Intuition for Regularization", "Intuition for LASSO and Ridge Regression", "Stochastic and Batched Gradient Descent", "Parallelizing Gradient Descent"], "title": "Optimization"}, {"description": "A brief tour of selected unsupervised learning methods and an opportunity to apply techniques in practice on a real world problem.", "video": ["Introduction to Unsupervised Learning", "K-means", "DBSCAN", "DBSCAN Variable Density and Parallel Algorithms", "Kaggle Competition Peer Review"], "title": "Unsupervised Learning"}], "title": "Practical Predictive Analytics: Models and Methods"}, {"course_info": "About this course: This four-module course introduces users to Julia as a first language.  Julia is a high-level, high-performance dynamic programming language developed specifically for scientific computing. This language will be particularly useful for applications in physics, chemistry, astronomy, engineering, data science, bioinformatics and many more. As open source software, you will always have it available throughout your working life. It can also be used from the command line, program files or a new type of interface known as a Jupyter notebook (which is freely available as a service from JuliaBox.com).\n\nJulia is designed to address the requirements of high-performance numerical and scientific computing while also being effective for general-purpose programming. You will be able to access all the available processors and memory, scrape data from anywhere on the web, and have it always accessible through any device you care to use as long as it has a browser.  Join us to discover new computing possibilities. Let's get started on learning Julia.\n\nBy the end of the course you will be able to:\n- Programme using the Julia language by practising through assignments\n- Write your own simple Julia programs from scratch\n- Understand the advantages and capacities of Julia as a computing language\n- Work in Jupyter notebooks using the Julia language\n- Use various Julia packages such as  Plots, DataFrames and Stats\n\nThe course is delivered through video lectures, on-screen demonstrations, quizzes and practical peer-reviewed projects designed to give you an opportunity to work with the packages.", "level": "Beginner", "package_name": null, "created_by": "University of Cape Town", "package_num": null, "teach_by": [{"name": "Juan H Klopper", "department": "Department of Surgery "}, {"name": "Henri Laurie", "department": "Department of Mathematics and Applied Mathematics"}], "target_audience": "Who is this class for: This course is for anyone wanting to learn how to use Julia for data analysis. This includes data scientists, engineers, mathematical modelling and students looking for new tools to work with data. \n\nRecommended Background\nA knowledge of high school mathematics is a basic requirement. While the class is designed for students with limited programming experience, some beginner programmers may find the class quite fast-paced. We recommend you work through the material slower, never forgetting that the only way to learn a language is to use it. We use Jupyter notebooks for the course and we encourage you to experiment with the notebooks we have created. \n\nFor those of you who have done programming and want to get a taste of this language, you should be able to move through the material fast.", "rating": "4.6", "week_data": [{"description": "A warm welcome to Julia Scientific Programming. Over the next four weeks, we will provide you with an introduction to what Julia can offer. We have created a course which we hope will allow you to learn the basics of the language, and stimulate your imagination about how you can use Julia in your own context. This course is all about you exploring Julia - we can only demonstrate some of the capacity and encourage you to take the first steps. For those of you with a programming background, the course is intended to offer a jumpstart into using this language. If you are a novice or beginner programmer, you should follow along the simple coding but recognising that working through the material will not be sufficient to make you a proficient programmer in four weeks. You could see this as the ‘first date’ at the beginning of a long and beautiful new relationship. There is so much you will need to learn and discover. Good luck and we hope you enjoy the course! Best wishes, Henri and Juan", "video": ["Introduction to Julia scientific programming", "How this course works", "Is this course right for me?", "What to expect from Week 1", "Meet and greet ", "Programming Languages and why Julia is special", "Getting Ready: JuliaBox Part 1", "Using Jupyter Notebooks", "Getting Ready: JuliaBox Part 2", "The Julia REPL - Read, Evaluate and Print Loop", "JuliaBox and the Julia REPL", "Arithmetic Expressions", "Logical expressions", "Arithmetic and logical expressions in Julia", "Types: Julia Type System", "Arrays and Abstract types", "Types and Arrays in Julia", "Functions I - built-in functions", "Functions II - user-defined functions", "Julia functions", "Week 1: Getting Practice", "Approach to assessment in course", "How to Install Julia on Mac OS X", "Installing Julia on Linux", "Installing Julia on Windows", "Opening notebooks in IJulia", "What makes Julia special?", "Week 1 Graded Quiz"], "title": "Welcome to the course"}, {"description": "In our case study we use Julia to store, plot, select and slice data from the Ebola epidemic. Taking real data, we explain how to work in Julia using arrays, and for loops to work with the structures. By the end of this module, you will be able to: create an array from data;  learn to use the logical structures IF  and FOR ; conduct basic array slicing, getting the incidence data and generating total number of cases; use Plots to generate graphs and plot data; and combine the Ebola data outputs to show a plot of disease incidence in several countries.", "video": ["What to expect from Week 2", "Introduction to Week 2", "The Ebola Epidemic of 2014", "Loading data", "Creating CSV files (Optional)", "\"For\" Loops and data format conversions", "Data and Loops in Julia", "Simple Plots with the Plots Package", "Plotting Multiple Curves in a Single Diagram", "Plots in Julia", "Week 2: Getting Practice", "How to do a Peer Graded Assignment", "Creating a Notebook to describe a function (Optional)", "Week 2 - Graded quiz"], "title": "A context for exploring Julia: Working with data"}, {"description": "in this week, we demonstrate how it is possible to use Julia in the notebook environment to interpret a model and its fit to the data from the Ebola outbreak. For this, we apply the well-known SIR compartmental model in epidemiology. The SIR model labels three compartments, namely S = number susceptible, I =number infectious, and R =number recovered. By the end of this module, you will be able to: understand the SIR models; describe the basic parameters of an SIR model; plot the model-predicted curve and the data on the same diagram; adjust the parameters of the model so the model-predicted curve is close (or rather as close as you can make it) to the data.", "video": ["What to expect from Week 3", "Introduction to Week 3", "SIR Models of Disease Dynamics", "More on SIR Models", "Making simple models", "Plotting Data", "Using the Data: A rough fit of the model parameters", "Models", "Week 3 Getting practice", "Practicing fitting a circle to data", "Week 3 Wrap Up", "User Defined Types: Introduction", "User Defined Types: The Julia Type System & Declaring a Type", "User Defined Types: Creating Your Own Type", "User Defined Types: Conversion & Promotion & Parametrizing a Type", "User Defined Types: Equality of Values & Types & Defining Methods for User Types", "User Defined Types: More Complex Parameters", "User Defined Types: Screen Output of User Defined Types", "User Defined Types: Constraining Field Values", "Plotting data and fitting a curve", "User-defined types (Honors)"], "title": "Notebooks as Julia Programs"}, {"description": "As a scientific computing language, Julia is well suited to the task of working with data.  In this last module, we elaborate on the two most important concepts in Julia, arrays and functions.  They are the fundamental building blocks of holding and manipulating data.  You should see this week as offering you a chance to further explore concepts introduced in week one and two. You will also be introduced to more efficient ways of managing and visualizing your data.  By the end of this module, you will be able to:  1. Apply and understand how to work with arrays  2. Practice Julia functions 3. Explore extension packages 4. be familiar with the  Dataframes  package 5. Plot a variety of data from the dataset, ready for publication.", "video": ["What to expect from week 4", "Packages - Local installation of Julia vs. juliabox.com", "Collections: Introduction", "Collections: Creating Arrays", "Collections: Slicing & Modifying Arrays", "Collections: Comprehensions & Operations", "Collections: Additional - NA & Tuples", "Collections: Additional - Dictionaries", "Collections: Recap", "Collections", "Functions: Introduction", "Functions: Single & Multiple expression functions", "Functions: Optional & Keyword Arguments", "Functions: Variable Number of Arguments", "Functions: Additional Passing Arrays as Arguments & Type Parameters", "Functions: Additional - Stabby Functions & Passing Functions as Arguments", "Functions: Recap", "Functions", "Data Frames: Introduction", "Data Frames: Data Arrays & Data Frames", "Data Frames: Get to Know your Data", "Data Frames: Importing & Exporting", "Data Frames: Joins & Groups", "Data Frames: Sorting, Duplicates & NA", "Data Frames: Renaming Columns", "Data Frames: Recap", "Data: Introduction", "Data Vizualization: Randn & Distributions", "DataFrames and Data Visualizations", "Completing the course", "Data Vizualization: Comparison", "Data Vizualization: Plotly", "Data Vizualization: Recap", "Gadfly: Introduction", "Gadfly: Simple plotting & Adding Layers", "Gadfly: Using Themes", "Gadfly: Adding Titles and Axis Labels & Saving a Plot & Importing Data into a DataFrame", "Gadfly: Box Plots", "Gadfly: Density Plots & Histogram & Violin Plots", "Gadfly: QQ Plots & Scatter Plots & Vertical and Horizontal Lines", "Gadfly: More Examples", "Working with Distributions and DataFrames", "Working with data (Honors)", "Gadfly (Honors)"], "title": "Structuring data and functions in Julia"}], "title": "Julia Scientific Programming"}, {"course_info": "About this course: You may never be sure whether you have an effective user experience until you have tested it with users. In this course, you’ll learn how to design user-centered experiments, how to run such experiments, and how to analyze data from these experiments in order to evaluate and validate user experiences. You will work through real-world examples of experiments from the fields of UX, IxD, and HCI, understanding issues in experiment design and analysis. You will analyze multiple data sets using recipes given to you in the R statistical programming language -- no prior programming experience is assumed or required, but you will be required to read, understand, and modify code snippets provided to you. By the end of the course, you will be able to knowledgeably design, run, and analyze your own experiments that give statistical weight to your designs.", "level": "Intermediate", "package_name": "Interaction Design Specialization ", "created_by": "University of California, San Diego", "package_num": "7", "teach_by": [{"name": "Scott  Klemmer", "department": "Cognitive Science & Computer Science"}, {"name": "Jacob O. Wobbrock", "department": "The Information School"}], "target_audience": "Who is this class for: This class is at the level of advanced undergraduates or first-year graduate students in fields related to human-computer interaction, computer science, interaction design, user experience design, user-centered design, or human-technology behavior studies.", "rating": "3.6", "week_data": [{"description": "In this module, you will learn basic concepts relevant to the design and analysis of experiments, including mean comparisons, variance, statistical significance, practical significance, sampling, inclusion and exclusion criteria, and informed consent. You’ll also learn to think of an experiment in terms of its participants, apparatus, procedure, and design & analysis. This module covers lecture videos 1-2.", "video": ["01. What You Will Learn in this Course", "All Course Materials", "02. Basic Experiment Design Concepts", "Understanding the Basics"], "title": "Basic Experiment Design Concepts"}, {"description": "In this module, you will learn how to analyze user preferences (or other tallies) using tests of proportions. You will also get up and running with R and RStudio. Topics covered include independent and dependent variables, variable types, exploratory data analysis, p-values, asymptotic tests, exact tests, one-sample tests, two-sample tests, Chi-Square test, G-test, Fisher’s exact test, binomial test, multinomial test, post hoc tests, and pairwise comparisons. This module covers lecture videos 3-9.", "video": ["03. Description of a Study of User Preferences", "04. Getting Started with R and RStudio", "05. Exploring Data and a First Test of Proportions", "06. Understanding and Reporting Your First Statistical Test", "07. Exact Tests, Asymptotic Tests, and the Binomial Test", "08. More One-Sample Tests of Proportions", "09. Two-Sample Tests of Proportions", "Understanding Tests of Proportions", "Doing Tests of Proportions"], "title": "Tests of Proportions"}, {"description": "In this module, you will learn how to design and analyze a simple website A/B test. Topics include measurement error, independent variables as factors, factor levels, between-subjects factors, within-subjects factors, dependent variables as responses, response types, balanced designs, and how to report a t-test. You will perform your first analysis of variance in the form of an independent-samples t-test. This module covers lecture videos 10-11.", "video": ["10. Experiment Design Concepts in a Simple A/B Test", "11. Analyzing a Simple A/B Test with a T-Test", "Understanding Experiment Designs", "Doing Independent-Samples T-Tests"], "title": "The T-Test"}, {"description": "In this module, you will learn about how to ensure that your data is valid through the design of experiments, and that your analyses are valid by understanding and testing for their assumptions. Topics include how to achieve experimental control, confounds, ecological validity, the three assumptions of ANOVA, data distributions, residuals, normality, homoscedasticity, parametric versus nonparametric tests, the Shapiro-Wilk test, the Kolmogorov-Smirnov test, Levene’s test, the Brown-Forsythe test, and the Mann-Whitney U test. This module covers lecture videos 12-15.", "video": ["12. Designing for Experimental Control", "13. Data Assumptions and Distributions", "14. Testing for ANOVA Assumptions", "15. Mann-Whitney, a Nonparametric T-Test", "Understanding Validity", "Doing Tests of Assumptions"], "title": "Validity in Design and Analysis"}, {"description": "In this module, you will learn about one-factor between-subjects experiments. The experiment examined will be a between-subjects study of task completion time with various programming tools. You will understand and analyze data from two-level factors and three-level factors using the independent-samples t-test, Mann-Whitney U test, one-way ANOVA, and Kruskal-Wallis test. You will learn how to report an F-test. You will also understand omnibus tests and how they relate to post hoc pairwise comparisons with adjustments for multiple comparisons. This module covers lecture videos 16-18.", "video": ["16. Description of a Study for a Oneway ANOVA", "17. Analyzing and Reporting a Oneway ANOVA", "18. Kruskal-Wallis, a Nonparametric Oneway ANOVA", "Understanding Oneway Designs", "Doing Oneway ANOVAs"], "title": "One-Factor Between-Subjects Experiments"}, {"description": "In this module, you will learn about one-factor within-subjects experiments, also known as repeated measures designs. The experiment examined will be a within-subjects study of subjects searching for contacts in a smartphone contacts manager, including the analysis of times, errors, and effort Likert-type scale ratings. You will learn counterbalancing strategies to avoid carryover effects, including full counterbalancing, Latin Squares, and balanced Latin Squares. You will understand and analyze data from two-level factors and three-level factors using the paired-samples t-test, Wilcoxon signed-rank test, oneway repeated measures ANOVA, and Friedman test. This module covers lecture videos 19-23.", "video": ["19. Description of a Study for a Oneway Repeated Measures ANOVA", "20. Counterbalancing Repeated Measures Factors", "21. Long-Format and Wide-Format Data Tables", "22. The Paired T-Test and Wilcoxon Signed-Rank Test", "23. Analyzing a Repeated Measures ANOVA and Friedman Test", "Understanding Oneway Repeated Measures Designs", "Doing Oneway Repeated Measures ANOVAs"], "title": "One-Factor Within-Subjects Experiments"}, {"description": "In this module, you will learn about experiments with multiple factors and factorial ANOVAs. The experiment examined will be text entry performance on different smartphone keyboards while sitting, standing, and walking. Topics include mixed factorial designs, interaction effects, factorial ANOVAs, and the Aligned Rank Transform as a nonparametric factorial ANOVA. This module covers lecture videos 24-27.", "video": ["24. Description of a Study for a Factorial ANOVA", "25. Understanding Interaction Effects", "26. Analyzing a Factorial ANOVA", "27. The ART, a Nonparametric Factorial ANOVA", "Understanding Factorial Designs", "Doing Factorial ANOVAs"], "title": "Factorial Experiment Designs"}, {"description": "In this module, you will learn about analyses for non-normal or non-numeric responses for between-subjects experiments using Generalized Linear Models (GLM). We will revisit three previous experiments and analyze them using generalized models. Topics include a review of response distributions, nominal logistic regression, ordinal logistic regression, and Poisson regression. This module covers lecture videos 28-29.", "video": ["28. Introduction to Generalized Linear Models", "29. Analyzing Three Generalized Linear Models", "Understanding Generalized Linear Models", "Doing Generalized Linear Models"], "title": "Generalizing the Response"}, {"description": "In this module, you will learn about mixed effects models, specifically Linear Mixed Models (LMM) and Generalized Linear Mixed Models (GLMM). We will revisit our prior experiment on text entry performance on smartphones but this time, keeping every single measurement trial as part of the analysis. The full set of analyses covered in this course will also be reviewed. This module covers lecture videos 30-33.", "video": ["30. Introduction to Mixed Effects Models", "31. Analyzing a Linear Mixed Model", "32. Analyzing a Generalized Linear Mixed Model", "33. Course in Review", "Understanding Mixed Effects Models", "Doing Mixed Effects Models"], "title": "The Power of Mixed Effects Models"}], "title": "Designing, Running, and Analyzing Experiments"}, {"course_info": "About this course: This class provides an introduction to the Python programming language and the iPython notebook. This is the third course in the Genomic Big Data Science Specialization from Johns Hopkins University.", "level": null, "package_name": "Genomic Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "3", "teach_by": [{"name": "Mihaela Pertea, PhD", "department": "Center for Computational Biology"}, {"name": "Steven Salzberg, PhD", "department": "Biomedical Engineering, Computer Science, and Biostatistics"}], "target_audience": null, "rating": "4.3", "week_data": [{"description": "This week we will have an overview of Python and take the first steps towards programming.", "video": ["Welcome", "Pre-Course Survey", "Syllabus", "Lecture 1: Overview of Python", "Lecture 2.1 - First Steps Toward Programming Part 1", "Lecture 2.2 - First Steps Toward Programming Part 2", "Lecture 2.3 - First Steps Toward Programming Part 3 (8:57)", "Lecture 2.4 - First Steps Toward Programming Part 4 (9:58)", "Lecture 1 Quiz", "Lecture 2 Quiz"], "title": "Week One"}, {"description": "In this module, we'll be taking a look at Data Structures and Ifs and Loops.", "video": ["Lecture 3.1: Data Structures Part 1 (11:58)", "Lecture 3.2: Data Structures Part 2 (10:41)", "Lecture 4.1: Ifs and Loops Part 1 (11:26)", "Lecture 4.2: Ifs and Loops Part 2 (15:28)", "Lecture 3 Quiz", "Lecture 4 Quiz"], "title": "Week Two"}, {"description": "In this module, we have a long three-part lecture on Functions as well as a 10-minute look at Modules and Packages.", "video": ["Lecture 5.1: Functions Part 1 (5:54)", "Lecture 5.2: Functions Part 2 (8:20)", "Lecture 5.3: Functions Part 3 (13:24)", "Lecture 6: Modules and Packages (10:32)", "Lecture 5 Quiz", "Lecture 6 Quiz"], "title": "Week Three"}, {"description": "In this module, we have another long three-part lecture, this time about Communicating with the Outside, as well as a final lecture about Biopython.", "video": ["Lecture 7.1: Communicating with the Outside Part 1 (6:41)", "Lecture 7.2: Communicating with the Outside Part 2 (7:38)", "Lecture 7.3: Communicating with the Outside Part 3 (17:42)", "Lecture 8: Biopython (13:32)", "Final Exam Instructions", "Post Course Survey", "Lecture 7 Quiz", "Lecture 8 Quiz", "Final Exam (Read Instructions First)"], "title": "Week Four"}], "title": "Python for Genomic Data Science"}, {"course_info": "About this course: Are you trying to understand data from your research? Learn how and when to conduct mediation, moderation, and conditional indirect effects analyses? Or, perhaps, how to theorize and test your theoretical models? If so, this is the course for you! We will walk you through the steps of conducting multilevel analyses using a real dataset and provide articles and templates designed to facilitate your learning. You'll leave with the tools you need to analyze and interpret the results of the datasets you collect as a researcher. \n\nBy the end of this course, you will understand the differences between mediation and moderation and between moderated mediation and mediated moderation models (conditional indirect effects), and the importance of multilevel analysis. Most important, you will be able to run mediation, moderation, conditional indirect effect and multilevel models and interpret the results.\n\nThis course is supported by the BRAD Lab at the Darden School of Business, which studies organizational behavior, marketing, business ethics, judgment and decision-making, behavioral operations, and entrepreneurship, among other areas. More: http://www.darden.virginia.edu/brad-lab/", "level": "Beginner", "package_name": null, "created_by": "University of Virginia", "package_num": null, "teach_by": [{"name": "Cristiano Guarana", "department": "Darden School of Business"}], "target_audience": "Who is this class for: This course is designed for first or second year PhD students and people interested in research methods. Prior SPSS knowledge is a plus, but we will walk you through all the steps when using SPSS to run our analyses.", "rating": "4.5", "week_data": [{"description": "Welcome to the first week of our research methods course! We'll start with mediation analysis, following by parallel mediation, serial mediation, and moderation. Mediation is all about the mechanisms connecting the independent variable and dependent variable. Moderation refers to the circumstances under which the independent variable influences the dependent variable. By the end of this week, you will know how, when, and where the independent variable influences the dependent variable and how to theorize and conduct analysis using SPSS.", "video": ["Course Overview", "Course Overview and Requirements", "Say hello!", "What Is Mediation?", "Introduction to SPSS", "Running a Mediation Model", "Conducting a Parallel Mediation Analysis", "Conducting Serial Mediation Analysis", "Moderation Overview", "Conducting Moderation Analysis: Understanding the Outputs", "Conducting Moderation Analysis: Plotting Interactions", "Week 1 Quiz on Mediation and Moderation"], "title": "Mediation and Moderation"}, {"description": "Now that you know more about mediation and moderation, let's take a look at conditional indirect effects models, which are a combination of mediation and moderation models. First we will get to the heart of the differences between moderated mediation and mediated moderation models.This will allow you to fully understand the relationships between independent variables and dependent variables. By the end of the module, you will be able to theorize about conditional indirect effect models on SPSS and to test which path of the mediation model is affected by the moderator. Then you'll dive into SPSS, run different models, and learn how to interpret the results. \n", "video": ["Conditional Indirect Effects: Mediated Moderation Models", "Testing Mediated Moderation Models: Steps 1-2", "Testing Mediated Moderation Models: Steps 3-5", "Moderated Mediation Models Overview", "Testing First Stage Moderated Mediation Models", "Second Stage Moderated Mediation Models Overview", "Testing Second Stage Moderated Mediation Models", "Dual Stage Moderated Mediation Models Overview", "Testing Dual Stage Moderated Mediation Models", "Questions?", "Week 2 Quiz on Conditional Indirect Effects"], "title": "Conditional Indirect Effects"}, {"description": "Now that you know how to run mediation, moderation, and conditional indirect effect analyses, we can turn our attention to multilevel models. Multilevel models are statistical models of parameters that vary at more than one level. Think about employees nested in departments, or departments nested in firms. You will learn the importance of multilevel analysis to your research and get familiar with multilevel analysis language. By the end of this module, you will be able to use HLM software to run multilevel models and interpret the results.", "video": ["Multilevel Models Overview", "Multilevel Models Technical Elements", "Conducting Multilevel Analysis and Centering", "Conducting Multilevel Analysis and Interpreting Outputs", "Your research?", "Week 3 Quiz on Multilevel Analysis"], "title": "Multilevel Analysis"}], "title": "Understanding Your Data: Analytical Tools"}, {"course_info": "About this course: Welcome to the second course in the Data Analytics for Business specialization! \n\nThis course will introduce you to some of the most widely used predictive modeling techniques and their core principles. By taking this course, you will form a solid foundation of predictive analytics, which refers to tools and techniques for building statistical or machine learning models to make predictions based on data. You will learn how to carry out exploratory data analysis to gain insights and prepare data for predictive modeling, an essential skill valued in the business. \n\nYou’ll also learn how to summarize and visualize datasets using plots so that you can present your results in a compelling and meaningful way. We will use a practical predictive modeling software, XLMiner, which is a popular Excel plug-in. This course is designed for anyone who is interested in using data to gain insights and make better business decisions. The techniques discussed are applied in all functional areas within business organizations including accounting, finance, human resource management, marketing, operations, and strategic planning. \n\nThe expected prerequisites for this course include a prior working knowledge of Excel, introductory level algebra, and basic statistics.", "level": null, "package_name": "Advanced Business Analytics Specialization ", "created_by": "University of Colorado Boulder", "package_num": "2", "teach_by": [{"name": "Dan Zhang", "department": "Leeds School of Business"}], "target_audience": "Who is this class for: This course is designed for anyone who is interested in using data to gain insights and make better business decisions. The techniques discussed are applied in all functional areas within business organizations including accounting, finance, human resource management, marketing, operations, and strategic planning.\n \nThis course is primarily aimed at professionals who have a bachelor’s degree and/or some exposure to the business world.  The software tool used in the course is based on Microsoft Excel and therefore is accessible for anyone with some prior exposure to Excel. Those with technical degrees or more advanced business degrees like an MBA will find certain areas easier to absorb, and may get maximum value from the course.  However, even undergraduates in non-technical fields or advanced high-school students pursuing internships will be able to follow most concepts and get value from the course.  Finally, even professionals who have had deep experiences in methods will likely find value in this course.", "rating": "3.7", "week_data": [{"description": "At the end of this module students will be able to: 1. Carry out exploratory data analysis to gain insights and prepare data for predictive modeling 2. Summarize and visualize datasets using appropriate tools 3. Identify modeling techniques for prediction of continuous and discrete outcomes. 4. Explore datasets using Excel 5. Explain and perform several common data preprocessing steps         6. Choose appropriate graphs to explore and display datasets  ", "video": ["Introduction to the Course", "0. Introduction to the Module. Why Exploratory Data Analysis is Important", "1. Data Cleanup and Transformation", "2. Dealing With Missing Values", "3. Dealing with Outliers", "4. Adding and Removing Variables", "5. Common Graphs", "6. What is Good Data Visualization?", "Data Exploration ", "Register for Analytic Solver Platform for Education (ASPE)", "Week 1 Application Assignment 1 (optional): Data Cleanup", "Week 1 Quiz", "Week 1 Application Assignment 2: Data Visualization"], "title": " Exploratory Data Analysis and Visualizations"}, {"description": "This module introduces regression techniques to predict the value of continuous variables. Some fundamental concepts of predictive modeling are covered, including cross-validation, model selection, and overfitting. You will also learn how to build predictive models using the software tool XLMiner.", "video": ["0. Introduction to Predictive Modeling", "1. Introduction to Linear Regression", "2. Assessing Predictive Accuracy Using Cross-Validation", "3. Multiple Regression", "4. Improving Model Fit", "5. Model Selection", "6. Challenges of Predictive Modeling", "7. How to Build a Model using XLMiner", "Reflection on Statistical Techniques", "Week 2 Quiz", "Week 2 Application Assignment"], "title": "Predicting a Continuous Variable"}, {"description": "This module introduces logistic regression models to predict the value of binary variables. Unlike continuous variables, a binary variable can only take two different values and predicting its value is commonly called classification. Several important concepts regarding classification are discussed, including cross validation and confusion matrix, cost sensitive classification, and ROC curves. You will also learn how to build classification models using the software tool XLMiner.", "video": ["0. Introduction to classification", "1. Introduction to Logistic Regression", "2. Building Logistic Regression Model", "3. Multiple Logistic Regression", "4. Cross Validation and Confusion Matrix", "5. Cost Sensitive Classification", "6. Comparing Models Independent of Costs and Cutoffs", "7. Building Logistic Regression Models using XLMiner", "The Best Prediction Method ", "Week 3 Quiz", "Week 3 Application Assignment"], "title": "Predicting a Binary Outcome"}, {"description": "This module introduces more advanced predictive models, including trees and neural networks. Both trees and neural networks can be used to predict continuous or binary variables. You will also learn how to build trees and neural networks using the software tool XLMiner.", "video": ["0.Introduction to Advanced Predictive Modeling Techniques", "1. Introduction to Trees", "2. Classification Trees", "3. Regression Trees", "4. Bagging, Boosting, Random Forest", "5. Neural Networks", "6. Building Trees with XLMiner", "7. Building Neural Networks using XLMiner", "Reflection: Trees & Neural Networks ", "Final Course Assignment Quiz", "Week 4 Quiz", "Week 4 Application Assignment", "Final Course Assignment Peer Review"], "title": "Trees and Other Predictive Models"}], "title": "Predictive Modeling and Analytics "}, {"course_info": "About this course: Regression Analysis is perhaps the single most important Business Statistics tool used in the industry. Regression is the engine behind a multitude of data analytics applications used for many forms of forecasting and prediction.  \nThis is the fourth course in the specialization, \"Business Statistics and Analysis\". The course  introduces you to the very important tool known as Linear Regression. You will learn to apply various procedures such as dummy variable regressions, transforming variables, and interaction effects. All these are introduced and explained using easy to understand examples in Microsoft Excel.\nThe focus of the course is on understanding and application, rather than detailed mathematical derivations.\nNote: This course uses the ‘Data Analysis’ tool box which is standard with the Windows version of Microsoft Excel. It is also standard with the 2016 or later Mac version of Excel. However, it is not standard with earlier versions of Excel for Mac. \n\n\nWEEK 1\nModule 1: Regression Analysis: An Introduction\nIn this module you will get introduced to the Linear Regression Model. We will build a regression model and estimate it using Excel. We will use the estimated model to infer relationships between various variables and use the model to make predictions. The module also introduces the notion of errors, residuals and R-square in a regression model.\n\nTopics covered include:\n•\tIntroducing the Linear Regression\n•\tBuilding a Regression Model and estimating it using Excel\n•\tMaking inferences using the estimated model\n•\tUsing the Regression model to make predictions\n•\tErrors, Residuals and R-square\n \n\nWEEK 2\nModule 2: Regression Analysis: Hypothesis Testing and Goodness of Fit\nThis module presents different hypothesis tests you could do using the Regression output. These tests are an important part of inference and the module introduces them using Excel based examples. The p-values are introduced along with goodness of fit measures R-square and the adjusted R-square. Towards the end of module we introduce the ‘Dummy variable regression’ which is used to incorporate categorical variables in a regression. \n\nTopics covered include:\n•\tHypothesis testing in a Linear Regression\n•\t‘Goodness of Fit’ measures (R-square, adjusted R-square)\n•\tDummy variable Regression (using Categorical variables in a Regression)\n \n\nWEEK 3\nModule 3: Regression Analysis: Dummy Variables, Multicollinearity\nThis module continues with the application of Dummy variable Regression. You get to understand the interpretation of Regression output in the presence of categorical variables. Examples are worked out to re-inforce various concepts introduced. The module also explains what is Multicollinearity and how to deal with it. \n\nTopics covered include:\n•\tDummy variable Regression (using Categorical variables in a Regression)\n•\tInterpretation of coefficients and p-values in the presence of Dummy variables\n•\tMulticollinearity in Regression Models\n \n\nWEEK 4\nModule 4: Regression Analysis: Various Extensions\nThe module extends your understanding of the Linear Regression, introducing techniques such as mean-centering of variables and building confidence bounds for predictions using the Regression model. A powerful regression extension known as ‘Interaction variables’ is introduced and explained using examples. We also study the transformation of variables in a regression and in that context introduce the log-log and the semi-log regression models. \n\nTopics covered include:\n•\tMean centering of variables in a Regression model\n•\tBuilding confidence bounds for predictions using a Regression model\n•\tInteraction effects in a Regression\n•\tTransformation of variables\n•\tThe log-log and semi-log regression models", "level": null, "package_name": "Business Statistics and Analysis Specialization ", "created_by": "Rice University", "package_num": "4", "teach_by": [{"name": "Sharad Borle", "department": "Jones Graduate School of Business"}], "target_audience": null, "rating": "4.8", "week_data": [{"description": "", "video": ["Meet the Professor", "Course FAQs", "Pre-Course Survey", "Introducing Linear Regression: Building a Model", "Toy Sales.xlsx", "Slides, Lesson 1", "Practice Quiz", "Introducing Linear Regression: Estimating the Model", "Toy Sales.xlsx", "Slides, Lesson 2", "Practice Quiz", "Introducing Linear Regression: Estimating the Model", "Toy Sales.xlsx", "Slides, Lesson 3", "Practice Quiz", "Introducing Linear Regression: Predictions using the Model", "Toy Sales.xlsx", "Slides, Lesson 4", "Practice Quiz", "Errors, Residuals and R-square", "Toy Sales2.xlsx", "Slides, Lesson 5", "Practice Quiz", "Normality Assumption on the Errors", "Slides, Lesson 6", "Practice Quiz", "Regression Analysis: An Introduction"], "title": "Regression Analysis: An Introduction"}, {"description": "", "video": ["Hypothesis Testing in a Linear Regression", "Toy Sales.xlsx", "Toy Sales (with regression).xlsx", "Toy Sales (with regression, t-statistic).xlsx", "Toy Sales (with regression, t-cutoff)", "Slides, Lesson 1", "Practice Quiz", "Hypothesis Testing in a Linear Regression: using 'p-values'", "Toy Sales.xlsx", "Slides, Lesson 2", "Practice Quiz", "Hypothesis Testing in a Linear Regression: Confidence Intervals", "Toy Sales.xlsx", "Slides, Lesson 3", "Practice Quiz", "A Regression Application Using Housing Data", "Home Prices.xlsx", "Slides, Lesson 4", "Practice Quiz", "'Goodness of Fit' measures: R-square and Adjusted R-square", "Home Prices.xlsx", "Slides, Lesson 5", "Practice Quiz", "Categorical Variables in a Regression: Dummy Variables", "deliveries1.xlsx", "Slides, Lesson 6", "Practice Quiz", "Regression Analysis: Hypothesis Testing and Goodness of Fit"], "title": "Regression Analysis: Hypothesis Testing and Goodness of Fit"}, {"description": "", "video": ["Dummy Variable Regression: Extension to Multiple Categories", "deliveries2.xlsx", "Slides, Lesson 1", "Practice Quiz", "Dummy Variable Regression: Interpretation of Coefficients", "Slides, Lesson 2", "Practice Quiz", "Dummy Variable Regression: Estimation, Interpretation of p-values", "deliveries2.xlsx", "deliveries2 (for prediction).xlsx", "Slides, Lesson 3", "Practice Quiz", "A Regression Application Using Refrigerator data", "Refrigerators.xlsx", "Slides, Lesson 4", "Practice Quiz", "A Regression Application Using Refrigerator data (continued...)", "Cars.xlsx", "Slides, Lesson 5", "Practice Quiz", "Multicollinearity in Regression Models: What it is and How to Deal with it", "Cars.xlsx", "Slides, Lesson 6", "Practice Quiz", "Regression Analysis: Model Application and Multicollinearity"], "title": "Regression Analysis: Dummy Variables, Multicollinearity"}, {"description": "", "video": ["Mean Centering Variables in a Regression Model", "Height and Weight.xlsx", "Slides, Lesson 1", "Practice Quiz", "Building Confidence Bounds for Prediction Using a Regression Model", "Height and Weight.xlsx", "Slides, Lesson 2", "Practice Quiz", "Interaction Effects in a Regression: An Introduction", "Slides, Lesson 3", "Practice Quiz", "Interaction Effects in a Regression: An Application", "Height and Weight.xlsx", "Slides, Lesson 4", "Practice Quiz", "Transformation of Variables in a Regression: Improving Linearity", "Slides, Lesson 5", "Practice Quiz", "The Log-Log and the Semi-Log Regression Models", "Cocoa.xlsx", "Slides, Lesson 6", "Practice Quiz", "Course 4 Recap", "End-of-Course Survey", "Regression Analysis: Various Extensions"], "title": "Regression Analysis: Various Extensions"}], "title": "Linear Regression for Business Statistics"}, {"course_info": "About this course: This course provides an unique opportunity for you to learn key components of text mining and analytics aided by the real world datasets and the text mining toolkit written in Java. Hands-on experience in core text mining techniques including text preprocessing, sentiment analysis, and topic modeling help learners be trained to be a competent data scientists. \n\nEmpowered by bringing lecture notes together with lab sessions based on the y-TextMiner toolkit developed for the class, learners will be able to develop interesting text mining applications.", "level": "Intermediate", "package_name": null, "created_by": "Yonsei University", "package_num": null, "teach_by": [{"name": "Min Song", "department": "Library & Information Technology"}], "target_audience": null, "rating": "4.1", "week_data": [{"description": "", "video": ["1.1 Description of the course including the objectives and outcomes", "1.2 Explanations of the y-TextMiner package and the datasets", "1.3 How-to-do: workspace installation and setup", "1.4 How-to-use: the y-TextMiner package (download it at http://informatics.yonsei.ac.kr/yTextMiner/yTextMiner.zip)", "What is Text Mining?", "y-TextMiner installation and a simple Java program"], "title": "Course Logistics and the Text Mining Tool for the Course"}, {"description": "", "video": ["2.1 Description of possible project ideas", "2.2 What is text mining?", "2.3 Description of preprocessing techniques", "2.4 How-to-do: normalization including tokenization and lemmatization", "2.5 How-to-do: N-Grams", "Text Preprocessing", "Preprocessing Practice"], "title": "Text Preprocessing"}, {"description": "", "video": ["3.1 Description of stopword removal, stemming, and POS tagging", "3.2 Explanations of named entity recognition", "3.3 Explanations of dependency parsing", "3.4 How-to-do: stopword removal and stemming", "3.5 How-to-do: NER and POS Tagging", "3.6 How-to-do: constituency and dependency parsing", "Stemming and Lemmatization", "Named Entity Recognition", "Text Analysis Practice"], "title": "Text Analysis Techniques"}, {"description": "", "video": ["4.1 Explanations of TF*IDF", "4.2 Explanations of document classification", "4.3 Explanations of sentiment analysis", "4.4 How-to-do: computation of tf*idf weighting", "4.5 How-to-do: classification with Logistic Regression", "Text Classification", "TF-IDF", "Document Classification Practice"], "title": "Term Weighting and Document Classification"}, {"description": "", "video": ["5.1 Explanations of sentiment analysis with supervised learning", "5.2 Explanations of sentiment analysis with unsupervised learning", "5.3 Explanations of sentiment analysis with CoreNLP, LingPipe and SentiWordNet", "5.4 How-to-do: sentiment analysis with CoreNLP", "5.5 How-to-do: sentiment analysis with LingPipe", "5.6 How-to-do: sentiment analysis with SentiWordNet", "Opinion mining and sentiment analysis by Bo Pang and Lillian Lee", "Sentiment Analysis Practice"], "title": "Sentiment Analysis"}, {"description": "", "video": ["6.1 Description of Topic Modeling", "6.2 Explanations of LDA and DMR", "6.3 Description of Topic Modeling with Mallet", "6.4 How-to-do: LDA", "6.5 How-to-do: DMR", "Introduction to Probabilistic Topic Models by David Blei", "Topic Modeling Practice"], "title": "Topic Modeling"}], "title": "Hands-on Text Mining and Analytics"}, {"course_info": "About this course: Who is this course for?  \nThis course is designed for students, business analysts, and data scientists who want to apply statistical knowledge and techniques to business contexts. For example, it may be suited to experienced statisticians, analysts, engineers who want to move more into a business role. \n\nYou will find this course exciting and rewarding if you already have a background in statistics, can use R or another programming language and are familiar with databases and data analysis techniques such as regression, classification, and clustering.\nHowever, it contains a number of recitals and R Studio tutorials which will consolidate your competences, enable you to play more freely with data and explore new features and statistical functions in R.\n\nWith this course, you’ll have a first overview on Strategic Business Analytics topics. We’ll discuss a wide variety of applications of Business Analytics. From Marketing to Supply Chain or Credit Scoring and HR Analytics, etc. We’ll cover many different data analytics techniques, each time explaining how to be relevant for your business.\n\nWe’ll pay special attention to how you can produce convincing, actionable, and efficient insights. We'll also present you with different data analytics tools to be applied to different types of issues.\nBy doing so, we’ll help you develop four sets of skills needed to leverage value from data: Analytics, IT, Business and Communication. \n\nBy the end of this MOOC, you should be able to approach a business issue using Analytics by (1) qualifying the issue at hand in quantitative terms, (2) conducting relevant data analyses, and (3) presenting your conclusions and recommendations in a business-oriented, actionable and efficient way.\n\nPrerequisites : 1/ Be able to use R or to program 2/ To know the fundamentals of databases, data analysis (regression, classification, clustering)\n\nWe give credit to Pauline Glikman, Albane Gaubert, Elias Abou Khalil-Lanvin (Students at ESSEC BUSINESS SCHOOL) for their contribution to this course design.", "level": null, "package_name": "Strategic Business Analytics Specialization ", "created_by": "ESSEC Business School", "package_num": "1", "teach_by": [{"name": "Nicolas Glady ", "department": "Marketing Department "}], "target_audience": null, "rating": "4.4", "week_data": [{"description": " In this module, we will introduce you to the course and instructional approach. You will learn that Strategic Business Analytics relies on four distinct skills: IT, Analytics, Business and Communication.", "video": ["Welcome to the course", "Becoming a Business Analytics expert", "Why? It is all about value not data", "How to leverage data for value - from data to insight", "Dataset for practice quiz", "Practice Quizz Module 1"], "title": "Introduction to Strategic Business Analytics"}, {"description": "In this module, you will learn how identifying groups of observations enables you to improve business efficiency. You will then learn to create those groups in a business-oriented and actionable way. We will use examples to illustrate various concepts. The assessments will also provide you with opportunities to replicate these examples.", "video": ["Introduction: what’s the point of finding groups within data?", "Basic clustering using ad-hoc techniques: the example of product management", "Identifying groups within data: what's the intuition behind clustering? The example of HR Analytics", "Introduction to Customer Segmentation", "Presentation of Pauline Glikman", "Recital M2 - SKU example", "Recital M2 - HR example", "Recital M2 - Telecom example", "Script and dataset files to replicate recitals", "Wrap-up:  identifying groups within data", "Quiz - Module 2"], "title": "Finding groups within Data"}, {"description": "In this module, you will learn why using rigorous statistical methods to understand the relationship between different events is crucial. \n\nWe’ll cover two examples: first, using a credit scoring example, you will learn how to derive information about what makes an individual more or less likely to have a strong credit score? Then, in a second example drawn from HR Analytics, you will learn to estimate what makes an employee more or less likely to leave the company. \nAs usual, we invite you to replicate those examples thanks to the recital \nand to use the assessments provided at the end of the module to strengthen your understanding of these concepts.", "video": ["Understanding causes and consequences: introduction", "Why use Business Analytics to understand the relationship between causes and consequences", "Understanding what distinguishes two categories", "Beyond the regression estimates: reporting effects in a visual way", "Recital M3 - Credit score example", "Recital M3 - HR example", "Script and dataset files to replicate recitals", "Wrap-up: identifying causes to effects", "Quiz - module 3"], "title": "Factors leading to events"}, {"description": " In this module you will learn more about the importance of forecasting the future.\n\nYou will learn through examples from various sectors: first, using the previous examples of credit scoring and HR Analytics, you will learn to predict what will happen. Then, you will be introduced to predictive maintenance using survival analysis via a case discussion. Finally, we’ll discuss seasonality in the context of the first example discussed in this MOOC: using analytics for managing your supply chain and logistics better. \n", "video": ["Predictions & Forecasting: introduction", "Predicting events: sales, defaults, risks, churn, etc.", "Using classification and regression techniques to forecast", "Predicting when an event will happen with survival analysis", "Introduction to time series and seasonality", "Recital M4 - Credit Score", "Recital M4 - HR example", "Recital M4 - Predictive maintenance example", "Recital M4 - Chocolate Sales example", "Script and dataset files to replicate recitals", "Wrap-up: forecasting events", "Quiz Module 4"], "title": "Predictions and Forecasting"}, {"description": "So far, you’ve learnt to use Business Analytics to glean important information relevant to the success of your business. In this module, you’ll learn more about how to present your Business Analytics work to a business audience. This module is also important for your final capstone project presentation.You’ll learn that it is important to find an angle, and tell a story.Instead of presenting a list of results that are not connected to each other, you will learn to take your audience by the hand and steer it to the recommendations you want to conclude on.You’ll learn to structure your story and your slides, and master the most used visualization tips and tricks. The assessment at the end of this module will provide an opportunity for you to practice these methods and to prepare the first step of the capstone project.", "video": ["Reporting your results: introduction", "It's all about the story", "One slide / One idea", "One picture is worth a thousand words", "Recital M5 - How to present your findings", "Presentation Tips", "Wrap-up: reporting your results", "Datasets for Peer Review Assignment", "Peer Review"], "title": "Recommendation production and prioritization"}], "title": "Foundations of strategic business analytics"}, {"course_info": "About this course: In this course you will learn how to create models for decision making. We will start with cluster analysis, a technique for data reduction that is very useful in market segmentation. You will then learn the basics of Monte Carlo simulation that will help you model the uncertainty that is prevalent in many business decisions. A key element of decision making is to identify the best course of action. Since businesses problems often have too many alternative solutions, you will learn how optimization can help you identify the best option. What is really exciting about this course is that you won’t need to know a computer language or advanced statistics to learn about these predictive and prescriptive analytic models. The Analytic Solver Platform and basic knowledge of Excel is all you’ll need. Learners participating in assignments will be able to get free access to the Analytic Solver Platform.", "level": null, "package_name": "Advanced Business Analytics Specialization ", "created_by": "University of Colorado Boulder", "package_num": "3", "teach_by": [{"name": "Manuel Laguna", "department": "Leeds School of Business"}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "", "video": ["Introduction to the Course", "0. What is Cluster Analysis", "1. Data Reduction and Unsupervised Learning", "2. Preparing Data and Measuring Dissimilarities", "3. Hierarchical and k-Means Clustering", "4. Cluster Analysis with Excel", "5. Cluster Analysis with XLMiner", "Register for Analytic Solver Platform for Education (ASPE)", "Week 1 Discussion: Cluster Analysis in Nilsen Analytics Tools", "Week 1 Quiz", "Week 1 Application Assignment - Clustering"], "title": "Data Exploration and Reduction — Cluster Analysis"}, {"description": "", "video": ["0. Risk Analysis and Monte Carlo Simulation", "1. Adding Uncertainty to a Spreadsheet Model", "2. Defining Output Variables and Analyzing the Results", "3. Using Historical Data to Model Uncertainty", "4. Models with Correlated Uncertain Variables", "5. Creating and Interpreting Charts", "6. Using Average Values versus Simulation", "Week 2 Discussion: How Betterment Holdings Inc. Used Monte Carlo Simulation", "Week 2 Quiz", "Week 2 Application Assignment - Monte Carlo Simulation"], "title": "Dealing with Uncertainty and Analyzing Risk"}, {"description": "At the end of this module students should be able to: 1. Develop a spreadsheet model for an optimization problem 2. Use Excel to solve optimization models 3. Interpret solutions and conduct what-if analysis", "video": ["0. Optimization and Decision Making", "1. Formulating an Optimization Problem", "2. Developing a Spreadsheet Model", "3. Adding Optimization to a Spreadsheet Model", "4. What-if Analysis and the Sensitivity Report", "5. Evaluating Scenarios and Visualizing Results to Gain Practical Insights", "6. Digital Marketing Application of Optimization", "Week 3 Discussion: How Amazon Uses Optimization for Its Supply Chain", "Week 3 Quiz", "Week 3 Application Assignment - Linear Optimization"], "title": "Identifying the Best Options — Optimization"}, {"description": "At the end of this module students should be able to: 1. Given a business situation, apply an appropriate technique to identify the best solution alternatives 2. Formulate and solve models for business problems that requires yes/no decisions and logical constraints 3. Create models that mix techniques and tools such as simulation and optimizationAnalyze and interpret results to make informed decisions", "video": ["0. Advanced Models for Better Decisions", "1. Business Problems with Yes/No Decisions", "2. Formulation and Solution of Binary Optimization Problems", "3. Metaheuristic Optimization", "4. Chance Constraints and Value At Risk", "5. Simulation Optimization", "Week 4 Discussion: How Uber Optimized Its Dispatch System", "Week 4 Quiz", "Week 4 Application Assignment - Simulation Optimization"], "title": "Decision Analytics"}], "title": "Business Analytics for Decision Making"}, {"course_info": "About this course: This is the fourth course in the Data Warehouse for Business Intelligence specialization. Ideally, the courses should be taken in sequence.  In this course, you will gain the knowledge and skills for using data warehouses for business intelligence purposes and for working as a business intelligence developer. You’ll have the opportunity to work with large data sets in a data warehouse environment and will learn the use of MicroStrategy's Online Analytical Processing (OLAP) and Visualization capabilities to create visualizations and dashboards. \n\nThe course gives an overview of how business intelligence technologies can support decision making across any number of business sectors. These technologies have had a profound impact on corporate strategy, performance, and competitiveness and broadly encompass  decision support systems, business intelligence systems, and visual analytics. Modules are organized around the business intelligence concepts, tools, and applications, and the use of data warehouse for business reporting and online analytical processing, for creating visualizations and dashboards, and for business performance management and descriptive analytics.", "level": null, "package_name": "Data Warehousing for Business Intelligence Specialization ", "created_by": "University of Colorado System", "package_num": "4", "teach_by": [{"name": "Jahangir Karimi", "department": "Information Systems University of Colorado Denver"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "Module 1 explains the role of computerized support for decision making and its importance. It starts by identifying the different types of decisions managers face, and the process through which they make decisions. It then focuses on decision making styles, the four stages of Simon’s decision making process, and common strategies and approaches of decision makers. In the next two lessons, you will learn the role of  Decision Support Systems (DSS), understand its main components, the various DSS types and classification, and how DSS have changed over time. Finally, in lesson 4, we focus on how DSS supports each phase of decision making and summarize the evolution of DSS applications, and on how they have changed over time.  I recommend that you go to Ready Made DSS sites and use some of DSS that are listed for various types of decisions. You will need to install MicroStrategy Desktop to analyze three stand-alone offline dashboards in a peer evaluated exercise.", "video": ["Course Introduction Video Lecture", "Optional Text Book", "Additional Resources - Course Overview", "Overview of Decision Making Video Lecture", "Powerpoint and Lecture Notes for Lesson 1.1", "Additional Resources Lesson 1.1", "Conceptual Foundations of Decision Making Video Lecture", "Powerpoint and Lecture Notes for Lesson 1.2", "Additional Resources Lesson 1.2", "Periodicals", "Decision Support Systems Video Lecture", "Powerpoint and Lecture Notes for Lesson 1.3", "Additional Resources for Lesson 1.3", "Additional Web Resources", "Vendors and Software Companies", "Decision Making Support in Practice Video Lecture", "Powerpoint and Lecture Notes for Lesson 1.4", "Additional Resources for Lesson 1.4", "Ready Made DSS Products and Services", "MicroStrategy Desktop Software Download and Installations  Steps for PC and MAC", "MicroStrategy Desktop Connections to Oracle VM on PC and MAC", "MicroStrategy Desktop Welcome Training Video", "Dashboards Demonstration Videos", "Module 1 Practice Quiz", "Q & A  and Errors Knowledge base", "Assignment for Module 1: Offline Dashboards with Advanced Visualizations"], "title": "Decision Making and Decision Support Systems"}, {"description": "Now that you understand the conceptual foundation of decision making and DSS, in module 2 we start by defining business intelligence (BI), BI architecture, and its components, and relate them to DSS.  In lesson 2, you will learn the main components of  BI platforms, their capabilities, and understand the competitive landscape of BI platforms. In lesson 3, you will learn the building blocks of business reports, the types of business reports, and the components and structure of business reporting systems . Finally in lesson 4, you will learn different types of OLAP and their applications, and comprehend the differences between OLAP and OLTP. You will need to use MicroStrategy Desktop to create effective and compelling data visualizations to analyze data and acquire insights into business practices in a peer evaluated exercise. \n", "video": ["BI Concepts Video Lecture", "Powerpoint and Lecture Notes for Lesson 2.1", "Additional Resources for Lesson 2.1", "Training Video for Connecting Data in MicroStrategy Desktop", "BI Platform Capabilities Video Lecture", "Powerpoint and Lecture Notes for Lesson 2.2", "Additional Resources for Lesson 2.2", "Training Video for Microstrategy Desktop BI Capabilities", "Business Reporting Video Lecture", "Powerpoint and Lecture Notes for Lesson 2.3", "Additional Resources for Lesson 2.3", "Training Videos for Connecting  to Spreadsheets, Joining Datasets, and Data Blending in MicroStrategy Desktop", "BI OLAP Styles Video Lecture", "Powerpoint and Lecture Notes for Lesson 2. 4", "Additional Resources for Lesson 2.4", "Training Video for Wrangling and Profiling Data in MicroStrategy", "Module 2 Practice Quiz", "Assignment for Module 2: World Wide Carbon Emissions Scenario", "Modules 1 and 2 Graded Quiz #1"], "title": "Business Intelligence Concepts and Platform Capabilities"}, {"description": "This module continues on the top job responsibilities of BI analysts by focusing on creating data visualizations and dashboards. You will first learn the importance of data visualization and different types of data that can be visually represented. You will then learn about the types of basic and composite charts. This will help you to determine which visualization is most effective to display data for a given data set, and to identify best practices for designing data visualizations. In lesson 3, you will learn the common characteristics of dashboard, the types of dashboards, and the list attributes of metrics usually included in dashboards. Finally in lesson 4, you will learn the guidelines for designing dashboard and the common pitfalls of dashboard design. You will need to use MicroStrategy  Desktop Visual Insight to design a dashboard for a Financial Services company in a peer evaluated exercise.", "video": ["Data Visualization Video Lecture", "Powerpoint and Lecture Notes for Lesson 3.1", "Additional Resources for Lesson 3.1", "Training Video for  Visual Insight in MicroStrategy Desktop", "Data Visualization Guidlines and Pitfalls Video Lecture", "Powerpoint and Lecture Notes for Lesson 3.2", "Additional Resources for Lesson 3.2", "Training Video for Exploring Data in MicroStrategy Desktop", "Comprehensive Training Video for Showing Data Visualization Steps", "Performance Dashboards Video Lecture", "Powerpoint and Lecture Notes for Lesson 3.3", "Additional Resources for Lesson 3.3", "Training Videos for Creating Dashboard in MicroStrategy Desktop", "Dashboard Design Guidelines and Pitfalls Video Lecture", "Powerpoint and Lecture Notes for Lesson 3.4", "Additional Resources for Lesson 3.4", "Training Video for Sharing a Dashboard in MicroStrategy Desktop", "Comprehensive Training Video for Creating Dashboard Using Advanced Visualization", "Module 3 Practice Quiz", "Assignment for Module 3: Design Dashboard for a Financial Service Company "], "title": "Data Visualization and Dashboard Design"}, {"description": "This module focuses on how BI is used for Business Performance Management (BPM). You will learn the main components of BPM as well as the four phases of BPM cycle and how organizations typically deploy BPM. In lesson 2, you will learn the purpose of Performance Measurement System and how organizations need to define the key performance indicators (KPIs) for their performance management system. In lesson 3, you will learn the four balanced scorecards perspectives and the differences between dashboards and scorecards. You will also be able to compare and contrast the benefits of using balanced scorecard versus using Six Sigma in a performance measurement system. Finally in lesson 4, you will learn the role of visual and business analytics (BA) in BI and how various forms of BA are supported in practice. At the end of the module, you will apply these concepts to create a dashboard, blend it with external data sets, and explore various visualization capabilities to find insights faster in a peer evaluated exercise.", "video": ["Business Performance Management Video Lecture", "Powerpoint and Lecture Notes for Lesson 4.1", "Additional Resources for Lesson 4.1", "Training Videos for Enriching and Modeling Data with MicroStrategy", "Performance Measurement System Video Lecture", "Powerpoint and Lecture Notes for Lesson 4.2", "Additional Resources for Lesson 4.2", "Training Videos for Connecting to MDX and Excel Files and Creating a Mashups in MicroStrategy Desktop", "Balanced Scorecards Versus Six Sigma Video Lecture", "Powerpoint and Lecture Notes for Lesson 4.3", "Additional Resources for Lesson 4.3", "Training Video for MicroStrategy Desktop Business Performance Analysis", "Business Analytics Video Lecture", "Powerpoint and Lecture Notes for Lesson 4.4", "Additional Resources for Lesson 4.4", "Training Video for Connecting to Social Media Sources in MicroStrategy Desktop", "Module 4 Practice Quiz", "Assignment for Module 4: Advanced Enterprise Data Discovery"], "title": " Business Performance Management Systems"}, {"description": "Module 5 covers BI maturity and strategy. You will learn different levels of BI maturity, the factors that impact BI maturity within an organization, and the main challenges and the potential solutions for a pervasive BI maturity within an organization. The last lesson will focus on the critical success factors for implementing a BI strategy, BI framework, and BI implementation targets. Finally, in your summative project, you will use MicroStrategy visual analytics capabilities to analyze KPIs for a fast food company to find the causes for problems .", "video": ["BI Maturity Video Lecture", "Powerpoint and Lecture Notes for Lesson 5.1", "Additional Resources for Lesson 5.1", "BI Strategy Video Lecture", "Powerpoint and Lecture Notes for Lesson 5.2", "Additional Resources for Lesson 5.2", "Course Closing Video", "Summative  Project: BPM For  Blazin' Burger Fast Food Restaurant", "Modules 3, 4, and 5 Graded Quiz #2"], "title": "BI Maturity, Strategy, and Summative Project"}], "title": "Business Intelligence Concepts, Tools, and Applications"}, {"course_info": "About this course: The value of IoT can be found within the analysis of data gathered from the system under observation, where insights gained can have direct impact on business and operational transformation.   Through analysis data correlation, patterns, trends, and other insight are discovered.  Insight leads to better communication between stakeholders, or actionable insights, which can be used to raise alerts or send commands, back to IoT devices.\nWith a focus on the topic of Exploratory Data Analysis, the course provides an in-depth look at mathematical foundations of basic statistical measures, and how they can be used in conjunction with advanced charting libraries to make use of the world’s best pattern recognition system – the human brain.  Learn how to work with the data, and depict it in ways that support visual inspections, and derive to inferences about the data. Identify interesting characteristics, patterns, trends, deviations or inconsistencies, and potential outliers.  The goal is that you are able to implement end-to-end analytic workflows at scale, from data acquisition to actionable insights. \nThrough a series of lectures and exercises students get the needed skills to perform such analysis on any data, although we clearly focus on IoT Sensor Event Data.\n\nAfter completing this course, you will be able to:\n•\tDescribe how basic statistical measures, are used to reveal  patterns within the data \n•\tRecognize data characteristics, patterns, trends, deviations or inconsistencies, and potential outliers.\n•\tIdentify useful techniques for working with big data such as dimension reduction and feature selection methods \n•\tUse advanced tools and charting libraries to:\n      o\tAutomatically store data from IoT device(s) \n      o\timprove efficiency of analysis of big-data with partitioning and parallel analysis \n      o\tVisualize the data in an number of 2D and 3D formats (Box Plot, Run Chart, Scatter Plot, Pareto Chart, and Multidimensional Scaling)\n\nFor successful completion of the course, the following prerequisites are recommended: \n•\tBasic programming skills in any programming language (python preferred)\n•\tA good grasp of basic algebra and algebraic equations\n•\t(optional) “A developer's guide to the Internet of Things (IoT)” - a Coursera course\n•\tBasic SQL is a plus\n\nIn order to complete this course, the following technologies will be used:\n(These technologies are introduced in the course as necessary so no previous knowledge is required.)\n•\tIBM Watson IoT Platform (MQTT Message Broker as a Service, Device Management and Operational Rule Engine)\n•\tIBM Bluemix (Open Standard Platform Cloud)\n•\tNode-Red\n•\tCloudant NoSQL (Apache CouchDB)\n•\tApacheSpark\n•\tLanguages: R, Scala and Python (focus on Python)\n\nThis course takes four weeks, 4-6h per week", "level": "Beginner", "package_name": null, "created_by": "IBM", "package_num": null, "teach_by": [{"name": "Romeo Kienzler", "department": "IBM Watson IoT"}], "target_audience": "Who is this class for: This course is designed for developers who want to improve their data analysis skills or data analysts who want to become expert in finding interesting patterns in IoT Sensor Data. ", "rating": "4.5", "week_data": [{"description": "Analysis of data starts with a hypothesis and through exploration, those hypothesis are tested.  Exploratory analysis in IoT considers large amounts of data, past or current, from multiple sources and summarizes its main characteristics.  Data is strategically inspected, cleaned, and models are created with the purpose of gaining insight, predicting  future data, and supporting decision making.     This learning module introduces methods for turning raw IoT data into insight ", "video": ["Course Introduction", "Course overview", "Challenges in IoT sensor data analysis and terminology", "Methods used for IoT data analysis", "Overview of technology used within the course", "Challenges, terminology, methods and technology", "Week 1 Programming Assignment 1", "Week 1 Programming Assignment 2"], "title": "Introduction to exploratory analysis"}, {"description": "Data analysis for IoT indicates that you have to build a solution for performing scalable analytics, on a large amount of data that arrives in great volumes and velocity.  Such a solution needs to be supported by a number of tools.   This module introduces common and popular tools, and highlights how they help data analyst produce viable end-to-end  solutions. ", "video": ["Get an IBM Bluemix promo code", "Data storage solutions", "Storing IoT Data - considerations when working with big data", "Download the “IoT Data storage cost calculator”", "ApacheSpark and how it supports the data scientist", "Programming language options on ApacheSpark", "Functional programming basics", "Introduction of Cloudant", "ApacheSparkSQL", "Overview of end-to-end scenario", "Data Science Experience", "Setup Environment with ApacheBahir IMPORTANT and MANDATORY", "Exercise 1 (Mandatory)", "Data storage solutions, and ApacheSpark", "Programming language options and functional programming", "ApacheSparkSQL, Cloudant, and the End to End Scenario", "Week 2 Programming Assignment"], "title": "Tools that support IoT solutions"}, {"description": "This learning module explores mathematical foundations supporting Exploratory Data Analysis (EDA) techniques.   ", "video": ["Overview of the week...", "Averages", "Standard deviation", "Skewness", "Kurtosis", "Covariance, Covariance matrices, correlation", "Multidimensional vector spaces", "Exercise 2", "Averages and standard deviation", "Skewness and kurtosis", "Covariance, correlation and multidimensional Vector Spaces", "Programming Assignment 3"], "title": "Mathematical Foundations on Exploratory Data Analysis"}, {"description": "This learning module details a variety of methods for plotting IoT time series sensor data using different methods in order to gain insights of hidden patterns in your data", "video": ["Overview of the week", "Plotting with ApacheSpark and python's matplotlib", "Exercise 3.1", "Dimensionality reduction", "PCA", "Exercise 3.2", "Course summary (and a bit more)", "Visualization and dimension reduction", "Programming Assignment Week 4"], "title": "Data Visualization"}], "title": "A developer's guide to Exploring and Visualizing IoT Data"}, {"course_info": "About this course: Confidence intervals and Hypothesis tests are very important tools in the Business Statistics toolbox. A mastery over these topics will help enhance your business decision making and allow you to understand and measure the extent of ‘risk’ or ‘uncertainty’ in various business processes. \nThis is the third course in the specialization \"Business Statistics and Analysis\" and the course  advances your knowledge about Business Statistics by introducing you to Confidence Intervals and Hypothesis Testing. We first conceptually understand these tools and their business application. We then introduce various calculations to constructing confidence intervals and to conduct different kinds of Hypothesis Tests. These are done by easy to understand applications.\n\nTo successfully complete course assignments, students must have access to a Windows version of Microsoft Excel 2010 or later. Please note that earlier versions of Microsoft Excel (2007 and earlier) will not be compatible to some Excel functions covered in this course. \n\n\nWEEK 1\nModule 1: Confidence Interval - Introduction\nIn this module you will get to conceptually understand what a confidence interval is and how is its constructed. We will introduce the various building blocks for the confidence interval such as the t-distribution, the t-statistic, the z-statistic and their various excel formulas. We will then use these building blocks to construct confidence intervals.\n\nTopics covered include:\n•\tIntroducing the t-distribution, the T.DIST and T.INV excel functions\n•\tConceptual understanding of a Confidence Interval\n•\tThe z-statistic and the t-statistic\n•\tConstructing a Confidence Interval using z-statistic and t-statistic \n\n\nWEEK 2\nModule 2: Confidence Interval - Applications\nThis module presents various business applications of the confidence interval including an application where we use the confidence interval to calculate an appropriate sample size. We also introduce with an application, the confidence interval for a population proportion. Towards the close of module we start introducing the concept of Hypothesis Testing.\n\nTopics covered include:\n•\tApplications of Confidence Interval\n•\tConfidence Interval for a Population Proportion\n•\tSample Size Calculation\n•\tHypothesis Testing, An Introduction\n\n\nWEEK 3\nModule 3: Hypothesis Testing\nThis module introduces Hypothesis Testing. You get to understand the logic behind hypothesis tests. The four steps for conducting a hypothesis test are introduced and you get to apply them for hypothesis tests for a population mean as well as population proportion. You will understand the difference between single tail hypothesis tests and two tail hypothesis tests and also the Type I and Type II errors associated with hypothesis tests and ways to reduce such errors. \n\nTopics covered include:\n•\tThe Logic of Hypothesis Testing\n•\tThe Four Steps for conducting a Hypothesis Test\n•\tSingle Tail and Two Tail Hypothesis Tests\n•\tGuidelines, Formulas and an Application of Hypothesis Test\n•\tHypothesis Test for a Population Proportion\n•\tType I and Type II Errors in a Hypothesis \n\n\nWEEK 4\nModule 4: Hypothesis Test - Differences in Mean\nIn this module, you'll apply Hypothesis Tests to test the difference between two different data, such hypothesis tests are called difference in means tests. We will introduce the three kinds of difference in means test and apply them to various business applications. We will also introduce the Excel dialog box to conduct such hypothesis tests.\n\nTopics covered include:\n•\tIntroducing the Difference-In-Means Hypothesis Test\n•\tApplications of the Difference-In-Means Hypothesis Test\n•\tThe Equal & Unequal Variance Assumption and the Paired t-test for difference in means.\n•\tSome more applications", "level": null, "package_name": "Business Statistics and Analysis Specialization ", "created_by": "Rice University", "package_num": "3", "teach_by": [{"name": "Sharad Borle", "department": "Jones Graduate School of Business"}], "target_audience": null, "rating": "4.8", "week_data": [{"description": "", "video": ["Meet the Professor", "Course FAQs", "Pre-Course Survey", "Introducing the t-distribution, the T.DIST Function", "Slides, Lesson 1", "Practice Quiz", "t-distribution Continued, the T.INV Function", "Slides, Lesson 2", "Practice Quiz", "Introducing Confidence Interval", "Slides, Lesson 3", "Practice Quiz", "Introducing Confidence Interval (Continued)", "Slides, Lesson 4", "Practice Quiz", "The z-statistic and the t-statistic", "Slides, Lesson 5", "Practice Quiz", "Using z- and t- statistics to Construct Confidence Interval", "Slides, Lesson 6", "Practice Quiz", "Week 1 Recap", "Confidence Interval - Introduction"], "title": "Confidence Interval - Introduction"}, {"description": "", "video": ["Application of Confidence Interval", "Home_Sizes.xlsx", "Slides, Lesson 1", "Practice Quiz", "Confidence Interval for a Population Proportion", "Slides, Lesson 2", "Practice Quiz", "Sample Size Calculation", "Slides, Lesson 3", "Practice Quiz", "Sample Size Calculation (Continued)", "Slides, Lesson 4", "Practice Quiz", "Hypothesis Testing, An Introduction", "Slides, Lesson 5", "Practice Quiz", "Week 2 Recap", "Confidence Interval - Applications"], "title": "Confidence Interval - Applications"}, {"description": "", "video": ["The Logic of Hypothesis Testing", "Slides, Lesson 1", "Practice Quiz", "Conducting a Hypothesis Test, the Four Steps", "Slides, Lesson 2", "Practice Quiz", "Single Tail and Two Tail Hypothesis Tests", "Slides, Lesson 3", "Practice Quiz", "Guidelines, Formulas and an Application of Hypothesis Test", "Average Age.xlsx", "Slides, Lesson 4", "Practice Quiz", "Hypothesis Test for a Population Proportion", "Slides, Lesson 5", "Practice Quiz", "Type I and Type II Errors in a Hypothesis Test", "Slides, Lesson 6", "Practice Quiz", "Week 3 Recap", "Hypothesis Testing"], "title": "Hypothesis Testing"}, {"description": "", "video": ["Introducing the Difference-In-Means Hypothesis Test", "Athletes.xlsx", "Slides, Lesson 1", "Practice Quiz", "Application of the Difference-In-Means Hypothesis Test", "Slides, Lesson 2", "Practice Quiz", "Application (Continued) - Equal & Unequal Variance Assumption", "Athletes.xlsx", "Slides, Lesson 3", "Practice Quiz", "The Paired t-Test for Means", "Training.xlsx", "Slides, Lesson 4", "Practice Quiz", "Some More Applications", "Store Sales", "Slides, Lesson 5", "Practice Quiz", "Some More Applications (Continued)", "Age Data", "Slides, Lesson 6", "Practice Quiz", "Week 4 Recap", "End-of-Course Survey", "Hypothesis Test - Differences in Mean"], "title": "Hypothesis Test - Differences in Mean"}], "title": "Business Applications of Hypothesis Testing and Confidence Interval Estimation "}, {"course_info": "About this course: Neurohacking describes how to use the R programming language (https://cran.r-project.org/) and its associated package to perform manipulation, processing, and analysis of neuroimaging data. We focus on publicly-available structural magnetic resonance imaging (MRI). We discuss concepts such as inhomogeneity correction, image registration, and image visualization.\n\nBy the end of this course, you will be able to:\n\nRead/write images of the brain in the NIfTI (Neuroimaging Informatics Technology Initiative) format\nVisualize and explore these images\nPerform inhomogeneity correction, brain extraction, and image registration (within a subject and to a template).", "level": "Intermediate", "package_name": null, "created_by": "Johns Hopkins University", "package_num": null, "teach_by": [{"name": "Dr. Elizabeth Sweeney ", "department": "Biostatistics"}, {"name": "Ciprian M. Crainiceanu", "department": "Biostatstics"}, {"name": "John Muschelli III ", "department": "Biostatistics"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "", "video": ["Meet the Team", "Syllabus", "Introduction", "Structural MRI and R", "Theme 1 Quiz"], "title": "Introduction"}, {"description": "In this section, we will discuss different formats that brain images come in, as well as some of the commonly done magnetic resonance imaging (MRI) scans.", "video": ["Data Structures and Operations", "The NIfTI Format", "Basic Visualization 1", "Basic Visualization 2", "Basic Data Manipulation", "Transformations and Smoothing", "Basic MRI Contrasts", "Theme 2 Lecture 1 Quiz", "Theme 2 Lecture 2 Quiz", "Theme 2 Lecture 3 Quiz", "Theme 2 Lecture 4 Quiz", "Theme 2 Lecture 5 Quiz", "Theme 2 Lecture 6 Quiz"], "title": "Neuroimaging: Formats and Visualization"}, {"description": "In this section, we will discuss the steps done to process brain MRI data.  We will discuss inhomogeneity correction, brain extraction or skull stripping, and various image registration techniques.", "video": ["Preprocessing: An Overview", "Registration", "fslr: System Requirements", "Bias Field Correction Using fslr", "Brain Extraction using fslr", "Image Registration Using fslr", "Installing ANTsR", "Basic Data Manipulation with ANTsR", "Pre-Processing with ANTSR", "Theme 3 Quiz 1", "Theme 3 Quiz 2", "Theme 3 Quiz 3", "Theme 3 Quiz 4", "Theme 3 Quiz 5", "Theme 3 Quiz 6", "Theme 3 Quiz 7", "Theme 3 Quiz 8", "Theme 3 Quiz 9"], "title": "Image Processing"}, {"description": "In this section, we will discuss the different types of registration and how one would go through processing a multi-sequence MRI scan, as well as wrapper functions that make the process much easier.  We also cover interactive exploration of brain image data and tissue-level (white/gray matter and cerebrospinal fluid (CSF)) segmentation from a T1-weighted image.", "video": ["Advanced Brain Image Processing", "Co-Registration using fslr", "Co-Registration using ANTsR", "Wrapper Functions for Processing Pipelines", "Across-Visit Co-Registration of T1 Images", "Rigid Registration to a Template", "Affine Registration of T1 to Template", "Nonlinear Registration of T1 to Template", "Getting ROI Anatomic Information from Non-Linear Registration", "Tissue-Level Segmentation", "Papayar: Interactively Viewing Images in R", "Papayar Script", "Theme 4 Quiz 1", "Theme 4 Quiz 2", "Theme 4 Quiz 3", "Theme 4 Quiz 4", "Theme 4 Quiz 5", "Theme 4 Quiz 6", "Theme 4 Quiz 7", "Theme 4 Quiz 8", "Theme 4 Quiz 9", "Theme 4 Quiz 10"], "title": "Extended Image Processing"}], "title": "Introduction to Neurohacking In R"}, {"course_info": "About this course: Learn how advances in geospatial technology and analytical methods have changed how we do everything, and discover how to make maps and analyze geographic patterns using the latest tools.\n\nThe past decade has seen an explosion of new mechanisms for understanding and using location information in widely-accessible technologies. This Geospatial Revolution has resulted in the development of consumer GPS tools, interactive web maps, and location-aware mobile devices. These radical advances are making it possible for people from all walks of life to use, collect, and understand spatial information like never before.\n \nThis course brings together core concepts in cartography, geographic information systems, and spatial thinking with real-world examples to provide the fundamentals necessary to engage with Geography beyond the surface-level. We will explore what makes spatial information special, how spatial data is created, how spatial analysis is conducted, and how to design maps so that they’re effective at telling the stories we wish to share. To gain experience using this knowledge, we will work with the latest mapping and analysis software to explore geographic problems.", "level": "Beginner", "package_name": null, "created_by": "The Pennsylvania State University", "package_num": null, "teach_by": [{"name": "Dr. Anthony C. Robinson", "department": "Department of Geography"}], "target_audience": "Who is this class for: No background is required; all are welcome. If you're already a Geospatial Guru, then you might find this class a bit basic, in which case I hope you'll consider taking the online geospatial education courses that we offer at Penn State: more information on our online certificate and Master's in GIS programs is available at: www.pennstategis.com.", "rating": "4.7", "week_data": [{"description": "Watch a short video to learn how this class works (it's a bit different than other MOOCs) and help make a map with your classmates.", "video": ["Course Introduction", "Let's Make A Map"], "title": "Getting Started"}, {"description": "Discover the Geospatial Revolution and its impact on the rapidly evolving science of Geography.", "video": ["Lesson 1 - Lecture 1", "The Geospatial Revolution", "Lesson 1 - Lecture 2", "The Changing Nature of Place", "What Is Geography?", "Geospatial Revolution Video", "Lesson 1 Mapping Activity", "Privacy and the Geospatial Revolution", "Lesson 1 Quiz"], "title": "The Changing Nature of Place"}, {"description": "Explore what it means to think spatially and consider the impacts of scale and time.", "video": ["Lesson 2 - Lecture 1", "Spatial is Special", "Thinking like a Geographer", "Lesson 2 - Lecture 2", "Spatial Relationships", "Scale and Time", "Geospatial Revolution Video", "Lesson 2 Mapping Activity", "Change Matters In Geography", "Lesson 2 Quiz"], "title": "Spatial is Special"}, {"description": "Examine the key elements of spatial datasets.", "video": ["Lesson 3 - Lecture 1", "Spatial Data", "The Earth from Above", "Lesson 3 - Lecture 2", "Who Makes Spatial Data?", "Describing Spatial Data", "Lesson 3 Mapping Activity", "Geospatial Revolution Video", "Where Do Disasters Happen, and How Can Maps Help?", "Lesson 3 Quiz"], "title": "Understanding Spatial Data"}, {"description": "Learn how Geographers solve problems using spatial analysis techniques.", "video": ["Lesson 4 - Lecture 1", "Analysis Pitfalls", "Lesson 4 - Lecture 2", "Spatial Analysis", "Normalization", "Geospatial Revolution Video", "Lesson 4 Mapping Activity", "Mapping Social Media - What's It Good For?", "Lesson 4 Quiz"], "title": "Doing Spatial Analysis"}, {"description": "Explore the key elements of effective cartographic design.", "video": ["Lesson 5 - Lecture 1", "Making Great Maps", "Layout & Symbols", "Lesson 5 - Lecture 2", "Choosing Color Schemes", "Data Classification", "Text on Maps", "Geospatial Revolution Video", "Storytelling With Maps", "Resources", "Postscript", "Storytelling With Maps", "Final Exam"], "title": "Making Great Maps"}], "title": "Maps and the Geospatial Revolution"}, {"course_info": "About this course: This course will provide you with an overview over existing data products and a good understanding of the data collection landscape. With the help of various examples you will learn how to identify which data sources likely matches your research question, how to turn your research question into measurable pieces, and how to think about an analysis plan. Furthermore this course will provide you with a general framework that allows you to not only understand each step required for a successful data collection and analysis, but also help you to identify errors associated with different data sources. You will learn some metrics to quantify each potential error, and thus you will have tools at hand to describe the quality of a data source. Finally we will introduce different large scale data collection efforts done by private industry and government agencies, and review the learned concepts through these examples. This course is suitable for beginners as well as those that know about one particular data source, but not others, and are looking for a general framework to evaluate data products.", "level": "Intermediate", "package_name": "Survey Data Collection and Analytics  Specialization ", "created_by": "University of Maryland, College Park", "package_num": "1", "teach_by": [{"name": "Frauke Kreuter, Ph.D.", "department": "Adjunct Research Professor, Institute for Social Research"}, {"name": "Mariel Leonard", "department": "Joint Program in Survey Methodology"}], "target_audience": null, "rating": "4.1", "week_data": [{"description": "The first course in the specialization provides an overview of the topics to come. This module walks you through the process of data collection and analysis. Starting with a research question and a review of existing data sources, we cover survey data collection techniques, highlight the importance of data curation, and some basic features that can affect your data analysis when dealing with sample data. Issues of data access and resources for access are introduced in this module. ", "video": ["Course Overview", "Readings and Resources List", "Research Question Design", "Discussion Prompt: Your own experience", "Types of Data", "Examples of Found Data", "Visualizing the Data Generation Process", "Data Curation", "Data Analysis", "Access Issues", "Access Resources", "Summary", "Discussion Prompt: Privacy", "Handouts", "AAPOR (2015)", "Couper (2013)", "Quiz for Week 1"], "title": "Research Designs and Data Sources"}, {"description": "In this module we will emphasize the importance of having a well specified research question and analysis plan. We will provide an overview over the various data collection strategies, a variety of available modes for data collection and some thinking on how to choose the right mode. ", "video": ["Issues with Inductive Reasoning", "Planning on What You Want to Observe", "Planning on How to Collect Data", "New Modes", "Web and Google", "Choosing a Mode", "Handouts", "Jäckle et al. (2015)", "Quiz for Week 2"], "title": "Measurements and Analysis Plan"}, {"description": "In this module you will be introduced to a general framework that allows you to not only understand each step required for a successful data collection and analysis, but also help you to identify errors associated with different data sources. You will learn some metrics to quantify each potential error, and thus you will have tools at hand to describe the quality of a data source.", "video": ["Quality of Data", "Inference", "Survey Life Cycle from a Design Perspective - Measurement", "Survey Life Cycle from a Design Perspective - Representation", "Survey Lifecycle from a Process Perspective", "Survey Lifeycle from a Quality Perspective", "Survey Lifecycle from a Quality Perspective (II) - Metrics", "Survey Lifecycle from a Quality Perspective (III) - Coverage and Sampling", "Handouts", "Groves (2011)", "Groves & Lyberg (2010)", "Quiz for Week 3"], "title": "Quality Framework"}, {"description": "In this module we introduce a few surveys across a variety of topics. For each we highlight data collection features. The surveys span a variety of topics. We challenge you to think about alternative data sources that can be used to gather the same information or insights.", "video": ["NCVS", "NSDUH", "SCA", "NAEP", "BRFSS", "CES", "Discussion Prompt: Alternative Data Sources", "SHARE", "ESS", "Discussion prompt: In your country", "Handouts", "Davidov (2008)", "Quiz for Week 4"], "title": "Application of TSE Framework to Existing Surveys"}], "title": "Framework for Data Collection and Analysis"}, {"course_info": "About this course: The data science revolution has produced reams of new data from a wide variety of new sources. These new datasets are being used to answer new questions in way never before conceived. Visualization remains one of the most powerful ways draw conclusions from data, but the influx of new data types requires the development of new visualization techniques and building blocks. This course provides you with the skills for creating those new visualization building blocks. We focus on the ggplot2 framework and describe how to use and extend the system to suit the specific needs of your organization or team. Upon completing this course, learners will be able to build the tools needed to visualize a wide variety of data types and will have the fundamentals needed to address new data types as they come about.", "level": "Intermediate", "package_name": "Mastering Software Development in R Specialization ", "created_by": "Johns Hopkins University", "package_num": "4", "teach_by": [{"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Brooke Anderson", "department": "Colorado State University"}], "target_audience": null, "rating": "4.0", "week_data": [{"description": "Before we get started, we'll take a quick overview of the course.", "video": ["Welcome to Building Data Visualization Tools", "Textbook: Mastering Software Development in R", "Syllabus"], "title": "Welcome to Building Data Visualization Tools"}, {"description": "Now, we'll dive into creating and customizing ggplot2 plots.", "video": ["Introduction", "Initializing a ggplot object", "Plot aesthetics", "Creating a basic ggplot plot", "Geoms", "Using multiple geoms", "Constant aesthetics", "Example plots", "Extensions of ggplot2", "Introduction", "Guidelines for good plots", "Scales and color", "To find out more", "Plotting with ggplot2"], "title": "Plotting with ggplot2"}, {"description": "Mapping is a critical part of many data visualizations. During this module, we'll teach you how to create simple and dynamic maps with ggplot2 and ggmap, how to overlay data, and how to create chloropleth maps of US counties.", "video": ["Introduction", "Basics of Mapping", "ggmap, Google Maps API", "Mapping US counties and states", "More advanced mapping– Spatial objects", "Where to find more on mapping with R", "Overview of htmlWidgets", "plotly package", "Creating your own widget", "Mapping and interactive plots"], "title": "Mapping and interactive plots"}, {"description": "The grid package in R implements the primitive graphical functions that underly the ggplot2 plotting system. In this module, you'll learn how to work with grid to build graphics.", "video": ["Introduction", "Overview of grid graphics", "Grobs", "Viewports", "Grid graphics coordinate systems", "The gridExtra package", "Where to find more about grid graphics", "Basics of grid graphics"], "title": "The grid Package"}, {"description": "Building and modifying a theme in ggplot2 is a key feature of the ggplot2 package and system for building data graphics. In this final module, you'll learn to build a new theme and modifying existing themes with new features.", "video": ["Introduction", "Why Build a New Theme?", "Default Theme", "Building a New Theme", "Summary", "Introduction", "Building a Geom", "Example: An Automatic Transparency Geom", "Building a Stat", "Example: Normal Confidence Intervals", "Combining Geoms and Stats", "Summary", "Build a New Geom"], "title": "Building New Graphical Elements"}], "title": "Building Data Visualization Tools"}, {"course_info": "About this course: In this course, you will develop and test hypotheses about your data. You will learn a variety of statistical tests, as well as strategies to know how to apply the appropriate one to your specific data and question. Using your choice of two powerful statistical software packages (SAS or Python), you will explore ANOVA, Chi-Square, and Pearson correlation analysis. This course will guide you through basic statistical principles to give you the tools to answer questions you have developed. Throughout the course, you will share your progress with others to gain valuable feedback and provide insight to other learners about their work.", "level": null, "package_name": "Data Analysis and Interpretation Specialization ", "created_by": "Wesleyan University", "package_num": "2", "teach_by": [{"name": "Jen Rose", "department": "Psychology"}, {"name": "Lisa Dierker", "department": "Psychology"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "This session starts where the Data Management and Visualization course left off. Now that you have selected a data set and research question, managed your variables of interest and visualized their relationship graphically, we are ready to test those relationships statistically. The first group of videos describe the process of hypothesis testing which you will use throughout this course to test relationships between different kinds of variables (quantitative and categorical). Next, we show you how to test hypotheses in the context of Analysis of Variance (when you have one quantitative variable and one categorical variable). Your task will be to write a program that manages any additional variables you may need and runs and interprets an Analysis of Variance test. Note that if your research question does not include one quantitative variable, you can use one from your data set just to get some practice with the tool. If your research question does not include a categorical variable, you can categorize one that is quantitative.", "video": ["Lesson 1 - The role of probability in inference", "Lesson 2 - From sample to population", "Lesson 3 - Steps in hypothesis testing", "Lesson 4 - What is a p value?", "Lesson 5 - How to choose a statistical test", "Lesson 6 - Ideas behind ANOVA", "Choosing SAS or Python", "Getting Started with SAS", "Getting Started with Python", "Course Codebooks", "Course Data Sets", "Uploading Your Own Data to SAS", "SAS Program Code for Video Examples", "SAS Lesson 7 - ANOVA: Explanatory variable with 2 levels", "SAS Lesson 8 - ANOVA: Explanatory variables with more than 2 levels", "SAS Lesson 9 - Post hoc tests for ANOVA", "SAS Lesson 10 - ANOVA summary", "Python Program Code for Video Examples", "Python Lesson 7 - ANOVA: Explanatory variables with two levels", "Python Lesson 8 - ANOVA: Explanatory variables with more than 2 levels", "Python Lesson 9 - Post hoc tests for ANOVA", "Python Lesson 10 - ANOVA Summary", "Getting set up for the assignments", "Tumblr Instructions", "Example: Running an analysis of variance", "Running an analysis of variance"], "title": "Hypothesis Testing and ANOVA"}, {"description": "This session shows you how to test hypotheses in the context of a Chi-Square Test of Independence (when you have two categorical variables). Your task will be to write a program that manages any additional variables you may need and runs and interprets a Chi-Square Test of Independence. Note that if your research question only includes quantitative variables, you can categorize those just to get some practice with the tool. ", "video": ["Lesson 1 - Ideas behind the Chi Square test of independence", "SAS Program Code for Video Examples", "SAS Lesson 2 - Chi Square Test of independence in practice", "SAS Lesson 3 - Post hoc tests for Chi Square tests of independence", "SAS Lesson 4 - Chi Square summary", "Python Program Code for Video Examples", "Python Lesson 2 - Chi Square test of independence in practice", "Python Lesson 3 - Post hoc tests for Chi Square tests of independence", "Python Lesson 4 - Chi Square summary", "Example: Running a Chi-Square Test of Independence", "Running a Chi-Square Test of Independence "], "title": "Chi Square Test of Independence"}, {"description": "This session shows you how to test hypotheses in the context of a Pearson Correlation (when you have two quantitative variables). Your task will be to write a program that manages any additional variables you may need and runs and interprets a correlation coefficient. Note that if your research question only includes categorical variables, you can choose other variables from your data set just to get some practice with the tool. ", "video": ["Lesson 1 - Pearson Correlation", "Lesson 2 - Correlation Example", "SAS Program Code for Video Examples", "SAS Lesson 3 - Calculating Correlation", "Python Program Code for Video Examples", "Python Lesson 3 - Calculating Correlation", "Generating a Correlation Coefficient"], "title": "Pearson Correlation"}, {"description": "In this session, we will discuss the basic concept of statistical interaction (also known as moderation). In statistics, moderation occurs when the relationship between two variables depends on a third variable. The effect of a moderating variable is often characterized statistically as an interaction; that is, a third variable that affects the direction and/or strength of the relation between your explanatory (X) and response (Y) variable. Your task will be to test your own research question in the context of one or more potential moderating variables. ", "video": ["SAS Program Code for Video Examples", "SAS Lesson 1 - Defining moderation, a.k.a. statistical interaction", "SAS Lesson 2 - Testing moderation in the context of ANOVA", "SAS Lesson 3 - Testing moderation in the context of chi square", "SAS Lesson 4 - Testing moderation in the context of correlation", "Python Lesson 1 - Defining moderation, a.k.a. statistical interaction", "Python Lesson 2 - Testing moderation in the context of ANOVA", "Python Lesson 3 - Testing moderation in the context of Chi-Square", "Python Lesson 4 - Testing moderation in the context of correlation", "Python Program Code for Video Examples", "A Question of Causation (Used with Permission from Annenberg Learner)", "Testing a Potential Moderator"], "title": "Exploring Statistical Interactions"}], "title": "Data Analysis Tools"}, {"course_info": "About this course: This course focuses on one of the most important tools in your data analysis arsenal: regression analysis. Using either SAS or Python, you will begin with linear regression and then learn how to adapt when two variables do not present a clear linear relationship. You will examine multiple predictors of your outcome and be able to identify confounding variables, which can tell a more compelling story about your results. You will learn the assumptions underlying regression analysis, how to interpret regression coefficients, and how to use regression diagnostic plots and other tools to evaluate the quality of your regression model. Throughout the course, you will share with others the regression models you have developed and the stories they tell you.", "level": null, "package_name": "Data Analysis and Interpretation Specialization ", "created_by": "Wesleyan University", "package_num": "3", "teach_by": [{"name": "Jen Rose", "department": "Psychology"}, {"name": "Lisa Dierker", "department": "Psychology"}], "target_audience": null, "rating": "4.3", "week_data": [{"description": "This session starts where the Data Analysis Tools course left off. This first set of videos provides you with some conceptual background about the major types of data you may work with, which will increase your competence in choosing the statistical analysis that’s most appropriate given the structure of your data, and in understanding the limitations of your data set. We also introduce you to the concept of confounding variables, which are variables that may be the reason for the association between your explanatory and response variable. Finally, you will gain experience in describing your data by writing about your sample, the study data collection procedures, and your measures and data management steps. ", "video": ["Some Guidance for Learners New to the Specialization", "Lesson 1: Observational Data", "Lesson 2: Experimental Data", "Lesson 3: Confounding Variables", "Lesson 4: Introduction to Multivariate Methods", "Getting Set up for Assignments", "Tumblr Instructions", "How to Write About Data", "Writing About Your Data: Example Assignment", "Writing About Your Data"], "title": "Introduction to Regression"}, {"description": "In this session, we discuss more about the importance of testing for confounding, and provide examples of situations in which a confounding variable can explain the association between an explanatory and response variable. In addition, now that you have statistically tested the association between an explanatory variable and your response variable, you will test and interpret this association using basic linear regression analysis for a quantitative response variable. You will also learn about how the linear regression model can be used to predict your observed response variable. Finally, we will also discuss the statistical assumptions underlying the linear regression model, and show you some best practices for coding your explanatory variables\nNote that if your research question does not include one quantitative response variable, you can use one from your data set just to get some practice with the tool. \n", "video": ["SAS or Python - Which to Choose?", "Getting Started with SAS", "Getting Started with Python", "Course Codebooks", "Course Data Sets", "Uploading Your Own Data to SAS", "SAS Program Code for Video Examples", "SAS Lesson 1: More on Confounding Variables", "SAS Lesson 2: Testing a Basic Linear Regression Mode", "SAS Lesson 3: Categorical Explanatory Variables", "Python Program Code for Video Examples", "Python Lesson 1: More on Confounding Variables", "Python Lesson 2: Testing a Basic Linear Regression Model", "Python Lesson 3: Categorical Explanatory Variables", "Lesson 4: Linear Regression Assumptions", "Outlier Decision Tree", "Lesson 5: Centering Explanatory Variables", "Test a Basic Linear Regression Model"], "title": "Basics of Linear Regression"}, {"description": "Multiple regression analysis is tool that allows you to expand on your research question, and conduct a more rigorous test of the association between your explanatory and response variable by adding additional quantitative and/or categorical explanatory variables to your linear regression model. In this session, you will apply and interpret a multiple regression analysis for a quantitative response variable, and will learn how to use confidence intervals to take into account error in estimating a population parameter. You will also learn how to account for nonlinear associations in a linear regression model. Finally, you will develop experience using regression diagnostic techniques to evaluate how well your multiple regression model predicts your observed response variable. \nNote that if you have not yet identified additional explanatory variables, you should choose at least one additional explanatory variable from your data set. When you go back to your codebooks, ask yourself a few questions like “What other variables might explain the association between my explanatory and response variable?”; “What other variables might explain more of the variability in my response variable?”, or even “What other explanatory variables might be interesting to explore?” Additional explanatory variables can be either quantitative, categorical, or both. Although you need only two explanatory variables to test a multiple regression model, we encourage you to identify more than one additional explanatory variable. Doing so will really allow you to experience the power of multiple regression analysis, and will increase your confidence in your ability to test and interpret more complex regression models. If your research question does not include one quantitative response variable, you can use the same quantitative response variable that you used in Module 2, or you may choose another one from your data set. ", "video": ["SAS Program Code for Video Examples", "SAS Lesson 1: Multiple Regression", "SAS Lesson 2: Confidence Intervals", "SAS Lesson 3: Polynomial Regression", "SAS Lesson 4: Evaluating Model Fit, pt. 1", "SAS Lesson 5: Evaluating Model Fit, pt. 2", "Python Program Code for Video Examples", "Python Lesson 1: Multiple Regression", "Python Lesson 2: Confidence Intervals", "Python Lesson 3: Polynomial Regression", "Python Lesson 4: Evaluating Model Fit, pt. 1", "Python Lesson 5: Evaluating Model Fit, pt. 2", "Test a Multiple Regression Model "], "title": "Multiple Regression"}, {"description": "In this session, we will discuss some things that you should keep in mind as you continue to use data analysis in the future. We will also teach also you how to test a categorical explanatory variable with more than two categories in a multiple regression analysis. Finally, we introduce you to logistic regression analysis for a binary response variable with multiple explanatory variables. Logistic regression is simply another form of the linear regression model, so the basic idea is the same as a multiple regression analysis. But, unlike the multiple regression model, the logistic regression model is designed to test binary response variables. You will gain experience testing and interpreting a logistic regression model, including using odds ratios and confidence intervals to determine the magnitude of the association between your explanatory variables and response variable.   \nYou can use the same explanatory variables that you used to test your multiple regression model with a quantitative outcome, but your response variable needs to be binary (categorical with 2 categories). If you have a quantitative response variable, you will have to bin it into 2 categories. Alternatively, you can choose a different binary response variable from your data set that you can use to test a logistic regression model. If you have a categorical response variable with more than two categories, you will need to collapse it into two categories.\n", "video": ["SAS Program Code for Video Examples", "SAS Lesson 1: Categorical Explanatory Variables with More Than Two Categories", "Python Program Code for Video Examples", "Python Lesson 1: Categorical Explanatory Variables with More Than Two Categories", "Lesson 2: A Few Things to Keep in Mind", "SAS Lesson 3: Logistic Regression for a Binary Response Variable, pt 1", "SAS Lesson 4: Logistic Regression for a Binary Response Variable, pt. 2", "Python Lesson 3: Logistic Regression for a Binary Response Variable, pt. 1", "Python Lesson 4: Logistic Regression for a Binary Response Variable, pt. 2", "Week 1 Video Credits", "Week 2 Video Credits", "Week 3 Video Credits", "Week 4 Video Credits", "Test a Logistic Regression Model"], "title": "Logistic Regression"}], "title": "Regression Modeling in Practice"}, {"course_info": "About this course: In the final capstone project you will apply the skills you learned by building a large data-intensive application using real-world data.\n\nYou will implement a complete application processing several gigabytes of data. This application will show interactive visualizations of the evolution of temperatures over time all over the world.\n\nThe development of such an application will involve:\n — transforming data provided by weather stations into meaningful information like, for instance, the average temperature of each point of the globe over the last ten years ;\n — then, making images from this information by using spatial and linear interpolation techniques ;\n — finally, implementing how the user interface will react to users’ actions.", "level": null, "package_name": "Functional Programming in Scala Specialization ", "created_by": "École Polytechnique Fédérale de Lausanne", "package_num": "5", "teach_by": [{"name": "Dr. Julien Richard-Foy", "department": "Scala Center"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "Get an overview of the project and all the information to get started. Transform data provided by weather stations into meaningful information.", "video": ["Project overview", "Scaffolding material", "Project setup", "Data extraction", "Scaffolding material"], "title": "Project overview"}, {"description": "Transform temperature data into images, using various interpolation techniques.", "video": ["Raw data visualization"], "title": "Raw data display"}, {"description": "Generate images compatible with most Web-based mapping libraries.", "video": ["Milestone: interactive visualization in a Web app"], "title": "Interactive visualization"}, {"description": "Get more meaning from your data: compute temperature deviations compared to normals.", "video": ["Data manipulation"], "title": "Data manipulation"}, {"description": "Generate images using bilinear interpolation.", "video": ["Value-added information visualization"], "title": "Value-added information visualization"}, {"description": "Implement how the user interface will react to users’ actions", "video": ["Interactive user interface"], "title": "Interactive user interface"}], "title": "Functional Programming in Scala Capstone"}, {"course_info": "About this course: We will learn computational methods -- algorithms and data structures -- for analyzing DNA sequencing data. We will learn a little about DNA, genomics, and how DNA sequencing is used.  We will use Python to implement key algorithms and data structures and to analyze real genomes and DNA sequencing datasets.", "level": null, "package_name": "Genomic Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "4", "teach_by": [{"name": "Ben Langmead, PhD", "department": "Computer Science"}, {"name": "Jacob Pritt", "department": "Department of Computer Science"}], "target_audience": null, "rating": "4.8", "week_data": [{"description": "This module we begin our exploration of algorithms for analyzing DNA sequencing data. We'll discuss DNA sequencing technology, its past and present, and how it works.\n", "video": ["Welcome to Algorithms for DNA Sequencing", "Pre Course Survey", "Syllabus", "Setting up Python (and Jupyter)", "Getting slides and notebooks", "Using data files with Python programs", "Module 1 Introduction", "Lecture: Why study this?", "Lecture: DNA sequencing past and present", "Lecture: Genomes as strings, reads as substrings", "Lecture: String definitions and Python examples", "Practical: String basics ", "Practical: Manipulating DNA strings ", "Practical: Downloading and parsing a genome ", "Lecture: How DNA gets copied", "Optional lecture: How second-generation sequencers work ", "Optional lecture: Sequencing errors and base qualities ", "Lecture: Sequencing reads in FASTQ format", "Practical: Working with sequencing reads ", "Practical: Analyzing reads by position ", "Lecture: Sequencers give pieces to genomic puzzles", "Lecture: Read alignment and why it's hard", "Lecture: Naive exact matching", "Practical: Matching artificial reads ", "Practical: Matching real reads ", "Programming Homework 1 Instructions (Read First)", "Module 1", "Programming Homework 1"], "title": "DNA sequencing, strings and matching"}, {"description": "In this module, we learn useful and flexible new algorithms for solving the exact and approximate matching problems.  We'll start by learning Boyer-Moore, a fast and very widely used algorithm for exact matching", "video": ["Week 2 Introduction ", "Lecture: Boyer-Moore basics", "Lecture: Boyer-Moore: putting it all together", "Lecture: Diversion: Repetitive elements", "Practical: Implementing Boyer-Moore ", "Lecture: Preprocessing", "Lecture: Indexing and the k-mer index", "Lecture: Ordered structures for indexing", "Lecture: Hash tables for indexing", "Practical: Implementing a k-mer index ", "Lecture: Variations on k-mer indexes", "Lecture: Genome indexes used in research", "Lecture: Approximate matching, Hamming and edit distance", "Lecture: Pigeonhole principle", "Practical: Implementing the pigeonhole principle ", "Programming Homework 2 Instructions (Read First)", "Module 2", "Programming Homework 2"], "title": "Preprocessing, indexing and approximate matching"}, {"description": "This week we finish our discussion of read alignment by learning about algorithms that solve both the edit distance problem and related biosequence analysis problems, like global and local alignment.", "video": ["Module 3 Introduction ", "Lecture: Solving the edit distance problem", "Lecture: Using dynamic programming for edit distance", "Practical: Implementing dynamic programming for edit distance ", "Lecture: A new solution to approximate matching", "Lecture: Meet the family: global and local alignment", "Practical: Implementing global alignment ", "Lecture: Read alignment in the field", "Lecture: Assembly: working from scratch", "Lecture: First and second laws of assembly", "Lecture: Overlap graphs", "Practical: Overlaps between pairs of reads ", "Practical: Finding and representing all overlaps ", "Programming Homework 3 Instructions (Read First)", "Module 3", "Programming Homework 3"], "title": "Edit distance, assembly, overlaps"}, {"description": "In the last module we began our discussion of the assembly problem and we saw a couple basic principles behind it.  In this module, we'll learn a few ways to solve the alignment problem.", "video": ["Module 4 introduction ", "Lecture: The shortest common superstring problem", "Practical: Implementing shortest common superstring ", "Lecture: Greedy shortest common superstring", "Practical: Implementing greedy shortest common superstring ", "Lecture: Third law of assembly: repeats are bad", "Lecture: De Bruijn graphs and Eulerian walks", "Practical: Building a De Bruijn graph ", "Lecture: When Eulerian walks go wrong", "Lecture: Assemblers in practice", "Lecture: The future is long?", "Lecture: Computer science and life science", "Lecture: Thank yous ", "Post Course Survey", "Programming Homework 4", "Module 4"], "title": "Algorithms for assembly"}], "title": "Algorithms for DNA Sequencing"}, {"course_info": "About this course: Want to know how to query and process petabytes of data in seconds? Curious about data analysis that scales automatically as your data grows? Welcome to the Data Insights course!\n\nThis 1-week, accelerated online course teaches participants how to derive insights through data analysis and visualization using the Google Cloud Platform. The course features interactive scenarios and hands-on labs where participants explore, mine, load, visualize, and extract insights from diverse Google BigQuery datasets. The course covers data loading, querying, schema modeling, optimizing performance, query pricing,  and data visualization.\n\nPREREQUISITES\nTo get the most out of this course, participants must complete the prior courses in this specialization:\n• Exploring and Preparing your Data\n• Storing and Visualizing your Data\n• Architecture and Performance", "level": "Intermediate", "package_name": "From Data to Insights with Google Cloud Platform Specialization ", "created_by": "Google Cloud", "package_num": "4", "teach_by": [{"name": "Google Cloud Training", "department": null}], "target_audience": "Who is this class for: • Data Analysts, Business Analysts, Business Intelligence professionals\n• Cloud Data Engineers who will be partnering with Data Analysts to build scalable data solutions on Google Cloud Platform", "rating": null, "week_data": [], "title": "Applying Machine Learning to your Data with GCP"}, {"course_info": "About this course: With marketers are poised to be the largest users of data within the organization, there is a need to make sense of the variety of consumer data that the organization collects. Surveys, transaction histories and billing records can all provide insight into consumers’ future behavior, provided that they are interpreted correctly. In Introduction to Marketing Analytics, we introduce the tools that learners will need to convert raw data into marketing insights. The included exercises are conducted using Microsoft Excel, ensuring that learners will have the tools they need to extract information from the data available to them. The course provides learners with exposure to essential tools including exploratory data analysis, as well as regression methods that can be used to investigate the impact of marketing activity on aggregate data (e.g., sales) and on individual-level choice data (e.g., brand choices). \n\nTo successfully complete the assignments in this course, you will require Microsoft Excel. If you do not have Excel, you can download a free 30-day trial here: https://products.office.com/en-us/try", "level": "Intermediate", "package_name": "Foundations of Marketing Analytics Specialization ", "created_by": "Emory University", "package_num": "1", "teach_by": [{"name": "David Schweidel", "department": "Goizueta Business School"}], "target_audience": "Who is this class for: This course is aimed at those familiar with business and marketing concepts or those currently working in a business analyst or marketing research role. ", "rating": "4.4", "week_data": [{"description": "In this module, students will be introduced to the instructor, Dr. David Schweidel and get and overview of the course. ", "video": ["Meet Dr. Schweidel", "Getting Started", "A Note About Exercises & Quizzes", "Introduce Yourself"], "title": "Meet Dr. Schweidel & Course Overview"}, {"description": "Modules 2 and 3 focus on identifying appropriate descriptive statistics (measures of central tendency and dispersion) for different types of data, as well as recoding data using reference commands to prepare it for analysis. Additionally, you will manipulate and summarize data using pivot tables in Excel, produce visualizations that are appropriate based on the type of data being analyzed, and interpret statistics and visualizations to draw conclusions to address relevant marketing questions.", "video": ["Course Objectives & Example 1: Political Advertising Expenditures", "Course Goals & Example 2: Performing Arts Centers", "Organizing Data", "The Motion Picture Industry", "How Companies Learn Your Secrets", "Big Data in the Big Apple", "Categorical Variables in the Motion Picture Industry ", "Motion Picture Industry Example Data Set", "Module 2 Quiz"], "title": "Exploring your Data with Visualization and Descriptive Statistics, Part 1"}, {"description": "Modules 2 and 3 focus on identifying appropriate descriptive statistics (measures of central tendency and dispersion) for different types of data, as well as recoding data using reference commands to prepare it for analysis. Additionally, you will manipulate and summarize data using pivot tables in Excel, produce visualizations that are appropriate based on the type of data being analyzed, and interpret statistics and visualizations to draw conclusions to address relevant marketing questions.", "video": ["Excel Analysis of Motion Picture Industry Data", "Displaying Conditional Distributions", "Analyzing Qualitative Variables", "Steps in Constructing Histograms", "Common Descriptive Statistics for Quantitative Data", "Giving Viewers What They Want", "Why Personalization is Key for Content Marketing", "Assignment: Survey Analysis Assignment", "Visualizing Data: Technique Review", "Module 3 Quiz"], "title": "Exploring your Data with Visualization and Descriptive Statistics, Part 2"}, {"description": "In this module, you will be asked to determine the appropriate type of regression for different types of marketing data and will perform regression analysis to assess the impact of marketing actions on outcomes of interest, such as sales, traffic, and brand choices. You will also be asked to interpret regression output to understand overall model performance and importance of different predictors, as well as make predictions using the appropriate regression model.", "video": ["Regression-Based Modeling", "Introduction to Customer Analytics", "Customer Choices Drive Business Decisions", "Illustrating Customer Analytics in Excel", "How to Find Your Most Valuable Customers", "Assignment: Customer Acquisition Example", "Logit v. Probit Models", "Module 4 Quiz"], "title": "Regression Analysis for Marketing Data"}, {"description": "This final module will connect the results of regression analysis to marketing decisions. You will learn to build tools that allow users to evaluate outcomes based on different marketing decisions, as well as characterize the extent of uncertainty in outcomes based on the selected marketing decisions.", "video": ["Customer Valuation Excel Demonstration", "Inventory Management Excel Demonstration", "Encouraging TV Binge Watching May Backfire On Advertisers", "Use Big Data to Create Value for Customers, Not Just Target Them", "Assignment: Using Regression for Inventory Management", "Course Reflection", "Module 5 Quiz", "Acquisition & Retention Assignment"], "title": "From Analysis to Action"}], "title": "Meaningful Marketing Insights"}, {"course_info": "About this course: Learn to use the tools that are available from the Galaxy Project. This is the second course in the Genomic Big Data Science Specialization.", "level": null, "package_name": "Genomic Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "2", "teach_by": [{"name": "James Taylor, PhD", "department": null}], "target_audience": null, "rating": "3.6", "week_data": [{"description": "This week, we will present some of the research challenges that motivated the development of the Galaxy framework. We will then introduce Galaxy, describe what the Galaxy framework is, and look at different ways you can use it.", "video": ["Welcome", "Syllabus", "Pre-Course Survey", "A Note about AWS", "Challenges of Reproducibility", "Introduction to the Galaxy Platform", "Module 1 Quiz"], "title": "Introduction"}, {"description": "In this module and the following modules we will start to use Galaxy to perform different types of analysis. ", "video": ["Galaxy 101: Genomic Intervals", "Galaxy 101: Workflows", "Annotation, Sharing, and Publishing", "Module 2 Quiz"], "title": "Galaxy 101"}, {"description": "In this module we will be studying sequence data quality control as well as ChIP-Sequence Analysis with MACS.", "video": ["Sequence Data Quality Control", "ChIP-Sequence Analysis with MACS", "Module 3 Quiz"], "title": "Working with sequence data"}, {"description": "In these final modules, we'll take a look at working with sequence data and RNA-seq and at installing and running your own Galaxy.", "video": ["TopHat availability", "RNA-seq Analysis: Mapping", "RNA Sequence Analysis: Assembly Quantitation, and Differential Expression", "Installing Galaxy Locally", "Galaxy on the Cloud", "Post-Course Survey", "Module 4 Quiz", "Module 5 Quiz", "Course Project"], "title": "RNA-seq & Running your own Galaxy"}], "title": "Genomic Data Science with Galaxy"}, {"course_info": "About this course: Welcome to the Coursera specialization, From Data to Insights with Google Cloud Platform brought to you by the Google Cloud team. I’m Evan Jones (a data enthusiast) and I’m going to be your guide.\n\nThis first course in this specialization is Exploring and Preparing your Data. Here we will see what the common challenges faced by data analysts are and how to solve them with the big data tools on Google Cloud Platform. You’ll pick up some SQL along the way and become very familiar with using BigQuery and Cloud Dataprep to analyze and transform your datasets.", "level": "Beginner", "package_name": "From Data to Insights with Google Cloud Platform Specialization ", "created_by": "Google Cloud", "package_num": "1", "teach_by": [{"name": "Google Cloud Training", "department": null}], "target_audience": "Who is this class for: This is course is primarily aimed at new Data ​Analysts, ​Business ​Analysts, and Business ​Intelligence ​professionals. It is also intended for Cloud ​Data ​Engineers ​who ​will ​be partnering ​with ​Data ​Analysts ​to ​build scalable ​data ​solutions ​on ​Google Cloud ​Platform.\n\nNote that this first course has a module on SQL basics that will be serve as a review for those already familiar with the language.", "rating": "4.4", "week_data": [{"description": "Learn the courses, content, and technologies that are part of this data analyst specialization", "video": ["Introduction: From Data to Insights with Google Cloud Platform specialization"], "title": "Welcome to From ​Data ​to ​Insights ​with ​Google ​Cloud Platform: ​Exploring ​and ​Preparing ​your ​Data"}, {"description": "Understand the core principles behind Google Cloud Platform and how to leverage them for big data analysis", "video": ["Highlight ​Analytics ​Challenges ​Faced ​by ​Data ​Analysts", "Compare ​Big ​Data ​On-Premise ​vs ​on ​the ​Cloud", "Learn ​from ​Real-World ​Use ​Cases ​of ​Companies ​Transformed ​through Analytics ​on ​the ​Cloud", "Navigate ​Google ​Cloud ​Platform ​Project ​Basics", "Lab 0 Overview", "Lab 0: Getting Started with Google Cloud Platform and Qwiklabs", "Lab 0 Review", "Module 1 Quiz"], "title": "Module 1: Introduction ​to ​Data ​on Google ​Cloud ​Platform"}, {"description": "Learn what are the key big data tools on Google Cloud Platform that you will be using to analyze, prepare, and visualize data", "video": ["Walkthrough ​Data ​Analyst ​Tasks, ​Challenges, ​and ​Introduce ​Google Cloud ​Platform ​Data ​Tools", "Demo: ​Analyze ​10 ​Billion ​Records ​with ​Google ​BigQuery", "Explore ​9 ​Fundamental ​Google ​BigQuery ​Features", "Walkthrough: Data Architecture Diagram", "Compare ​GCP ​Tools ​for ​Analysts, ​Data ​Scientists, ​and ​Data ​Engineers", "Lab 1 Overview", "Lab 1: Exploring your Public Dataset with Google BigQuery", "Lab 1 Solution", "Module 2 Quiz"], "title": "Module 2: ​Big ​Data ​Tools ​Overview"}, {"description": "Learn how to query your data with the basics of SQL (Structured Query Language) and practice writing queries in BigQuery", "video": ["Compare Common Data Exploration Techniques", "Explore Real IRS Data", "Query Basics", "Intro to Functions", "Demo: Explore Schemas in the BigQuery UI", "Filters, Aggregates, and Duplicates", "Data Types, Date Functions, and NULLs", "Wildcard Filters with LIKE", "Lab 2 Overview", "Lab 2: Troubleshoot Common SQL Errors with BigQuery", "Lab 2 Solution", "Module 3 Quiz"], "title": "Module 3: ​Exploring ​your ​Data ​with SQL"}, {"description": "Understand how pricing works in BigQuery and how you can best optimize your queries", "video": ["Walkthrough of a BigQuery Job", "Calculate ​BigQuery ​Pricing: ​Storage, ​Querying, ​and ​Streaming ​Costs", "Demo: Try out the Price Calculator", "Reserved Slots", "Query Validator, Quotas, and Common Pitfalls", "Optimize ​Queries ​for ​Cost", "Lab 3 Overview", "Lab 3: Calculating Google BigQuery Pricing", "Lab 3 Solution", "Module 4 Quiz"], "title": "Module 4: ​Google ​BigQuery ​Pricing"}, {"description": "Understand the importance of creating high quality datasets and learn the tools that will help you transform your data", "video": ["Examine ​the ​5 ​Principles ​of ​Dataset ​Integrity", "Characterize ​Dataset ​Shape ​and ​Skew", "Clean ​and ​Transform ​Data ​using ​SQL", "Clean ​and ​Transform ​Data ​using ​a ​new ​UI: ​Introducing ​Cloud ​Dataprep", "Lab 4a Overview", "Lab 4a: Ingesting Datasets and Creating Flows with Cloud Dataprep", "Lab 4a Solution", "Walkthrough of Cloud Dataprep Features", "Lab 4b Overview", "Lab 4b: Exploring and Transforming Data with Cloud Dataprep", "Lab 4b Solution", "Module 5 Quiz"], "title": "Module 5: ​Cleaning ​and ​Transforming your ​Data"}], "title": "Exploring ​and ​Preparing ​your ​Data with BigQuery"}, {"course_info": "About this course: This course will provide learners with an introduction to research data management and sharing. After completing this course, learners will understand the diversity of data and their management needs across the research data lifecycle, be able to identify the components of good data management plans, and be familiar with best practices for working with data including the organization, documentation, and storage and security of data. Learners will also understand the impetus and importance of archiving and sharing data as well as how to assess the trustworthiness of repositories. \n\nToday, an increasing number of funding agencies, journals, and other stakeholders are requiring data producers to share, archive, and plan for the management of their data. In order to respond to these requirements, researchers and information professionals will need the data management and curation knowledge and skills that support the long-term preservation, access, and reuse of data. Effectively managing data can also help optimize research outputs, increase the impact of research, and support open scientific inquiry. After completing this course, learners will be better equipped to manage data throughout the entire research data lifecycle from project planning to the end of the project when data ideally are shared and made available within a trustworthy repository.\n\nThis course was developed by the Curating Research Assets and Data Using Lifecycle Education (CRADLE) Project in collaboration with EDINA at the University of Edinburgh. \n\nThis course was made possible in part by the Institute of Museum and Library Services under award #RE-06-13-0052-13. The views, findings, conclusions or recommendations expressed in this Research Data Management and Sharing MOOC do not necessarily represent those of the Institute of Museum and Library Services.\n\nHashtag: #RDMSmooc", "level": null, "package_name": null, "created_by": "The University of North Carolina at Chapel Hill, The University of Edinburgh", "package_num": null, "teach_by": [{"name": "Helen Tibbo", "department": "School of Information and Library Science"}, {"name": "Sarah Jones", "department": "Digital Curation Centre, UK"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "This week introduces multiple types of research data in an array of contexts as well as important data management concepts including metadata and the research data lifecycle. We will also define the concept of data management, identify the roles and responsibilities of key stakeholders, and examine various data management tasks throughout the research data lifecycle. ", "video": ["Welcome", "Research Data Defined", "Types of Data and Metadata", "Research Data Lifecycle", "Summary & Additional Resources", "Why Manage Data?", "Data Management Stakeholders", "Data Management Across the Research Lifecycle", "Summary & Additional Resources", "\"What Are Data?\"", "\"Why is Data Management Important?\"", "Describing Data", "Understanding Research Data"], "title": "Understanding Research Data"}, {"description": "This week provides an overview of Data Management Plans (DMPs) including the components of good DMPs, the DMP policies of several funding agencies, and information on data management planning tools.  ", "video": ["Introduction to Data Management Plans", "Funding Agency Requirements", "Data Management Plan Content", "Data Management Planning Tools", "Summary & Additional Resources", "Writing a Data Management Plan", "Data Management Planning"], "title": "Data Management Planning"}, {"description": "This week is brought to you by EDINA and the Data Library at the University of Edinburgh and is presented by Sarah Jones from the Digital Curation Centre. Sarah will introduce strategies for organizing research data including versioning and file naming conventions as well as data file formatting and transformations. She will also discuss why documenting data and data citation are important. Finally, she will present issues involved in storing, securing, and backing up research data.   ", "video": ["Good File Management in Research", "File Naming", "Versioning", "Organizing Data", "Summary & Additional Resources", "File Formats", "Data Transformations", "File Formats and Transformations", "Summary & Additional Resources", "Documentation", "Data Citation", "Documentation and Data Citation", "Summary & Additional Resources", "Storage", "Backup", "Data Security", "Encryption", "Storage and Security", "Summary & Additional Resources", "Interview - Natalia Calanzani", "Interview - Shaun Bevan", "\"How does good data management add value to research?\"", "\"Do you think data sharing helps to reduce waste and increase transparency of research?\"", "Working with Data"], "title": "Working with Data"}, {"description": "This week examines the benefits and challenges of sharing research data. We will also discuss how to protect confidentiality and how data ownership can affect data sharing. Finally, we will examine different types of access restrictions that may be placed on data as well as how to enable data sharing through the application of a standard license.", "video": ["Benefits of Sharing", "Challenges to Sharing", "Data Citations", "Summary & Additional Resources", "Protecting Confidentiality (Part 1)", "Protecting Confidentiality (Part 2)", "Intellectual Property and Data Ownership", "Access", "Summary & Additional Resources", "\"What Are the Benefits of Sharing Data?\"", "\"What Are the Drawbacks of Sharing Data?\"", "Citing Data", "Sharing Data"], "title": "Sharing Data"}, {"description": "During the final week of the course, we will examine the preservation needs of research data, introduce the concepts of authenticity and integrity, and identify the different types of metadata and their role in data discovery and reuse. We will also discuss the role of trustworthy repositories as well as how repositories demonstrate their trustworthiness through audit and certification. Finally, we will present key archival standards and best practices for ensuring data remains accessible and understandable for the long-term.", "video": ["Why Archive Data?", "Authenticity and Integrity", "Metadata", "Summary & Additional Resources", "Demonstrating Trustworthiness", "Data Curation Standards and Best Practices (Part 1)", "Data Curation Standards and Best Practices (Part 2)", "Summary & Additional Resources", "\"Why is Archiving Data Important?\"", "\"Why is Digital Preservation Important?\"", "Finding a Trustworthy Repository", "Conclusion", "Archiving Data"], "title": "Archiving Data"}], "title": "Research Data Management and Sharing"}, {"course_info": "About this course: Writing good code for data science is only part of the job. In order to maximizing the usefulness and reusability of data science software, code must be organized and distributed in a manner that adheres to community-based standards and provides a good user experience. This course covers the primary means by which R software is organized and distributed to others. We cover R package development, writing good documentation and vignettes, writing robust software, cross-platform development, continuous integration tools, and distributing packages via CRAN and GitHub. Learners will produce R packages that satisfy the criteria for submission to CRAN.", "level": "Intermediate", "package_name": "Mastering Software Development in R Specialization ", "created_by": "Johns Hopkins University", "package_num": "3", "teach_by": [{"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Brooke Anderson", "department": "Colorado State University"}], "target_audience": null, "rating": "4.3", "week_data": [{"description": "", "video": ["Welcome to Building R Packages", "Before You Start", "Using Mac OS", "Using Windows", "Using Unix/Linux", "R packages", "Basic Structure of an R Package", "DESCRIPTION File", "NAMESPACE File", "Namespace Function Notation", "Loading and Attaching a Package Namespace", "The R Sub-directory", "The man Sub-directory", "Summary", "The devtools package", "Creating a Package", "Other Functions", "R Package and devtools"], "title": "Getting Started with R Packages"}, {"description": "", "video": ["Documentation", "Vignette's and README Files", "Knitr / Markdown", "Common knitr Options", "Help Files and roxygen2", "Common roxygen2 Tags", "Overview", "Data for Demos", "Internal Data", "Data Packages", "Summary", "Introduction", "The testthat Package", "Passing CRAN Checks", "Documenting Code"], "title": "Documentation and Testing"}, {"description": "", "video": ["Overview", "The General Public License", "The MIT License", "The CC0 License", "Overview", "Paying it Forward", "Linus’s Law", "Hiring", "Summary", "Introduction", "git", "Initializing a git repository", "Committing", "Browsing History", "Linking local repo to GitHub repo", "Syncing RStudio and GitHub", "Issues", "Pull Request", "Merge Conflicts", "Introduction", "The Unix Philosophy", "Default Values", "Naming Things", "Playing Well With Others", "Summary", "Testing, GitHub, and Open Source"], "title": "Licensing, Version Control, and Software Design"}, {"description": "", "video": ["Overview", "Web Services for Continuous Integration", "Using Travis", "Using AppVeyor", "Summary", "Introduction", "Handling Paths", "Saving Files & rappdirs", "rappdirs", "Options and Starting R", "Package Installation", "Environmental Attributes", "Summary", "Building an R Package"], "title": "Continuous Integration and Cross Platform Development"}], "title": "Building R Packages"}, {"course_info": "About this course: Learn to analyze big data using Apache Spark's distributed computing framework. \n\nIn a series of focused, practical tasks, you will start by launching a spark cluster on Amazon's EC2 cloud computing platform. As you progress to working with real data, you will gain exposure to a variety of useful tools, including RDFlib and SPARQL. \n \nThe practical tasks on this course make use of the Gutenberg Project data - the world's largest open collection of ebooks. This offers no end of opportunity for highly engaging and novel analyses.\n\nAs the taught material and example code is given in Python, it is strongly recommended that all students have previous Python programming experience. Furthermore, launching and interacting with a cluster on EC2 requires basic knowledge of Unix command line, and some experience with a command-line editor such as vim or nano would also be advantageous.\n\nWith these minimal prerequisites, this course is designed to get you up and running in Spark as quickly and painlessly as possible, so that by the end, you will be comfortable and competent enough to start engineering your own big data solutions.", "level": "Intermediate", "package_name": null, "created_by": "University of London", "package_num": null, "teach_by": [{"name": " Dr Sorrel Harriet", "department": "Computing"}, {"name": "Christophe Rhodes", "department": "Department of Computing, Goldsmiths"}], "target_audience": "Who is this class for: This course is primarily aimed at post-graduates and those in the science and engineering fields with an interest in distributed computing and big data analysis. The course may also be of interest to 3rd year undergraduate students undertaking research projects.", "rating": null, "week_data": [{"description": "This week, you'll gain essential background knowledge along with the practical skills needed to run applications in Apache Spark. You'll also take the steps necessary to launch a Spark cluster on the Amazon EC2 cloud computing platform.", "video": ["Course introduction", "About this course", "Prerequisite Skills Quiz", "Week 1 Resource zip", "Introduction", "Week 1 Introduction Quiz", "Tips for following this lesson", "Create a normal AWS account", "Create a normal AWS account with billing alarm", "Accessing AWS educate credits", "Lesson 1.1 Practice Quiz", "Set up AWS account with a billing alarm", "Tips for following this video", "Launch a Spark cluster on EC2", "Launch a Spark cluster on EC2", "Additional guidance for starter accounts", "Accessing the pyspark interactive shell", "Lesson 1.2 Practice Quiz", "Tips for following this lesson", "How to install Spark locally", "What is Spark?", "Fundamentals", "Lesson 1.3 Practice Quiz", "Tips for following this lesson", "Setting up your development environment", "Setting up your development environment", "Lesson 1.4 Practice Quiz", "Submitting applications to a cluster", "Lesson 1.5 Practice Quiz", "Summary", "Week 1 Summary Quiz", "Run a test application on a Spark cluster"], "title": "Getting Started in Spark on EC2"}, {"description": "This week you'll learn how to read and write data in Spark. The techniques you'll be shown can be used with data stored locally, or in partnership with the Amazon S3 cloud storage facility. To help get you started, we'll also show you how to upload a subset of the Gutenberg Project dataset onto Amazon S3.", "video": ["Introduction", "Week 2 Resources zip", "Get the Gutenberg project dataset", "Lesson 2.1 Practice Quiz", "Tips for following this lesson", "Reading and writing RDDs", "Using Spark methods to read and write data on S3", "Lesson 2.2 Practice Quiz", "Tips for following this lesson", "Reading data from Amazon S3 with boto3", "Using boto3 to read data from Amazon S3", "Lesson 2.3 Practice Quiz", "Tips for following this lesson", "2.4 Writing objects to Amazon S3 (Spark methods)", "Configuring Spark for accessing S3", "Lesson 2.4 Practice Quiz", "Week 2 Summary Quiz", "Read and Write Data on S3"], "title": "Reading and Writing Data"}, {"description": "This week you'll be getting to grips with some useful tools in preparation for working with the Gutenberg Project data set. In this week's assessment, you will exercise your data wrangling skills to produce a catalogue index file from the Gutenberg Project meta data, a resource that should prove useful in your final assessment.", "video": ["Week 3 Resources zip", "Tips for following this lesson", "Tools for working with data Regex", "Lesson 3.1 Practice Quiz", "What is RDF?", "Lesson 3.2 Practice Quiz", "Tips for following this lesson", "Using RDFLib", "Lesson 3.3 Practice Quiz", "Summary", "Week 3 Summary Quiz", "Produce a catalogue index file from the Gutenberg Project meta data"], "title": "Tools for Working with Data"}, {"description": "This week you'll learn Spark programming in some detail, in preparation for working with the Gutenberg collection of ebooks. The areas that will be covered should lead you to write much more efficient and successful Spark applications.", "video": ["Introduction", "Week 4 Resources zip", "Tips for following this lesson", "4.1 Working with data frames", "Lesson 4.1 Practice Quiz", "Tips for following this lesson", "Pipelines and cacheing", "Lesson 4.2 Practice Quiz", "Tips for following this lesson", "Spark performance", "Lesson 4.3 Practice Quiz", "Spark configuration", "Lesson 4.4 Practice Quiz", "Tips for following this lesson", "Spark examples", "Lesson 4.5 Practice Quiz", "Summary", "Summary", "Week 4 Summary Quiz", "Gutenberg Project gender pronoun count"], "title": "Programming in Spark"}], "title": "Introduction to Apache Spark and AWS"}, {"course_info": "About this course: If I Googled you, what would I find?\n\nAs we move around the online world we leave tracks and traces of our activity all the time: social media accounts, tagged images, professional presences, scraps of text, but also many artefacts we don't always realise we are leaving behind, or that others leave about us.  \n\nIn this course you will hear from a range of experts and you will have an opportunity to explore and reflect on your own online tracks and traces, to understand why your digital footprint is important. We will introduce you to some of the tools and approaches to effectively manage your online presence (or digital footprint).  \n\nThe course will focus on the different dimensions of a digital footprint, including developing an effective online presence, managing your privacy, creating opportunities for networking, balancing and managing professional and personal presences (eprofessionalism). By the end of this course (MOOC) you should be equipped to ensure that your digital footprint works for you, whether you want to be more private online, or are looking to create a more effective and impactful presence.  \n\nYou can also join the conversation on Twitter using the hashtag #DFMOOC and follow us @DFMOOC\n\nWe hope you enjoy the course!", "level": "Beginner", "package_name": null, "created_by": "The University of Edinburgh", "package_num": null, "teach_by": [{"name": "Louise Connelly", "department": "University of Edinburgh"}, {"name": "Nicola Osborne", "department": "EDINA, University of Edinburgh"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "In week 1 we provide information on how to engage with the course as well as our Twitter account, using #DFMOOC.  This week, we will introduce you to the topic of Digital Footprint. There are a range of activities, videos and resources for you to work through. By the end of week 1, you will have the opportunity to critically reflect on your own online presence and consider how you can make better informed choices as well as setting appropriate personal goals around your digital footprint. ", "video": ["How to use this course", "Using social media on this course", "Introduction to the course and week 1 (Nicola Osborne)", "About week 1", "Step 1: Have you looked?", "Step 2: Search for yourself online", "Step 3: What did you find?", "What is a digital footprint?", "The Uncontainable Self - Part 1 (Nicola Osborne)", "The Uncontainable Self and Professional Identity - Part 2 (Nicola Osborne)", "Taking Gaming Into Adulthood (Dr Jill MacKay)", "Who owns your data? - Part 1 (Nicola Osborne)", "Who Owns Your Data? - Part 2 (Nicola Osborne)", "Who Owns Your Data? - Part 3 (Nicola Osborne)", "Week 1 Additional Resources", "Week 1 graded quiz"], "title": "What makes an online presence effective?"}, {"description": "This week, there is a range of experts who focus on why a digital footprint matters and how to create an effective online presence. The themes explored include data after death, privacy online and managing your online data. The activity and quizzes will help you to consider your own online presence and what you might do to make it more effective and work for you!", "video": ["Introduction to week 2 (Dr Louise Connelly)", "About week 2", "Step 1: Zeemap Activity: Introducing yourself and giving advice", "Step 2: Zeemap Activity - Review (Dr Louise Connelly)", "Reflective question: Privacy (non-assessed)", "Creating an effective online presence (Dr Louise Connelly)", "Digital Footprints (Dr Rachel Buchanan)", "Managing your online presence (Interview with Dr Ben Marder)", "Tag Review Function, Taking Control Back (Dr Ben Marder)", "Checklist: creating an effective online presence", "Digital Footprint Action Plan", "Our Data, Our Rights (Interview with Prof. Lilian Edwards)", "Death and Privacy (Interview  with Prof. Lilian Edwards)", "Data after death (Interview with Prof. Lilian Edwards)", "Privacy Tips 1-6 (Nicola Osborne)", "Privacy Tips 7-12 (Nicola Osborne)", "Audience online, time/longevity of data, visibility  (Interview with David Brake)", "Audience (Interview with David Brake)", "Time (Interview with David Brake)", "One piece of advice about managing your digital footprint", "Week 2 Additional Resources", "Reflecting on your digital footprint", "Week 2 graded quiz"], "title": "Why does your digital footprint matter?"}, {"description": "This week, we examine the idea of a professional online presence and what this might mean for different people and professions. Understanding whether your personal and professional online presence should blur or be kept separate can be a challenge. We will hear from experts in Business, Nursing, Science, Education, and a Careers Consultant. They will provide useful advice on what you can do to make your online presence more professional and potentially help you with finding a job,  standing out from the crowd, complying with professional bodies' guidelines, and much more. The activities, including the quizzes and peer assessment will help you to reflect on your own online presence. There will also be an opportunity to get advice from your peers and consider putting into action what you have learned during this course.", "video": ["Welcome to week 3 (Dr Louise Connelly)", "About week 3", "Activity: draw your digital footprint (Gallery)", "Professional networking online (Rebecca Valentine)", "5 Tips - creating an effective online presence (Rebecca Valentine)", "Recruitment & business (Copil Yanez)", "Student leaders and online presence (Tanya Lubicz-Nawrocka)", "Professional bodies (Dr Louise Connelly)", "E-professionalism & Nursing - Part 1 (Interview with Sharon Levy)", "E-Professionalism & Nursing - Part 2 (Interview with Sharon Levy)", "The E-Nurse (Interview with Sharon Levy)", "Professional Bodies' Guidelines", "Online Academic Presence (Dr Jill MacKay)", "Academic presence: social sciences - part 1 (Interview with Dr Karen Gregory)", "Academic presence: social sciences - Part 2 (Interview with Dr Karen Gregory)", "How social media has been beneficial", "One piece of advice about creating a professional online presence", "Week 3 Additional Resources", "Digital Footprint Survey", "Week 3 graded quiz"], "title": "What does it mean to be an effective online professional?"}], "title": "Digital Footprint"}, {"course_info": "About this course: The capstone project class will allow students to create a usable/public data product that can be used to show your skills to potential employers. Projects will be drawn from real-world problems and will be conducted with industry, government, and academic partners.", "level": null, "package_name": "Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "10", "teach_by": [{"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "This week, we introduce the project so you can get a clear grip on the problem at hand and begin working with the dataset.", "video": ["Welcome to the Capstone Project", "Project Overview", "Welcome from SwiftKey", "You Are a Data Scientist Now", "Syllabus", "Introduction to Task 0: Understanding the Problem", "Task 0 - Understanding the problem", "About the Copora", "Introduction to Task 1: Getting and Cleaning the Data", "Task 1 - Getting and cleaning the data", "Regular Expressions: Part 1 (Optional)", "Regular Expressions: Part 2 (Optional)", "Quiz 1: Getting Started"], "title": "Overview, Understanding the Problem, and Getting the Data"}, {"description": "This week, we move on to the next tasks, exploratory data analysis and modeling. You'll also submit your milestone report and review submissions from your classmates.", "video": ["Introduction to Task 2: Exploratory Data Analysis", "Task 2 - Exploratory Data Analysis", "Introduction to Task 3: Modeling", "Task 3 - Modeling", "Milestone Report"], "title": "Exploratory Data Analysis and Modeling"}, {"description": "This week, you'll build and evaluate your prediction model. The goal is to make your model efficient and accurate. ", "video": ["Introduction to Task 4: Prediction Model", "Task 4 - Prediction Model", "Quiz 2: Natural language processing I"], "title": "Prediction Model"}, {"description": "This week's goal is to improve the predictive accuracy while reducing computational runtime and model complexity.", "video": ["Introduction to Task 5: Creative Exploration", "Task 5 - Creative Exploration", "Quiz 3: Natural language processing II"], "title": "Creative Exploration"}, {"description": "This week, you'll work on developing the first component of your final project, your data product. ", "video": ["Introduction to Task 6: Data Product", "Task 6 - Data Product"], "title": "Data Product "}, {"description": "This week, you'll work on developing the second component of your final project, a slide deck to accompany your data product. ", "video": ["Introduction to Task 7: Slide Deck", "Task 7 - Slide Deck"], "title": "Slide Deck"}, {"description": "This week, you'll submit your final project and review the work of your classmates.", "video": ["Congratulations!", "Final Project Submission"], "title": "Final Project Submission and Evaluation "}], "title": "Data Science Capstone"}, {"course_info": "About this course: Relational Database Support for Data Warehouses is the third course in the Data Warehousing for Business Intelligence specialization. In this course, you'll use analytical elements of SQL for answering business intelligence questions. You'll learn features of relational database management systems for managing summary data commonly used in business intelligence reporting. Because of the importance and difficulty of managing implementations of data warehouses, we'll also delve into storage architectures, scalable parallel processing, data governance, and big data impacts.", "level": null, "package_name": "Data Warehousing for Business Intelligence Specialization ", "created_by": "University of Colorado System", "package_num": "3", "teach_by": [{"name": "Michael Mannino", "department": "Business School, University of Colorado Denver"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "Module 1 introduces the course and covers concepts that provide a context for the remainder of this course. In the first two lessons, you’ll understand the objectives for the course and know what topics and assignments to expect. In the remaining lessons, you will learn about DBMS extensions, a review of schema patterns, data warehouses used in practice problems and assignments, and examples of data warehouses in education and health care. This informational module will ensure that you have the background for success in later modules that emphasize details and hands-on skills.You should also read about the software requirements in the lesson at the end of module 1. I recommend that you try to install the Oracle software this week before assignments begin in week 2. If you have taken other courses in the specialization, you may already have installed the Oracle software.", "video": ["Course introduction video", "Course objectives video lecture", "Powerpoint lecture notes for lesson 1", "Course topics and assignments video lecture", "Powerpoint lecture notes for lesson 2", "Optional textbook", "DBMS extensions video lecture", "Powerpoint lecture notes for lesson 3", "Relational database schema patterns video lecture", "Powerpoint lecture notes for lesson 4", "Colorado Education Data Warehouse video lecture", "Powerpoint lecture notes for lesson 5", "Data warehouse standards in health care video lecture", "Powerpoint lecture notes for lesson 6", "Overview of software requirements", "Overview of database software installation", "Oracle installation notes", "Making connections to a local Oracle database", "SQL statements for Store Sales tables", "SQL statements for Inventory tables", "Optional textbook reading material", "Module 1 quiz"], "title": "DBMS Extensions and Example Data Warehouses"}, {"description": "Now that you have the informational context for relational database support of data warehouses, you’ll start using relational databases to write business intelligence queries! In module 2, you will learn an important extension of the SQL SELECT statement for subtotal operators. You’ll apply what you’ve learned in practice and graded problems using Oracle SQL for problems involving the CUBE, ROLLUP, and GROUPING SETS operators. Because the subtotal operators are part of the SQL standard, your learning will readily apply to other enterprise DBMSs. At the end of this module, you will have solid background to write queries using the SQL subtotal operators as a data warehouse analyst.", "video": ["GROUP BY clause review video lecture", "Powerpoint lecture notes for lesson 1", "Additional problems for lesson 1", "SQL CUBE operator video lecture", "Powerpoint lecture notes for lesson 2", "Additional problems for lesson 2", "SQL ROLLUP operator video lecture", "Powerpoint lecture notes for lesson 3", "Additional problems for lesson 3", "SQL GROUPING SETS operator video lecture", "Powerpoint lecture notes for lesson 4", "Additional problems for lesson 4", "Variations of subtotal operators video lecture", "Powerpoint lecture notes for lesson 5", "Additional problems for lesson 5", "Optional textbook reading material", "Assignment notes", "Module 2 quiz", "Quiz for module 2 assignment", "Assignment for module 2"], "title": "SQL Subtotal Operators"}, {"description": "After your experience using the SQL subtotal operators, you are ready to learn another important SQL extension for business intelligence applications. In module 3, you will learn about an extended processing model for SQL analytic functions that support common analysis in business intelligence applications. You’ll apply what you’ve learned in practice and graded problems using Oracle SQL for problems involving qualitative ranking of business units, window comparisons showing relationships of business units over time, and quantitative contributions showing performance thresholds and contributions of individual business units to a whole business. Because analytic functions are part of the SQL standard, your learning will apply to other enterprise DBMSs. At the end of this module, you will have solid background to write queries using the SQL analytic functions as a data warehouse analyst.", "video": ["Processing Model and Basic Syntax video lecture", "Powerpoint lecture notes for lesson 1", "Additional  problems for lesson 1", "Extended Syntax and Ranking Functions video lecture", "Powerpoint lecture notes for lesson 2", "Additional problems for lesson 2", "Window Comparison I video lecture", "Powerpoint lecture notes for lesson 3", "Additional problems for lesson 3", "Window Comparisons II video lecture", "Powerpoint lecture notes for lesson 4", "Additional problems for lesson 4", "Functions for Ratio Comparisons video lecture", "Powerpoint lecture notes for lesson 5", "Additional problems for lesson 5", "Optional textbook reading material", "Assignment notes", "Module 3 quiz", "Quiz for module 3 assignment", "Assignment for module 3"], "title": "SQL Analytic Functions"}, {"description": "After acquiring query formulation skills for development of business intelligence applications, you are ready to learn about DBMS extensions for efficient query execution. Business intelligence queries can use lots of resources so materialized view processing and design has become an important extension of DBMSs. In module 4, you will learn about an SQL statement for creating materialized views, processing requirements for materialized views, and rules for rewriting queries using materialized views. To gain insight about the complexity of query rewriting, you will practice rewriting queries using materialized views. To provide closure about relational database support for data warehouses, you will learn about about Oracle tools for data integration, the Oracle Data Integrator, along with two SQL statements useful for specific data integration tasks. After this module, you will have a solid background to use materialized views to improve query performance and deploy the Extraction, Loading, and Transformation approach for data integration as a data warehouse administrator or analyst.", "video": ["Background on traditional views video lecture", "Powerpoint lecture notes for lesson 1", "Additional problems for lesson 1", "Materialized view definition and processing video lecture", "Powerpoint lecture notes for lesson 2", "Additional problems for lesson 2", "Query Rewriting Rules video lecture", "Powerpoint lecture notes for lesson 3", "Query Rewriting Examples video lecture", "Powerpoint lecture notes for lesson 4", "Additional problems for lesson 4", "Oracle Tools for Data Integration video lecture", "Powerpoint lecture notes for lesson 5", "Additional problems for lesson 5", "Optional textbook reading material", "Assignment notes", "Module 4 quiz", "Quiz for module 4 assignment", "Assignment for module 4"], "title": "Materialized View Processing and Design"}, {"description": "Module 5 finishes the course with a return to conceptual material about physical design technologies and data governance practices. You will learn about storage architectures, scalable parallel processing, big data issues, and data governance. After this module, you will have background about conceptual issues important for data warehouse administrators.", "video": ["Storage Architectures video lecture", "Powerpoint lecture notes for lesson 1", "Scalable Parallel Processing Approaches video lecture", "Powerpoint lecture notes for lesson 2", "Big data issues video lecture", "Powerpoint lecture notes for lesson 3", "Data Governance video lecture", "Powerpoint lecture notes for lesson 4", "Optional textbook reading material", "Closing Lecture", "Module 5 quiz"], "title": "Physical Design and Governance"}], "title": "Relational Database Support for Data Warehouses"}, {"course_info": "About this course: This course is designed to provide you with an understanding of the role of data and technology in human capital management. Every topic in the course will be covered in the most practical way so that learners get hands-on experience. In the course we use the 4Ts principle: Task, Theory, Technique and Technology so that there is always a connection to organizational performance objectives, an overview of underlying theories and principles, and specific tools which help achieve business objectives.\n\nYou will learn \n●\twhat combination of data, technologies, and tools can be used in people management processes to improve organization’s performance\n●\thow to use some of these tools and how to select the ones that suit your objectives and budget\n●\tto design individual and team development plans and measure its ROI for the organization\n●\thow to figure out the qualities that lead employees to their best performance so you know what to encourage in current and look for in new employees\n●\thow to identify the right channels to recruit your employees or team members\n●\twhat combination of monetary and non-monetary motivation tools work best for your organization \n●\thow to predict what people will leave in the near future and how to make sure some of them stay\n●\thow to measure engagement and make a strong organizational culture improve performance", "level": null, "package_name": null, "created_by": "Moscow Institute of Physics and Technology", "package_num": null, "teach_by": [{"name": "Ilya Breyman", "department": "Center of Innovative Educational Technologies"}, {"name": "Alexey Dolinskiy", "department": "Center of Innovative Educational Technologies"}], "target_audience": null, "rating": "4.0", "week_data": [{"description": "As the name suggests, this module will introduce the course and its content, as well as give an overview of what triggered the explosion of People Analytics. ", "video": ["Introduction Video", "Grading Policy", "Communication Rules", "Lecture 1 About the Course", "Lecture 2 HR Transformation", "Lecture 3 Course Structure", "Reading list", "Module I Quiz"], "title": "Introduction"}, {"description": "In this module, we talk about measuring performance, setting KPIs, organizing performance evaluation and touch upon the issue of biases. In your first peer-reviewed assignment you will get to apply your knowledge as a manager of a sports club. You can get in the right mood by watching (or rewatching) Moneyball. ", "video": ["Lecture 1 Why Performance Matters", "Lecture 2 Setting Objectives and Measuring Performance", "Lecture 3 Data Presentation", "Reading list", "Module 2. Applying skills", "Module II Quiz", "Anyball Team Performance"], "title": "Performance"}, {"description": "Culture is a very elusive topic, but it is still measurable and it affects performance a lot. In this module we discuss its various aspects, why it affects performance, and what are the \"enemies\" of performance-enhancing organizational cultures", "video": ["Lecture 1 Defining Culture", "Lecture 2 Understanding Culture", "Lecture 3 Assessing Culture", "Reading list", "Module 3. Applying skills", "Module III Quiz"], "title": "Culture and Assessments"}, {"description": "In this module we will talk about various compensation models and you will learn how to do compensation benchmarking. In your second peer-reviewed assignment you will apply the new knowledge to conduct your own benchmarking project.  ", "video": ["Lecture 1 Compensation and Performance", "Lecture 2 Compensation Models", "Lecture 3 Benchmarking", "Reading list", "Module 4. Applying skills", "Module IV Quiz", "Benchmarking compensation for a university professor"], "title": "Compensation"}, {"description": "After discussing how monetary motivation works, we continue with non-monetary motivation and engagement and how they affect performance. ", "video": ["Lecture 1 Understanding Motivation", "Lecture 2 Motivation Tools", "Lecture 3 Engagement", "Reading list", "Module 5. Applying skills", "Module V Motivation and Engagement"], "title": "Motivation and Engagement"}, {"description": "In this module, you will learn about recruitment analytics, assessing candidates, optimizing recruitment channels, etc. Your third peer-reviewed assignment will be baaed on a real-life dataset of a startup which took analytical approach to candidate marketing.  ", "video": ["Lecture 1 Workforce Planning Analytics", "Lecture 2 Recruitment Funnel", "Lecture 3 Selection Tools", "Reading list", "Module 6. Applying skills", "Module VI. Recruiting", " Analyzing Startup's Recruitment Funnel"], "title": "Workforce Planning and Recruitment"}, {"description": "The final module is dedicated to training and development planning and calculating its ROI. We hope you will enjoy the final peer-reviewed assignment, where you will be required to calculate the ROI of this class for you. ", "video": ["Lecture 1 Analyzing Learning Needs", "Lecture 2 Designing Training Programs", "Lecture 3 ROI Calculation", "Reading List", "Module 7. Applying skills", "Module VII. Development", "Calculating Your Development ROI"], "title": "Development"}], "title": "Introduction to People Analytics"}, {"course_info": "About this course: Introduces to the commands that you need to manage and analyze directories, files, and large sets of genomic data. This is the fourth course in the Genomic Big Data Science Specialization from Johns Hopkins University.", "level": null, "package_name": "Genomic Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "5", "teach_by": [{"name": "Liliana Florea, PhD", "department": "McKusick-Nathans Institute of Genetic Medicine"}], "target_audience": null, "rating": "4.2", "week_data": [{"description": "In this module, you will be introduced to command Line Tools for Genomic Data Science", "video": ["Welcome Message", "Syllabus", "VMBox Download & Instructions", "Pre-Course Survey", "Basic Unix Commands 1: Content Representation", "Basic Unix Commands 2: Files, Directories, Paths", "Basic Unix Commands 3: File Naming", "Basic Unix Commands 4: Content Creation", "Basic Unix Commands 5: Accessing Content I", "Basic Unix Commands 6: Accessing Content II", "Basic Unix Commands 7: Redirecting Content", "Basic Unix Commands 8: Querying Content", "Basic Unix Commands 9: Comparing Content", "Basic Unix Commands 10: Archiving Content", "Basic Unix Commands 11: Practical Exercises I", "Basic Unix Commands 12: Practical Exercises II", "Module 1 Exam Instructions **IMPORTANT**", "Module 1 Quiz", "Module 1 Exam"], "title": "Basic Unix Commands"}, {"description": "In this module, we'll be taking a look at Sequences and Genomic Features in a sequence of 10 presentations. ", "video": ["Sequences and Genomic Features 1: Molecular Bio Primer", "Sequences and Genomic Features 2: Sequence Representation and Generation", "Sequences and Genomic Features 3: Annotation", "Sequences and Genomic Features 4.1: Alignment I", "Sequences and Genomic Features 4.2: Alignment II", "Sequences and Genomic Features 5: Recreating Sequences & Features", "Sequences and Genomic Features 6: Genomic Feature Retrieval", "Sequences and Genomic Features 7: SAMtools I", "Sequences and Genomic Features 8: SAMtools II", "Sequences and Genomic Features 9: BEDtools I", "Sequences and Genomic Features 10: BEDtools II", "Module 2 Exam Instructions **IMPORTANT**", "Module 2 Quiz", "Module 2 Exam"], "title": "Week Two"}, {"description": "In this module, we'll be going over Alignment and Sequence Variation in another sequence of 8 presentations.", "video": ["Alignment & Sequence Variation 1: Overview", "Alignment & Sequence Variation 2: Alignment & Variant Detection Tools", "Alignment & Sequence Variation 3: VCF", "Alignment & Sequence Variation 4: Bowtie", "Alignment & Sequence Variation 5: BWA ", "Alignment & Sequence Variation 6: SAMtools (mpileup)", "Alignment & Sequence Variation 7: BCFtools", "Alignment & Sequence Variation 8: Variant Calling", "Module 3 Exam Instructions **IMPORTANT**", "Module 3 Quiz", "Module 3 Exam"], "title": "Week Three"}, {"description": "In this module, we'll be going over Tools for Transcriptomics in a sequence of 6 presentations.", "video": ["Tools for Transcriptomics 1: Overview", "Tools for Transcriptomics 2: RNA-seq", "Tools for Transcriptomics 3.1: Tophat I", "Tools for Transcriptomics 3.2: Tophat II ", "Tools for Transcriptomics 4: Cufflinks", "Tools for Transcriptomics 5: Cuffdiff", "Tools for Transcriptomics 6.1: Integrated Genomics Viewer I", "Tools for Transcriptomics 6.2: Integrated Genomics Viewer II ", "Module 4 Exam Instructions **IMPORTANT**", "Post-Course Survey", "Module 4 Quiz", "Module 4 Exam"], "title": "Week Four"}], "title": "Command Line Tools for Genomic Data Science"}, {"course_info": "About this course: Learn how probability, math, and statistics can be used to help baseball, football and basketball teams improve, player and lineup selection as well as in game strategy.", "level": "Beginner", "package_name": null, "created_by": "University of Houston System", "package_num": null, "teach_by": [{"name": "Professor Wayne Winston", "department": "Bauer College of Business"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "", "video": ["All you will learn", "Suggested Textbooks (Not Required)"], "title": "Before you start..."}, {"description": "You will learn how to predict a team’s won loss record from the number of runs, points, or goals scored by a team and its opponents. Then we will introduce you to multiple regression and show how multiple regression is used to evaluate baseball hitters. Excel data tables, VLOOKUP, MATCH, and INDEX functions will be discussed.  ", "video": ["Download Excel Files (Important!)", "1.1 Introduction", "Excel Files", "Recommended reading", "1.2 Pythagorean Theorem", "Excel file", "Recommended reading", "1.3 Ten runs = One win", "Homework Problems 1 and 2", "Excel file", "Excel file", "Recommended reading", "1.4 Runs created", "Excel Files", "Recommended reading", "1.5 Runs Created Per Game", "Homework Problem 3", "Excel Files", "Recommended reading", "1.6 Multiple Regression 1", "Excel Files", "Recommended reading", "1.7 Multiple Regression 2", "Excel Files", "Recommended reading", "1.8 Linear Weights", "Excel Files", "Recommended reading", "1.9 On Base Percentage + Slugging Percentage", "Excel Files", "Recommended reading", "1.10 Barry Bonds Runs per Game", "Homework Problems 4", "Test Questions 1 and 2", "Test Question 3", "Test Question 4"], "title": "Module 1"}, {"description": "You will concentrate on learning important Excel tools including Range Names, Tables, Conditional Formatting, PivotTables, and the family of COUNTIFS, SUMIFS, and AVERAGEIFS functions.  You will concentrate on learning important Excel tools including Range Names, Tables, Conditional Formatting, PivotTables, and the family of COUNTIFS, SUMIFS, and AVERAGEIFS functions.  ", "video": ["Download Excel Files (Important!)", "Excel Files", "Recommended Reading", "2.1 Excel Range Names", "Excel Files", "Recommended Reading", "2.2 Excel Tables", "Excel Files", "Recommended Reading", "2.3 Conditional Formatting: Top Bottom Rules", "Excel Files", "Recommended Reading", "2.4 Conditional Formatting: Data Bars", "Excel Files", "Recommended Reading", "2.5 Conditional Formatting: The Formula Option", "Excel Files", "Recommended Reading", "2.6 COUNTIF and COUNTIFS functions", "Excel Files", "Recommended Reading", "2.7 SUMIF and SUMIFS Functions", "Excel Files", "Recommended Reading", "2.8 AVERAGEIF and AVERAGEIFS Functions", "Homework Problem 5", "Excel Files", "Recommended Reading", "2.9 PivotTables I", "Excel Files", "Recommended Reading", "2.10 PivotTables II", "Homework Problem 6", "Test Question 5", "Test Question 6"], "title": "Module 2"}, {"description": "You will learn how Monte Carlo simulation works and how it can be used to evaluate a baseball team’s offense and the famous DEFLATEGATE controversy.", "video": ["Download Excel Files (Important!)", "Excel Files", "Recommended Reading", "3.1 Monte Carlo Simulation I", "Excel Files", "3.2 Monte Carlo Simulation Part II", "Excel Files", "3.3 Monte Carlo Simulation Part III", "Excel Files", "3.4 Baseball Monte Carlo simulation", "Excel Files", "Recommended Reading", "3.5 Resampling", "Excel Files", "3.6 Deflategate", "Homework Problem 8", "Test Question 7", "Test Question 8"], "title": "Module 3"}, {"description": "You will learn how to evaluate baseball fielding, baseball pitchers, and evaluate in game baseball decision-making. The math behind WAR (Wins above Replacement) and Park Factors will also be discussed. Modern developments such as infield shifts and pitch framing will also be discussed.", "video": ["Download Excel Files (Important!)", "Excel Files", "Recommended Reading", "4.1 Bunting and Expected Runs", "Excel Files", "Recommended reading", "4.2 Base Stealing", "Homework Problem 9", "Excel Files", "Recommended Reading", "4.3 Range Factor", "Excel Files", "4.4 UZR (Ultimate Zone Rating) and Fielding", "Homework Problem 10", "Homework Problem 11", "4.5 Introduction to Fan Graphs", "Excel Files", "Recommended Reading", "4.6 DIPS (Defense Independent Pitching Statistics)", "Homework Problem 12", "Excel Files", "Recommended Reading", "4.7 Win Probability Added", "Excel Files", "Recommended Reading", "4.8 Runs above Average", "Excel Files", "Recommended Reading", "4.9 WAR (Wins above Replacement)", "Homework Problem 13", "Excel Files", "4.10 What Makes your Team Win?", "Excel Files", "4.11 Defensive Shifts", "Excel Files", "4.12 Catcher Framing", "Excel Files", "Recommended Reading", "4.13 Park Factors", "Homework Problem 14", "Test Question 9", "Test Questions 10&11", "Test Question 12", "Test Question 13", "Test Question 14"], "title": "Module 4"}, {"description": "You will learn basic concepts involving random variables (specifically the normal random variable, expected value, variance and standard deviation.) You will learn how regression can be used to analyze what makes NFL teams win and decode the NFL QB rating system. You will also learn that momentum and the “hot hand” is mostly a myth. Finally, you will use Excel text functions and the concept of Expected Points per play to analyze the effectiveness of a football team’s play calling.", "video": ["Download Excel Files (Important!)", "Excel Files", "Recommended Reading", "5.1 Random Variables", "Excel Files", "Recommended Reading", "5.2 The Normal Random Variable", "Excel Files", "Recommended Reading", "5.3 Streakiness", "Excel Files", "5.4 What makes NFL teams win?", "Homework Problem 15", "Excel Files", "5.5 Deconstructing the NFL QB rating", "Homework Problem 16", "Excel Files", "Recommended Reading", "5.6 Football States and Values", "Excel Files", "Recommended Reading", "5.7 Football Decision Making", "Homework Problem 17", "Excel Files", "5.8 The Belichick Decision", "Homework Problem 18", "Excel Files", "Recommended Reading", "5.9 Text functions and Analyzing NFL plays", "Excel Files", "5.10 Texans 1st and 10 Play Selection", "Excel Files", "5.11 STDEVIF and Too Many Field Goal Attempts", "Homework Problem 19", "Test Question 15", "Test Question 16", "Test Question 17", "Test Question 18", "Test Question 19"], "title": "Module 5"}, {"description": "You will learn how two-person zero sum game theory sheds light on football play selection and soccer penalty kick strategies. Our discussion of basketball begins with an analysis of NBA shooting, box score based player metrics, and the Four Factor concept which explains what makes basketball teams win.", "video": ["Download Excel Files (Important!)", "Excel Files", "Recommended Reading", "6.1 Game Theory Part I", "Excel Files", "6.2 Game Theory Part II", "Homework Problem 20", "Excel Files", "6.3 Basketball Shot Selection", "Excel Files", "Recommended Reading", "6.4 Effective Field Goal Percentage (EFG)", "Homework Problem 21", "Excel Files", "6.5 Hollinger Team Metrics", "Excel Files", "Recommended Reading", "6.6 Four Factors", "Homework Problem 22", "Excel Files", "Recommended Reading", "6.7 Basketball Box Score Models", "Homework Problem 23", "Test Question 20", "Test Question 21", "Test Question 22", "Test Question 23"], "title": "Module 6"}, {"description": "You will learn about advanced basketball concepts such as Adjusted plus minus, ESPN’s RPM, SportVu data, and NBA in game decision-making.", "video": ["Download Excel Files (Important!)", "Excel Files", "7.1 Extracting Lineup data and Computing raw + -", "Excel Files", "7.2 Computing Pairs Lineup Statistics", "Excel Files", "7.3 Adjusted + -", "Homework Problem 24", "Excel Files", "7.4 ESPM RPM (Real Plus Minus)", "Homework Problem 25", "Excel Files", "7.5 How Analytics Helped the Mavs beat the Heat", "Excel Files", "7.6 SportsVu data", "Homework Problem 26", "Excel Files", "7.7 In Game Basketball Decision-Making", "Excel Files", "7.8 Hack a Jordan (or Dwight)", "Homework Problem 27", "Test 24", "Test 25", "Test 26", "Test 27"], "title": "Module 7"}, {"description": "You will learn how to use game results to rate sports teams and set point spreads. Simulation of the NCAA basketball tournament will aid you in filling out your 2016 bracket. Final 4 is in Houston!", "video": ["Download Excel Files (Important!)", "Excel Files", "Recommended Reading", "8.1 Rating NFL teams", "Homework Problem 28", "Excel Files", "Recommended Reading", "8.2 Computing Strength of Schedule", "Homework Problem 29", "Excel Files", "Recommended Reading", "8.3 Rating NFL Offenses and Defenses", "Homework Problem 30", "Excel Files", "Recommended Reading", "8.4 Multiplicative Rating Models for Soccer", "Homework Problem 31", "Excel Files", "8.5 Regularization", "Excel Files", "Recommended Reading", "8.6 Simulating the NCAA Tournament", "Excel Files", "8.7 NCAA Props Bets", "Homework Problem 32", "Test 28", "Test 29", "Test 30", "Test 31", "Test 32"], "title": "Module 8"}, {"description": "You will learn how to rate NASCAR drivers and get an introduction to sports betting concepts such as the Money line, Props Bets, and evaluation of gambling betting systems.", "video": ["Download Excel Files (Important!)", "Excel Files", "Recommended Reading", "9.1 Method of Maximum Likelihood", "Excel Files", "Recommended Reading", "9.2 Rating Teams Based on Wins and Losses", "Homework Problem 33", "Excel Files", "9.3 Simulating the NFL Playoffs", "Excel Files", "9.4 Rating NASCAR Drivers", "Excel Files", "9.5 How Good are NFL Point Spreads", "Excel Files", "9.6 Relating the Money Line to Point Spreads", "Recommended Reading", "Homework Problem 34", "Excel Files", "9.7 How Can you tell if a Betting System is Successful?", "Homework Problem 35", "Excel Files", "Recommended Reading", "9.8 Props Bets I", "Excel Files", "9.9 Props Bet II", "Excel Files", "9.10 Props Bet III", "Homework Problem 36", "Test 33", "Test 34", "Test 35", "Test 36"], "title": "Module 9"}, {"description": "You will learn how Kelly Growth can optimize your sports betting, how regression to the mean explains the SI cover jinx and how to optimize a daily fantasy sports lineup. We close with a discussion of golf analytics.", "video": ["Download Excel Files (Important!)", "Excel Files", "Recommended Reading", "10.1 Kelly Growth Part I", "Excel Files", "10.2 Kelly Growth Part II", "Excel Files", "10.3 Kelly Growth Part III", "Homework Problem 37", "Excel Files", "Recommended Reading", "10.4 Regression to the Mean", "Homework Problem 38", "Excel Files", "10.5 Daily Fantasy Sports Part I", "Excel Files", "10.6 Daily Fantasy Sports Part II", "Homework Problem 39", "Excel Files", "10.7 Golf Analytics", "Homework Problem 40", "Test 37", "Test 38", "Test 39", "Test 40"], "title": "Module 10"}, {"description": "Final exam has 10 questions.  Please download and open Excel files before taking the exam.  You will be referred to Excel files during the exam.  Each question is wort 1 point.  You need to answer 6 questions or more correctly to pass the exam.", "video": ["Download Excel Files", "Final Exam"], "title": "Final Exam"}], "title": "Math behind Moneyball"}, {"course_info": "About this course: What are the ethical considerations regarding the privacy and control of consumer information and big data, especially in the aftermath of recent large-scale data breaches?\n\nThis course provides a framework to analyze these concerns as you examine the ethical and privacy implications of collecting and managing big data. Explore the broader impact of the data science field on modern society and the principles of fairness, accountability and transparency as you gain a deeper understanding of the importance of a shared set of ethical values. You will examine the need for voluntary disclosure when leveraging metadata to inform basic algorithms and/or complex artificial intelligence systems while also learning best practices for responsible data management, understanding the significance of the Fair Information Practices Principles Act and the laws concerning the \"right to be forgotten.\"\n\nThis course will help you answer questions such as who owns data, how do we value privacy, how to receive informed consent and what it means to be fair.\n\nData scientists and anyone beginning to use or expand their use of data will benefit from this course. No particular previous knowledge needed.", "level": "Beginner", "package_name": null, "created_by": "University of Michigan", "package_num": null, "teach_by": [{"name": "H.V. Jagadish", "department": "Electrical Engineering and Computer Science"}], "target_audience": null, "rating": null, "week_data": [{"description": "Module 1 of this course establishes a basic foundation in the notion of simple utilitarian ethics we use for this course. The lecture material and the quiz questions are designed to get most people to come to an agreement about right and wrong, using the utilitarian framework taught here. If you bring your own moral sense to bear, or think hard about possible counter-arguments, it is likely that you can arrive at a different conclusion. But that discussion is not what this course is about. So resist that temptation, so that we can jointly lay a common foundation for the rest of this course.", "video": ["Course Syllabus", "Welcome Announcement", "Help us learn more about you!", "What are Ethics? - Introduction", "Data Science Ethics - Course Preview", "What are Ethics?", "Data Science Needs Ethics", "Case Study: Spam (not the meat)", "Module 1 Discussion", "Module 1 Quiz"], "title": "What are Ethics?"}, {"description": "Early experiments on human subjects were by scientists intent on advancing medicine, to the benefit of all humanity, disregard for welfare of individual human subjects. Often these were performed by white scientists, on black subject. In this module we will talk about the laws that govern the Principle of Informed Consent. We will also discuss why informed consent doesn’t work well for retrospective studies, or for the customers of electronic businesses.", "video": ["Human Subjects Research and Informed Consent: Part 1", "Human Subjects Research and Informed Consent: Part 2", "Limitations of Informed Consent", "Case Study: It's Not OKCupid", "Module 2 Discussion", "Module 2 Quiz"], "title": "History, Concept of Informed Consent"}, {"description": "Who owns data about you? We'll explore that question in this module. A few examples of personal data include copyrights for biographies; ownership of photos posted online, Yelp, Trip Advisor, public data capture, and data sale. We'll also explore the limits on recording and use of data. ", "video": ["Data Ownership", "Limits on Recording and Use", "Data Ownership Finale", "Case Study: Rate My Professor", "Module 3 Discussion", "Module 3 Quiz"], "title": "Data Ownership"}, {"description": "Privacy is a basic human need. Privacy means the ability to control information about yourself, not necessarily the ability to hide things. We have seen the rise different value systems with regards to privacy. Kids today are more likely to share personal information on social media, for example. So while values are changing, this doesn’t remove the fundamental need to be able to control personal information. In this module we'll examine the relationship between the services we are provided and the data we provide in exchange: for example, the location for a cell phone. We'll also compare and contrast \"data\" against \"metadata\".", "video": ["Privacy - Introduction", "Privacy", "History of Privacy", "Degrees of Privacy", "Modern Privacy Risks", "Case Study: Targeted Ads", "Case Study: The Naked Mile", "Case Study: Sneaky Mobile Apps", "Module 4 Discussion", "Module 4 Discussion Prompt References", "Module 4 Quiz"], "title": "Privacy"}, {"description": "Certain transactions can be performed anonymously. But many cannot, including where there is physical delivery of product. Two examples related to anonymous transactions we'll look at are \"block chains\" and \"bitcoin\". We'll also look at some of the drawbacks that come with anonymity.", "video": ["Anonymity", "De-identification Has Limited Value: Part 1", "De-identification Has Limited Value: Part 2", "Case Study: Credit Card Statements", "Module 5 Discussion", "Module 5 Quiz"], "title": "Anonymity"}, {"description": "Data validity is not a new concern. All too often, we see the inappropriate use of Data Science methods leading to erroneous conclusions. This module points out common errors, in language suited for a student with limited exposure to statistics. We'll focus on the notion of representative sample: opinionated customers, for example, are not necessarily representative of all customers.", "video": ["Data Validity - Introduction", "Validity", "Choice of Attributes and Measures", "Errors in Data Processing", "Errors in Model Design", "Managing Change", "Case Study: Three Blind Mice", "Case Study: Algorithms and Race", "Case Study: Algorithms in the Office", "Case Study: GermanWings Crash", "Case Study: Google Flu", "Module 6 Discussion", "Module 6 Quiz"], "title": "Data Validity"}, {"description": "What could be fairer than a data-driven analysis? Surely the dumb computer cannot harbor prejudice or stereotypes. While indeed the analysis technique may be completely neutral, given the assumptions, the model, the training data, and so forth, all of these boundary conditions are set by humans, who may reflect their biases in the analysis result, possibly without even intending to do so. Only recently have people begun to think about how algorithmic decisions can be unfair. Consider this article, published in the New York Times. This module discusses this cutting edge issue.", "video": ["Algorithmic Fairness - Introduction", "Algorithmic Fairness", "Correct But Misleading Results", "P Hacking", "Case Study: High Throughput Biology", "Case Study: Geopricing", "Case Study: Your Safety Is My Lost Income", "Module 7 Discussion", "Module 7 Quiz"], "title": "Algorithmic Fairness"}, {"description": "In Module 8, we consider societal consequences of Data Science that we should be concerned about even if there are no issues with fairness, validity, anonymity, privacy, ownership or human subjects research. These “systemic” concerns are often the hardest to address, yet just as important as other issues discussed before. For example, we consider ossification, or the tendency of algorithmic methods to learn and codify the current state of the world and thereby make it harder to change. Information asymmetry has long been exploited for the advantage of some, to the disadvantage of others. Information technology makes spread of information easier, and hence generally decreases asymmetry. However, Big Data sets and sophisticated analyses increase asymmetry in favor of those with ability to acquire/access. ", "video": ["Societal Consequences - Introduction", "Societal Impact", "Ossification", "Surveillance", "Case Study: Social Credit Scores", "Case Study: Predictive Policing", "Module 8 Discussion", "Module 8 Quiz"], "title": "Societal Consequences"}, {"description": "Finally, in Module 9, we tie all the issues we have considered together into a simple, two-point code of ethics for the practitioner.", "video": ["Code of Ethics", "Wrap Up", "Case Study: Algorithms and Facial Recognition", "Post-Course Survey", "Module 9 Quiz", "Data Ethics Case Study"], "title": "Code of Ethics"}, {"description": "This module contains lists of attributions for the external audio-visual resources used throughout the course.", "video": ["Week 1 Attributions", "Week 2 Attributions", "Week 3 Attributions", "Week 4 Attributions"], "title": "Attributions"}], "title": "Data Science Ethics"}, {"course_info": "About this course: Learn to use tools from the Bioconductor project to perform analysis of genomic data. This is the fifth course in the Genomic Big Data Specialization from Johns Hopkins University.", "level": null, "package_name": "Genomic Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "6", "teach_by": [{"name": "Kasper Daniel Hansen, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "3.9", "week_data": [{"description": "The class will cover how to install and use Bioconductor software. We will discuss common data structures, including ExpressionSets, SummarizedExperiment and GRanges used across several types of analyses.", "video": ["Welcome", "Syllabus", "Pre Course Survey", "Installing R on Windows ", "Installing R on A Mac ", "Installing R Studio on a Mac ", "What is Bioconductor", "Installing Bioconductor", "The Bioconductor Website", "Useful Online Resources", "R Base Types", "GRanges - Overview", "IRanges - Basic Usage", "GenomicRanges - GRanges", "GenomicRanges - Basic GRanges Usage", "GenomicRanges - seqinfo", "AnnotationHub", "Usecase: AnnotationHub and GRanges, part 1", "Usecase: AnnotationHub and GRanges, part 2", "Quiz 1"], "title": "Week One"}, {"description": "In this week we will learn how to represent and compute on biological sequences, both at the whole-genome level and at the level of millions of short reads.", "video": ["Biostrings", "BSgenome", "Biostrings - Matching", "BSgenome - Views", "GenomicRanges - Rle", "GenomicRanges - Lists", "GenomicFeatures", "rtracklayer - Data Import", "Quiz 2"], "title": "Week Two"}, {"description": " In this week we will cover Basic Data Types, ExpressionSet, biomaRt, and R S4.", "video": ["Basic Data Types", "Annotation Overview", "ExpressionSet Overview", "ExpressionSet", "SummarizedExperiment", "GEOquery", "biomaRt", "R S4 Classes", "R S4 Methods", "Quiz 3"], "title": "Week Three"}, {"description": "In this week, we will cover Getting data in Bioconductor, Rsamtools, oligo, limma, and minfi", "video": ["Getting data into Bioconductor", "Short Read", "Rsamtools", "oligo", "limma", "minfi", "Count-based RNA-seq analysis", "Post Course Survey", "Quiz 4"], "title": "Week Four"}], "title": "Bioconductor for Genomic Data Science"}, {"course_info": "About this course: The purpose of this course is to summarize new directions in Chinese history and social science produced by the creation and analysis of big historical datasets based on newly opened Chinese archival holdings, and to organize this knowledge in a framework that encourages learning about China in comparative perspective.\n\nOur course demonstrates how a new scholarship of discovery is redefining what is singular about modern China and modern Chinese history. Current understandings of human history and social theory are based largely on Western experience or on non-Western experience seen through a Western lens. This course offers alternative perspectives derived from Chinese experience over the last three centuries. We present specific case studies of this new scholarship of discovery divided into two stand-alone parts, which means that students can take any part without prior or subsequent attendance of the other part.\n\nPart 1 (this course) focuses on comparative inequality and opportunity and addresses two related questions ‘Who rises to the top?’ and ‘Who gets what?’.\n\nPart 2 (https://www.coursera.org/learn/understanding-china-history-part-2) turns to an arguably even more important question ‘Who are we?’ as seen through the framework of comparative population behavior - mortality, marriage, and reproduction – and their interaction with economic conditions and human values. We do so because mortality and reproduction are fundamental and universal, because they differ historically just as radically between China and the West as patterns of inequality and opportunity, and because these differences demonstrate the mutability of human behavior and values.\n\nCourse Overview video: https://youtu.be/dzUPRyJ4ETk", "level": null, "package_name": null, "created_by": "The Hong Kong University of Science and Technology", "package_num": null, "teach_by": [{"name": "James Z. Lee", "department": null}], "target_audience": "Who is this class for: Anyone interested in understanding China through its empirical data may join.", "rating": "4.5", "week_data": [{"description": "Before you start with the content for Module 1, please watch the Course Overview, review the Assignments and Grading page, and introduce yourself to other learners who will be studying this course with you.", "video": ["Course Overview", "Assignments and Grading", "Meet and Greet", "1.1: Introduction", "1.2: Who Gets What and Why?", "1.3: Social Mobility and the Examination System in Late Imperial China", "1.4: Cultural Reproduction and Education in Late Imperial and Contemporary China", "Module 1 Suggested Reading", "Quiz 1"], "title": "Orientation and Module 1: Social Structure and Education in Late Imperial China"}, {"description": "", "video": ["2.1: Comparing Inequality in Education and Income between China and the West", "2.2: Student Diversity at Peking University 1950-1999 and Suzhou University 1950-2003", "2.3: China’s Silent Revolution’s Ladder of Success", "Module 2 Suggested Reading", "Quiz 2"], "title": "Module 2: Education and Social Mobility in Contemporary China"}, {"description": "", "video": ["3.1: Wealth Distribution in the UK and US, 1700-2000", "3.2: Population Categories and Wealth Entitlements in China", "3.3: Land Distribution in Shuangcheng, 1870-1906", "3.4: Property Distribution in Contemporary China", "3.5: Comparative Wealth Distribution: Past/Present and East/West", "Module 3 Suggested Reading", "Quiz 3"], "title": "Module 3: Social Mobility and Wealth Distribution in Late Imperial and Contemporary China"}, {"description": "", "video": ["4.1: Wealth Distribution and Regime Change", "4.2: Wealth Distribution in Pre-Revolutionary China", "4.3: Political Processes and Institutions of Regime Change in Shuangcheng, 1946-1948", "4.4: Revolutionary Victims in Shuangcheng and Elsewhere", "4.5: Course Conclusion", "Module 4 Suggested Reading", "Quiz 4"], "title": "Module 4: Wealth Distribution and Regime Change in Twentieth Century China"}, {"description": "Now is time to test your understanding on the entire course. Take the final exam and complete the post-course survey. Your valuable feedback will certainly help us improve future iterations of the course.", "video": ["A Farewell Message from Professor James Lee", "Post-course Survey", "Final Exam"], "title": "Final Exam and Farewell"}], "title": "Understanding China, 1700-2000: A Data Analytic Approach, Part 1"}, {"course_info": "About this course: This course covers the analysis of Functional Magnetic Resonance Imaging (fMRI) data. It is a continuation of the course “Principles of fMRI, Part 1”", "level": null, "package_name": null, "created_by": "Johns Hopkins University, University of Colorado Boulder", "package_num": null, "teach_by": [{"name": "Martin Lindquist, PhD, MSc", "department": "Bloomberg School of Public Health | Johns Hopkins University"}, {"name": "Tor Wager", "department": "Department of Psychology and Neuroscience, The Institute of Cognitive Science | University of Colorado at Boulder"}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "This week we will discuss psychological and behavioral inference, as well as advanced experimental design. ", "video": ["Syllabus", "Module 1A: Crises in neuroscience and psychology: Problems and solutions", "Module 1B: Crises in neuroscience and psychology: Problems and solutions", "Module 2A: Pitfalls and Biases in Interpretation: Circularity, Reverse inference, Voodoo, and Beyond", "Module 2B: Pitfalls and Biases in Interpretation: Circularity, Reverse inference, Voodoo, and Beyond", "Module 3: Meta Analysis", "Module 4: Using meta-analysis to improve inference", "Module 5: Optimizing acquisition for your study goals", "Module 6: Resting-state fMRI", "Quiz 1"], "title": "Week 1"}, {"description": "This week we will continue with advanced experimental design, and also discuss advanced GLM modeling.", "video": ["Module 7A: Advanced Experimental Design I - Fundamentals of design efficiency", "Module 7B: Advanced Experimental Design I - Fundamentals of design efficiency", "Module 8: Advanced Experimental Design II – Efficiency in fMRI", "Module 9: Advanced Experimental Design III – Optimizing experimental designs with genetic algorithms", "Module 10: Model building: Parametric modulators", "Module 11: Basis Sets II: Smooth FIR, HTW, Basis sets at 2nd level", "Module 12A: Multicolinearity and diagnostics", "Module 12B: Multicolinearity and diagnostics", "Module 13: Robust regression", "Module 14A: Practical group analysis: Procedures and checks", "Module 14B: Practical group analysis: Procedures and checks", "Quiz 2"], "title": "Week 2"}, {"description": "This week we will focus on brain connectivity.", "video": ["Module 15: Brain Connectivity – different types", "Module 16: Functional Connectivity", "Module 17: Multivariate Decomposition Methods", "Module 18: Dynamic Connectivity", "Module 19: Network Analysis I – Graph theory", "Module 20: Effective Connectivity", "Module 21: Mediation, moderation, and structural models", "Module 21B: Mediation, moderation, and structural models", "Quiz 3"], "title": "Week 3"}, {"description": "This week we will focus on multi-voxel pattern analysis.", "video": ["Module 22: Directed graphs: DCM", "Module 23: Directed graphs: Granger causality", "Module 24: Principles of causal inference", "Module 25: Multi-voxel Pattern Analysis: A neuroscientific perspective", "Module 25B: Multi-voxel Pattern Analysis: A neuroscientific perspective", "Module 26: MVPA Intro", "Module 27: Performing MVPA I", "Module 28: Performing MVPA II", "Module 29: MVPA-PM Example – NPS - Expand: Social and physical pain", "Module 29B: MVPA-PM Example – NPS - Expand: Social and physical pain", "Quiz 4"], "title": "Week 4"}], "title": "Principles of fMRI 2"}, {"course_info": "About this course: The analytical process does not end with models than can predict with accuracy or prescribe the best solution to business problems. Developing these models and gaining insights from data do not necessarily lead to successful implementations. This depends on the ability to communicate results to those who make decisions. Presenting findings to decision makers who are not familiar with the language of analytics presents a challenge. In this course you will learn how to communicate analytics results to  stakeholders who do not understand the details of analytics but want evidence of analysis and data. You will be able to choose the right vehicles to present quantitative information, including those based on principles of data visualization. You will also learn how to develop and deliver data-analytics stories that provide context, insight, and interpretation.", "level": null, "package_name": "Advanced Business Analytics Specialization ", "created_by": "University of Colorado Boulder", "package_num": "4", "teach_by": [{"name": "Manuel Laguna", "department": "Leeds School of Business"}, {"name": "Dan Zhang", "department": "Leeds School of Business"}, {"name": "David Torgerson", "department": null}], "target_audience": null, "rating": "4.1", "week_data": [{"description": "In this module we’ll briefly review the Information-Action Value Chain we introduced in Course 1.  Then we’ll see how analytical techniques are applied in business problems, first by looking at some “classic” business problems that have been around for a long time, then by looking at some “emergent” business problems that have resulted from more recent advances in technology.", "video": ["0. Introduction to the Course", "1. Information Action Value Chain Redux", "2. Analytics in Classic Business Problems", "3. Analytics in Emergent Business Problems", "Types of Analytical Problems", "Is This a New Type of Analysis?", "Week 1 Quiz"], "title": "Introduction to the Course"}, {"description": "In this module we’ll learn about a variety of visualizations used to illustrate and communicate data. We will start with the different vehicles used  to present quantitative information. We will then look at a set of examples of data visualizations and discuss what makes them effective or ineffective.  Finally, we discuss Excel charts and why most of them should be avoided.  After completing this module, you will be able to better understand the characteristics of good data visualization and avoid common mistakes when creating your own graphs.\n", "video": ["Module Introduction", "Vehicles to Present Quantitative Information", "Data Visualization Examples", "Graphs in Excel and ASP", "Graphical Excellence", "Techniques to Display Multiple Variables", "Excel Charts to Avoid", "Share an Example of Effective or Ineffective Visualization", "Annual Energy Review Graph", "Week 2 Quiz"], "title": "Best  Practices in Data Visualization"}, {"description": "In this module we’ll cover a number of topics around interpreting data, gathering additional data, and pitching our recommendations based on our analysis.  First, we’ll discuss ways in which we misinterpret or misrepresent data and how to avoid them, such as mistaking correlation with causation, allowing cognitive biases to influence how we see data, and visualizing data in misleading ways.  We’ll also learn how experimentation can help us obtain more data, including compromises we may need to make in measurement.  Finally, we’ll discuss how we communicate our results and recommendations, with a focus on knowing our audience, telling compelling stories, and creating clear and effective communication materials.\n", "video": ["Correlation vs Causation", "Common Cognitive Biases", "Misleading With Data", "Market Experiments: When the action is the question", "Know thy Audience", "Telling compelling stories", "Making It Real", "What Examples of Correlation vs. Causation Have You Recently Encountered?", "CNN Article about Four-Day Work Week. Is This a Good Data-Based Argument? ", "Week 3 Quiz"], "title": "Interpreting, Telling, and Selling"}, {"description": "In our final module we’ll walk through two case studies and illustrate the ideas we’ve covered in the course and in the specialization as a whole.  The first case shows how experimentation can be used to create data, sometimes with surprising results.  The second case presents a comprehensive analysis that illustrates the entire analytic lifecycle, and shows how different methods and both quantitative and qualitative analysis can be brought together to solve one strategically important analytical problem.", "video": ["Case Study 1: Experiment Driven Analytics and Customer Churn", "Case Study 2: Multidimensional Analysis for Customer Acquisition - Part 1", "Case Study 2: Multidimensional Analysis for Customer Acquisition - Part 2", "Case Study 2: Multidimensional Analysis for Customer Acquisition - Part 3", "Communicating Control Group Results", "The \"Science and Art\" of Analytics", "Week 4 Quiz", "Final Course Assignment"], "title": "Acting on Data"}], "title": "Communicating Business Analytics Results"}, {"course_info": "About this course: The explosion in digital media - web, social and now mobile - represents a departure from how things were like in the last century. This proliferation of digital media is both a threat and an opportunity for many businesses. Business Analytics can be leveraged to process data, sentiment, buzz, contacts, context and other aspects of business interest in real time, for business performance and impact. The course picks and uses use-cases from a variety of industries and geographies, to showcase the potential and impact that business analytics done properly (or not) can have on business performance.", "level": "Beginner", "package_name": "Business Technology Management Specialization ", "created_by": "Indian School of Business", "package_num": "2", "teach_by": [{"name": "Sudhir Voleti", "department": "Marketing "}], "target_audience": null, "rating": "4.0", "week_data": [{"description": "An overview of the what and the why\nLearning outcomes by the end of week 1 should normatively be that (a) Students grasp what is business analytics from the perspective of a business manager, (b) identify areas of interest, overlap, co-ordination and conflict with other business functions and processes in the firm, (c) and, develop an appreciation for the value of data, of analyses and of the components of analytics.\n", "video": ["Introductory Business Analytics and Digital Media", "Learner Introduction", "Conceptual Preliminaries", "Framework for Problem Formulation", "Transformative Problem Formulation", "Share examples of D.Ps and R.O.s", "Data Types and Psychometric Scaling", "Introduction to Business Analytics and Psychometrics", "Exploratory Vs Confirmatory Dichotomy", "Revisiting Problem Formulation", "In Video Reading for Lecture \"Revisiting Problem Formulation\"", "Share examples of Predictive analytics in action", "Summative Assessment"], "title": "Introduction to Business Analytics"}, {"description": "An overview of the broad tools available for business analytics and the leveraging of digital media. \nLearning outcomes by the end of week 2 should normatively be that (a) Students have an understanding of the broad classes of analytics tools and platforms that currently dominate the market, (b) and, develop an appreciation for the pros and cons of the major groups of tools. \n", "video": ["Toolscape for Business Analytics", "A primer on Business Experimentation", "Share examples of Business experimentation", "Conceptual Preliminaries", "Software for this Course", "Instructions for Running the Analytics Desktop Apps using RStudio and GitHub", "Training Machines A People-Analytics Use Case", "The Exponential Learning Curve of the Machines", "Share some machine learning examples", "Toolscape Summary and Warp-up", "What principle drives Evolv’s approach? ", "Additional links and reading material", "Summative Assessment"], "title": "Toolscape"}, {"description": "Introduction to and the application of some important analytical processes in Marketing Analytics\nLearning outcomes by the end of week 3 should normatively be that  (a) Students have an understanding of the major processes and procedures typically used in a customer analytics setting, in particular factor and cluster analyses (b) and, develop an appreciation for the possibilities that emerge from recombining procedures, data, algorithms and problem formulation perspectives in open source environments.\n", "video": ["Motivating Customer Analytics", "preliminaries for Customer Analytics", "Factorizing Data", "Share examples of Factor-An application", "Clustering Data", "Share examples of Cluster-An or Joint Space Mapping", "Visualising Data", "Data sets and slides as reference material for Perceptual mapping", "How to Run Analytics Apps in R", "Analytics Apps - Examples & Intro", "Summative Assessement"], "title": "Customer Analytics "}, {"description": "An overview of the big questions, possibilities and challenges.\nLearning outcomes by the end of week 4 should normatively be that \n(a) Students have an understanding of the major types of digital media in use currently by people and firms,  (b) and, develop an appreciation for the types of problem solving, data collection, prediction and optimization that can be enabled using digital media tools. \n", "video": ["Motivating Technology for Brands", "Setting the Context: Digital Advertising", "The App Era in Digital Social Media", "Social Graphs and Hyper-Segmentation", " What is a 'social graph'?", "Non-Social Digital Media for the Enterprise", "Formulating and Analytics Problem", "What is the main take-away from the GE reading? Why do you think so?", "Additional Reading for Module \"Digital Media\"", "GE caselet: In-video Reading", "Summative Assessment"], "title": "Digital Media "}], "title": "Business Analytics and Digital Media"}, {"course_info": "About this course: This course will cover the steps used in weighting sample surveys, including methods for adjusting for nonresponse and using data external to the survey for calibration.  Among the techniques discussed are adjustments using estimated response propensities, poststratification, raking, and general regression estimation.  Alternative techniques for imputing values for missing items will be discussed.  For both weighting and imputation, the capabilities of different statistical software packages will be covered, including R®, Stata®, and SAS®.", "level": null, "package_name": "Survey Data Collection and Analytics  Specialization ", "created_by": "University of Maryland, College Park", "package_num": "5", "teach_by": [{"name": "Richard Valliant, Ph.D.", "department": "Joint Program in Survey Methodology"}], "target_audience": "Who is this class for: This course is aimed at undergraduates, graduate students, and working professionals who have an interest and need in preparing survey data for analysis and distribution to data users.", "rating": "3.8", "week_data": [{"description": "Weights are used to expand a sample to a population.  To accomplish this, the weights may correct for coverage errors in the sampling frame, adjust for nonresponse, and reduce variances of estimators by incorporating covariates. The series of steps needed to do this are covered in Module 1.", "video": ["Introduction", "Class notes + additional reading", "Quantities to Estimate", "Class notes", "Goals of Estimation", "Class Notes", "Statistical Interpretation of Estimates", "Class Notes", "Coverage Problems", "Class Notes", "Improving Precision", "Class Notes", "Effects of Weighting on SEs", "Class Notes", "Introductory quiz on weights", "Quantities", "Goals", "Interpretation", "Coverage", "Improving precision", "Effects on SEs"], "title": "General Steps in Weighting"}, {"description": "Specific steps in weighting include computing base weights, adjusting if there are cases whose eligibility we are unsure of, adjusting for nonresponse, and using covariates to calibrate the sample to external population controls.  We flesh out the general steps with specific details here.", "video": ["Overview", "Class Notes", "Base Weights", "Class Notes", "Nonresponse Adjustments", "Class Notes", "Response Propensities", "Class Notes", "Tree algorithms", "Class Notes", "Calibration", "Class Notes", "Overview", "Base weights", "Nonresponse", "Trees", "Calibration"], "title": "Specific Steps"}, {"description": "Software is critical to implementing the steps, but the R system is an excellent source of free routines. This module covers several R packages, including sampling, survey, and PracTools that will select samples and compute weights.", "video": ["Software", "Class Notes", "Base Weights", "Class Notes + Software", "More on Base Weights", "Class Notes", "Quiz on base weights", "Nonresponse Adjustments", "Class Notes + Software for propensity classes", "Quiz on nonresponse adjustments", "Examples of Calibration", "Software for Poststratification", "Class Notes + Software for calibration", "Quiz on calibration and poststratification", "Software"], "title": "Implementing the Steps"}, {"description": "In most surveys there will be items for which respondents do not provide information, even though the respondent completed enough of the data collection instrument to be considered \"complete\".  If only the cases with all items present are retained when fitting a model, quite a few cases may be excluded from the analysis. Imputing for the missing items avoids dropping the missing cases.  We cover methods of doing the imputing and of reflecting the effects of imputations on standard errors in this module.", "video": ["Reasons for Imputation", "Class Notes", "Means and hotdeck", "Class Notes", "Regression Imputation", "Class Notes", "Effect on Variances", "Class Notes", "mice R package", "mice example", "Class Notes + mice R package", "Reasons for imputing", "Means and hot deck", "Regression imputation", "Effects on variances", "Imputation software"], "title": "Imputing for Missing Items"}, {"description": "We briefly summarize the methods of weighting and imputation that were covered in Course 5.", "video": ["Summary", "Class Notes"], "title": "Summary of Course 5"}], "title": "Dealing With Missing Data"}, {"course_info": "About this course: This course is an introduction into formal concept analysis (FCA), a mathematical theory oriented at applications in knowledge representation, knowledge acquisition, data analysis and visualization. It provides tools for understanding the data by representing it as a hierarchy of concepts or, more exactly, a concept lattice. FCA can help in processing a wide class of data types providing a framework in which various data analysis and knowledge acquisition techniques can be formulated. In this course, we focus on some of these techniques, as well as cover the theoretical foundations and algorithmic issues of FCA.\nUpon completion of the course, the students will be able to use the mathematical techniques and computational tools of formal concept analysis in their own research projects involving data processing. Among other things, the students will learn about FCA-based approaches to clustering and dependency mining.\nThe course is self-contained, although basic knowledge of elementary set theory, propositional logic, and probability theory would help.\nEnd-of-the-week quizzes include easy questions aimed at checking basic understanding of the topic, as well as more advanced problems that may require some effort to be solved.", "level": "Intermediate", "package_name": null, "created_by": "National Research University Higher School of Economics", "package_num": null, "teach_by": [{"name": "Sergei Obiedkov ", "department": "Faculty of computer science"}], "target_audience": "Who is this class for: This course will be interesting for:\n• Bachelor students (3rd or 4th year)\n• Master students \n• Researchers and data analysts who want to get acquainted with formal concept analysis and its potential applications   ", "rating": "4.7", "week_data": [{"description": "This week we will learn the basic notions of formal concept analysis (FCA). We'll talk about some of its typical applications, such as conceptual clustering and search for implicational dependencies in data. We'll see a few examples of concept lattices and learn how to interpret them. The simplest data structure in formal concept analysis is the formal context. It is used to describe objects in terms of attributes they have. Derivation operators in a formal context link together object and attribute subsets; they are used to define formal concepts. They also give rise to closure operators, and we'll talk about what these are, too. We'll have a look at software called Concept Explorer, which is good for basic processing of formal contexts. We'll also talk a little bit about many-valued contexts, where attributes may have many values. Conceptual scaling is used to transform many-valued contexts into \"standard\", one-valued, formal contexts.", "video": ["Welcome to Formal Concept Analysis", "What is formal concept analysis?", "Understanding the concept lattice diagram", "Reading concepts from the lattice diagram", "Reading implications from the lattice diagram", "Further reading", "Conceptual clustering", "Formal contexts and derivation operators", "Formal concepts", "Closure operators", "Closure systems", "Software: Concept Explorer", "Many-valued contexts", "Conceptual scaling schemas", "Scaling ordinal data", "Reading concept lattice diagrams", "Formal concepts and closure operators"], "title": "Formal concept analysis in a nutshell"}, {"description": "This week we'll talk about some mathematical properties of concepts. We'll define a partial order on formal concepts, that of \"being less general\". Ordered in this way, the concepts of a formal concept constitute a special mathematical structure, a complete lattice. We'll learn what these are, and we'll see, through the basic theorem on concept lattices, that any complete lattice can, in a certain sense, be modelled by a formal context. We'll also discuss how a formal context can be simplified without loosing the structure of its concept lattice.", "video": ["The partial order on concepts", "Supremum and infimum", "Lattices", "The basic theorem (I)", "The basic theorem (II)", "Line diagrams", "Context clarification and reduction", "Context reduction: an example", "Supremum and infimum", "Lattices and complete lattices", "Clarification and reduction"], "title": "Concept lattices and their line diagrams"}, {"description": "We will consider a few algorithms that build the concept lattice of a formal context: a couple of naive approaches, which are easy to use if one wants to build the concept lattice of a small context; a more sophisticated approach, which enumerates concepts in a specific order; and an incremental strategy, which can be used to update the concept lattice when a new object is added to the context. We will also give a formal definition of implications, and we'll see how an implication can logically follow from a set of other implications.", "video": ["Finding the concepts", "Drawing a concept lattice diagram", "A naive algorithm for enumerating closed sets", "Representing sets by bit vectors", "Closures in lectic order", "Next Closure through an example", "The complexity of the algorithm", "Basic incremental strategy", "An example", "The definition of implications", "Examples of attribute implications", "Implication inference", "Computing the closure under implications", "Transposed context", "Closures in lectic order", "Implications"], "title": "Constructing concept lattices"}, {"description": "This week we'll continue talking about implications. We'll see that implication sets can be redundant, and we'll learn to summarise all valid implications of a formal context by its canonical (Duquenne–Guigues) basis. We'll study one concrete algorithm that computes the canonical basis, which turns out to be a modification of the Next Closure algorithm from the previous week. We'll also talk about what is known in database theory as functional dependencies, and we'll show how they are related to implications.", "video": ["Redundancy in implications", "Pseudo-closed sets and canonical basis", "Preclosed sets", "Preclosure operator", "Computing the canonical basis", "An example", "Complexity issues", "Functional dependencies", "Translation between functional dependencies and implications", "Implications and pseudo-intents", "Canonical basis", "Functional dependencies"], "title": "Implications"}, {"description": "What if we don't have a direct access to a formal context, but still want to compute its concept lattice and its implicational theory? This can be done if there is a domain expert (or an oracle) willing to answer our queries about the domain. We'll study an approach known as learning with queries that addresses this setting. We'll get to know a few standard types of queries, and we'll see how an implication set can be learnt in time polynomial of its size with so called membership and equivalence queries. We'll then introduce attribute exploration, a method from formal concept analysis, which may require exponential time, but which uses different queries, more suitable for building implicational theories and representative samples of subject domains.", "video": ["Basic introduction to learning with queries", "Learning binary patterns", "An easy case", "The general case", "Learning implications with queries", "Membership and equivalence queries for implications", "A polynomial-time algorithm", "Learning domain implications with queries", "Attribute exploration algorithm", "Attribute exploration of pairs of squares", "Object exploration", "Variations of attribute exploration", "Incompletely specified examples", "Completing incomplete contexts", "Learning with queries", "Learning implications with membership and equivalence queries", "Attribute exploration"], "title": "Interactive algorithms for learning implications"}, {"description": "A concept lattice can be exponentially large in the size of its formal context. Sometimes this can be due to noise in data. We'll study a few heuristics to filter out noisy concepts or select the most interesting concepts in a large lattice built from real data: stability and separation indices, concept probability, iceberg lattices. We will also talk about association rules, which is a name for implications that are supported by strong evidence, but may still have counterexamples in data. ", "video": ["Small changes in the context, big changes in the concept lattice", "Iceberg lattices", "Concept stability", "Separation index", "Concept probability", "Nested line diagrams", "Association rules", "Support and confidence", "Frequent closed sets", "Luxenburger basis", "Goodbye!", "Concept indices", "Association rules"], "title": "Working with real data"}], "title": "Introduction to Formal Concept Analysis"}, {"course_info": "About this course: Important note: The second assignment in this course covers the topic of Graph Analysis in the Cloud, in which you will use Elastic MapReduce and the Pig language to perform graph analysis over a moderately large dataset, about 600GB. In order to complete this assignment, you will need to make use of Amazon Web Services (AWS). Amazon has generously offered to provide up to $50 in free AWS credit to each learner in this course to allow you to complete the assignment. Further details regarding the process of receiving this credit are available in the welcome message for the course, as well as in the assignment itself. Please note that Amazon, University of Washington, and Coursera cannot reimburse you for any charges if you exhaust your credit.\n\nWhile we believe that this assignment contributes an excellent learning experience in this course, we understand that some learners may be unable or unwilling to use AWS. We are unable to issue Course Certificates for learners who do not complete the assignment that requires use of AWS. As such, you should not pay for a Course Certificate in Communicating Data Results if you are unable or unwilling to use AWS, as you will not be able to successfully complete the course without doing so.\n\nMaking predictions is not enough!  Effective data scientists know how to explain and interpret their results, and communicate findings accurately to stakeholders to inform business decisions.  Visualization is the field of research in computer science that studies effective communication of quantitative results by linking perception, cognition, and algorithms to exploit the enormous bandwidth of the human visual cortex.  In this course you will learn to recognize, design, and use effective visualizations.\n\nJust because you can make a prediction and convince others to act on it doesn’t mean you should.  In this course you will explore the ethical considerations around big data and how these considerations are beginning to influence policy and practice.   You will learn the foundational limitations of using technology to protect privacy and the codes of conduct emerging to guide the behavior of data scientists.  You will also learn the importance of reproducibility in data science and how the commercial cloud can help support reproducible research even for experiments involving massive datasets, complex computational infrastructures, or both.\n\nLearning Goals: After completing this course, you will be able to:\n1. Design and critique visualizations\n2. Explain the state-of-the-art in privacy, ethics, governance around big data and data science\n3. Use cloud computing to analyze large datasets in a reproducible way.", "level": null, "package_name": "Data Science at Scale Specialization ", "created_by": "University of Washington", "package_num": "3", "teach_by": [{"name": "Bill Howe", "department": "Scalable Data Analytics"}], "target_audience": null, "rating": "3.5", "week_data": [{"description": "Statistical inferences from large, heterogeneous, and noisy datasets are useless if you can't communicate them to your colleagues, your customers, your management and other stakeholders.  Learn the fundamental concepts behind information visualization, an increasingly critical field of research and increasingly important skillset for data scientists.  This module is taught by Cecilia Aragon, faculty in the Human Centered Design and Engineering Department.", "video": ["01 Introduction: What and Why", "02 Introduction: Motivating Examples", "03 Data Types: Definitions", "04 Mapping Data Types to Visual Attributes", "05 Data Types Exercise", "06 Data Types and Visual Mappings Exercises", "07 Data Dimensions", "08 Effective Visual Encoding", "09 Effective Visual Encoding Exercise", "10 Design Criteria for Visual Encoding", "11 The Eye is not a Camera", "12 Preattentive Processing", "13 Estimating Magnitude", "14 Evaluating Visualizations", "Crime Analytics: Visualization of Incident Reports"], "title": "Visualization"}, {"description": "Big Data has become closely linked to issues of privacy and ethics: As the limits on what we *can* do with data continue to evaporate, the question of what we *should* do with data becomes paramount.  Motivated in the context of case studies, you will learn the core principles of codes of conduct for data science and statistical analysis.  You will learn the limits of current theory on protecting privacy while still permitting useful statistical analysis. ", "video": ["Motivation: Barrow Alcohol Study", "Barrow Study Problems", "Reifying Ethics: Codes of Conduct", "ASA Code of Conduct: Responsibilities to Stakeholders", "Other Codes of Conduct", "Examples of Codified Rules: HIPAA", "Privacy Guarantees: First Attempts", "Examples of Privacy Leaks", "Formalizing the Privacy Problem", "Differential Privacy Defined", "Global Sensitivity", "Laplacian Noise", "Adding Laplacian Noise and Proving Differential Privacy", "Weaknesses of Differential Privacy"], "title": "Privacy and Ethics"}, {"description": "Science is facing a credibility crisis due to unreliable reproducibility, and as research becomes increasingly computational, the problem seems to be paradoxically getting worse.  But reproducibility is not just for academics: Data scientists who cannot share, explain, and defend their methods for others to build on are dangerous.  In this module, you will explore the importance of reproducible research and how cloud computing is offering new mechanisms for sharing code, data, environments, and even costs that are critical for practical reproducibility.", "video": ["Reproducibility and Data Science", "Reproducibility Gold Standard", "Anecdote: The Ocean Appliance", "Code + Data + Environment", "Cloud Computing Introduction", "Cloud Computing History", "Code + Data + Environment + Platform", "Cloud Computing for Reproducible Research", "Advantages of Virtualization for Reproducibility", "Complex Virtualization Scenarios", "Shared Laboratories", "Economies of Scale", "Provisioning for Peak Load", "Elasticity and Price Reductions", "Server Costs vs. Power Costs", "Reproducibility for Big Data", "Counter-Arguments and Summary", "AWS Credit Opt-in Consent Form", "Graph Analysis in the Cloud"], "title": "Reproducibility and Cloud Computing"}], "title": "Communicating Data Science Results"}, {"course_info": "About this course: This course aims to provide a succinct overview of the emerging discipline of Materials Informatics at the intersection of materials science, computational science, and information science. Attention is drawn to specific opportunities afforded by this new field in accelerating materials development and deployment efforts. A particular emphasis is placed on materials exhibiting hierarchical internal structures spanning multiple length/structure scales and the impediments involved in establishing invertible process-structure-property (PSP) linkages for these materials. More specifically, it is argued that modern data sciences (including advanced statistics, dimensionality reduction, and formulation of metamodels) and innovative cyberinfrastructure tools (including integration platforms, databases, and customized tools for enhancement of collaborations among cross-disciplinary team members) are likely to play a critical and pivotal role in addressing the above challenges.", "level": "Intermediate", "package_name": null, "created_by": "Georgia Institute of Technology", "package_num": null, "teach_by": [{"name": "Dr. Surya Kalidindi", "department": "The George W. Woodruff School of Mechanical Engineering"}], "target_audience": null, "rating": "4.1", "week_data": [{"description": "What you should know before you start the course", "video": ["Course Syllabus", "Frequently Asked Questions", "Suggested Reading", "Target Audience and Recommended Background", "Get More from Georgia Tech", "Consent Form"], "title": "Welcome"}, {"description": "•\tLearn and appreciate historical paradigms of advanced materials development while emphasizing the critical need for new approaches that employ data sciences and informatics as the glue to connect computational simulation and experiments to speed up the processes of materials discovery and development.\n•\tLearn about the emergence of key national and international 21st century initiatives in accelerated materials discovery and development and how they are expected to bring about a disruptive transformation of new product capabilities and time to market.", "video": ["Why Accelerate Material Discovery and Development?", "Historical Materials Development Cycles", "How do we accelerate materials development and deployment", "Emergence of multi-stakeholder initiatives", "The Materials Innovation Ecosystem", "Part 1:Multiscale Modeling and Multilevel Design of Materials", "Part 2: Multiscale Modeling and Multilevel design of Materials", "Decision-Making in Material Design", "Multilevel Systems-Based Materials Design", "Assignment #1", "Assignment #2", "Earn a Georgia Tech Badge/Certificate/CEUs", "Accelerating Materials Development and Deployment"], "title": "Accelerating Materials Development and Deployment "}, {"description": "•\tUnderstand property, structure and process spaces\n•\tLearn about Process-Structure-Property Linkages \n•\tLearn what does Materials Knowledge mean\n•\tLearn about a role of Data Science in Materials Knowledge System\n•\tOverview approaches and main components of Data Science\n•\tLearn about a new discipline - Materials Data Sciences", "video": ["Material Property, Material Structure, and Manufacturing Processes", "Process-Structure-Property (PSP) Linkages", "Role of Structures in PSP Linkages", "Data Science Terminology", "Main Components of Data Science", "What is Big Data?", "Materials Knowledge and Materials Data Science"], "title": "Materials Knowledge and Materials Data Science "}, {"description": "•\tLearn material structure and its digital representation\n•\tLearn how to calculate 2-point statistics \n•\tLearn how Principal Component Analysis can be used to reduce dimensionality\n•\tUnderstand Homogenization and Localization concepts\n", "video": ["Digital Representation of Material Structure", "Spatial Correlations: n-Point Statistics", "Computation and Visualization of 2-Point Spatial Correlations", "Principal Component Analyses (PCA) for low dimensional representations", "Principal Component Analyses (PCA) for low dimensional representation of material structure", "Homogenization: Passing Information to Higher Length Scales", "Materials Knowledge Improvement Cycles"], "title": "Materials Knowledge Improvement Cycles"}, {"description": "This module demonstrates a homogenization problem based on an example of two-phase composites", "video": ["Structure-Property Linkages using a Data Science Approach-Part 1", "Structure-Property Linkages using a Data Science Approach-Part 2", "Case Study in Homogenization: Plastic Properties of Two-Phase Composites"], "title": "Case Study in Homogenization: Plastic Properties of Two-Phase Composites"}, {"description": "•\tLearn about materials innovation system and cyberinfrastructure\n•\tReview Materials Databases, e-collaboration platforms and code repositories\n•\tLearn why integrated workflows are needed\n•\tDefine Metadata, Structured and Unstructured data\n•\tLearn about available services for e-collaborations\n", "video": ["Materials Innovation Ecosystem", "Materials Innovation Cyberinfrastucture", "e-Collaboration Platforms/Environments", "Materials Cyber-Infrastructure", "Introduction to PyMKS Materials Knowledge Systems in Python", "Materials Data Science with PyMKS", "PyMKS website", "PyMKS output", "Auto and Crosscorrelation for a microstructure", "Take another course like this !", "Materials Innovation Cyberinfrastructure and Integrated Workflows"], "title": "Materials Innovation Cyberinfrastructure and Integrated Workflows"}], "title": "Materials Data Sciences and Informatics"}, {"course_info": "About this course: Who is this course for ?\nThis course is RESTRICTED TO LEARNERS ENROLLED IN  Strategic Business Analytics SPECIALIZATION as a preparation to the capstone project. During the first two MOOCs, we focused on specific techniques for specific applications. Instead, with this third MOOC, we provide you with different examples  to open your mind to different applications from different industries and sectors.\nThe objective is to give you an helicopter overview on what's happening in this field. You will see how the tools presented in the two previous courses of the Specialization are used in real life projects. \nWe want to ignite your reflection process. Hence, you will best make use of the Accenture cases by watching first the MOOC and then investigate by yourself on the different concepts, industries, or challenges that are introduced during the videos.\n\nAt the end of this course learners will be able to: \n- identify the possible applications of business analytics,\n- hence, reflect on the possible solutions and added-value applications that could be proposed for their capstone project.\n\nThe cases will be presented by senior practitioners from Accenture with different backgrounds in term of industry, function, and country.  Special attention will be paid to the \"value case\" of the issue raised to prepare you for the capstone project of the specialization.\n\nAbout Accenture\nAccenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions—underpinned by the world’s largest delivery network—Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With more than 358,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com.", "level": null, "package_name": "Strategic Business Analytics Specialization ", "created_by": "ESSEC Business School", "package_num": "3", "teach_by": [{"name": "Nicolas Glady ", "department": "Marketing Department "}], "target_audience": null, "rating": null, "week_data": [{"description": "In this introductory module, Fabrice Marque, Managing Director Customer Strategy Practice Lead for France, Belgium and the Netherlands, also in charge of the ESSEC-Accenture Strategic Business Analytics Chair, will first introduce the MOOC in general. Then Michael Svilar, Global Accenture Data Science Group Lead, will identify the general trends in this sector. In this module, we will cover three different real-life examples. First, Rohit Banerji, Accenture business lead responsible for big data analytics for the resource sector, will present an example from a water utilities company. Second, Cian O’Hare, Managing Director at Accenture Digital, will present a case study from a global communication provider. Finally, Christopher Gray, public service expert at Accenture, will discuss challenges arising in the public sector where Analytics and Big Data can provide effective solutions.   \n\nAt the end of each example there will be quiz questions. Note that those questions may require you to collect additional information from that which was delivered during the videos. Do not hesitate to consult additional books, websites and examples about this topic: some of the answers can actually be found directly thanks to open access research engines or online encyclopedias! The objective with this final MOOC in the Strategic Business Analytics specialization is to assess whether you now master the different concepts that are implemented within this field.", "video": ["Introduction to Case Studies in Business Analytics with Accenture - Fabrice Marque", "Market trends and key challenges in Analytics - Mickael Svilar", "Why is Big Data really big? - Nicolas Glady", "Winning in Digital: Powered by Analytics - Jean-Pierre Bokobza", "Big data & predictive maintenance in the Utilities sector - Rohit Banerji", "Practice Quiz on resource Sector case", "Big data & advanced analytics in the Communications industry - Cian O’Hare", "Practice Quiz on the Global Communication case", "Advanced Analytics in the Public Service - Christopher Gray", "Practice quiz on the Public Service case", "Wrap-up: a conceptual framework of the applications of Big Data Analytics - Nicolas Glady", " Predictive maintenance for a water supplier : Internet of things"], "title": "Introduction to case studies in business analytics with Accenture"}, {"description": "During this module, different real-life examples will be discussed. Christine Removille, Digital Marketing Lead at the European Level, will present a data-centric digital transformation at a French TV company: Canal +. Edwin Van der Ouderaa, Financial Services Lead, will then explain how digital developments and data are disrupting the financial service sector.At the end of each video there will be  quiz questions. Do not hesitate to consult additional books, websites and examples about this topic! The objective with this final MOOC in the Strategic Business Analytics specialization is to assess whether you now master the different concepts that are implemented within this field.", "video": ["Context - Christine Removille", "Solution and success factors - Christine Removille", "Practice quiz on the \"Canal+\" case study", "Introduction and key digital trends in Financial Services", "Analytics capability based on “People like you\"", "How to leverage “People like you” micro-segmentation - Example 1 – Increase campaign yield", "How to leverage “People like you” micro-segmentation - Example 2 – Optimize pricing", "Digital transformation and wrap-up", "Practice quiz on the financial service case study"], "title": "Digital Transformation in the Media, the Financial Services and the Retail Sector"}, {"description": "During this module, two different real-life examples will be discussed. First, Paul Pierotti, Managing Director at Accenture Digital, will explain how Analytics can transform how health services are delivered. Second Xavier Cimino, Managing Director in charge of the Analytics Practice in the Life Science industry for Europe, will present an award-winning project in this sector. At the end of each video, there will be quiz questions. Do not hesitate to consult additional books, websites and examples about this topic! The objective with this final MOOC in the Strategic Business Analytics specialization is to assess whether you now master the different concepts that are implemented within this field.Finally, Michael Svilar, Global Accenture Data Science Group Lead, will conclude the MOOC.", "video": ["Healthcare analytics: a conceptual framework - Nicolas Glady", "Introduction and key challenges - Paul Pierotti", "Correlation between life expectancy and health spending - Paul Pierotti", "Presentation of 5 Health Analytics use cases - Paul Pierotti", "Focus on Care Management for patients with chronic diseases - Paul Pierotti", "Wrap up - Paul Pierotti", "Practice quiz on the health service case", "Advanced Analytics in the Pharmaceutical industry - Xavier Cimino", "Practice quiz on the Life science industry case", "How to create value from data? - Fabrice Marque", "Wrap up  - Mickael Svilar", "Data exploration is an iterative process - Nicolas Glady", "Analytics exploration - Oonagh O’Shea & Noelle Doody", "Wrap up & Capstone guidelines  - Nicolas Glady", "Preparation for the capstone project"], "title": "Advanced Analytics in Healthcare and the Pharmaceutical industry / Wrap up and Introduction to capstone"}], "title": "Case studies in business analytics with ACCENTURE"}, {"course_info": "About this course: Interprofessional Healthcare Informatics is a graduate-level, hands-on interactive exploration of real informatics tools and techniques offered by the University of Minnesota and the University of Minnesota's National Center for Interprofessional Practice and Education. We will be incorporating technology-enabled educational innovations to bring the subject matter to life. Over the 10 modules, we will create a vital online learning community and a working healthcare informatics network. \n\nWe will explore perspectives of clinicians like dentists, physical therapists, nurses, and physicians in all sorts of practice settings worldwide. Emerging technologies, telehealth, gaming, simulations, and eScience are just some of the topics that we will consider. \n\nThroughout the course, we’ll focus on creativity, controversy, and collaboration - as we collectively imagine and create the future within the rapidly evolving healthcare informatics milieu. All healthcare professionals and IT geeks are welcome!", "level": null, "package_name": null, "created_by": "University of Minnesota", "package_num": null, "teach_by": [{"name": "Karen  Monsen, Ph.D., RN, FAAN", "department": "School of Nursing"}], "target_audience": null, "rating": "4.3", "week_data": [{"description": "", "video": ["Course Introduction and Overview", "Welcome", "Course Team"], "title": "Introduction"}, {"description": "Week 1 begins! This week, we explore and apply theories of healthcare informatics to professional practice. By the end of this week, you will be able to: describe informatics theory, analyze informatics theory related to practice and analyze health topics of interest to healthcare.", "video": ["Informatics Theory Part 1", "Informatics Theory Part 2", "Further Discovery"], "title": "Informatics Theory"}, {"description": "This module explores and applies standardized terminologies to professional practice. By the end of this module, learners will be able to: analyze the transformation of data to information to knowledge and explore and apply standardized terminologies to professional practice.", "video": ["Data, Information, and Knowledge", "Introduction to the Omaha System", "An Omaha System Journey: Diane Thorson", "Further Discovery", "Data, Information, Knowledge, and the Omaha System "], "title": "Data, Information, and Knowledge"}, {"description": "This module links EHR use to evidence-based practice. By the end of this module, learners will be able to: identify the benefits and goals of an electronic health record and analyze evidence-based practice within the context of the electronic health record.", "video": ["Electronic Health Records ", "Evidence Based Practice", "Further Discovery", "Electronic Health Record (EHR) , Evidence-Based Practice (EBP)"], "title": "Electronic Health Record (EHR) Components, Evidence-Based Practice"}, {"description": "Week 5 begins! This week we examine informatics in relationship to new technologies in healthcare. Telehealth and technology are creating new ways to link people, and care, and health information. By the end of this week, you will be able to: examine applications of telehealth technologies and describe methods of engaging consumers in using health information technologies.", "video": ["Quality Improvement ", "Workflow Processes ", "Further Discovery", "Quality Improvement/ Workflow Analysis/ Redesign"], "title": "Quality Improvement/ Workflow Analysis/ Redesign"}, {"description": "This module examines informatics in relationship to new technologies in healthcare. By the end of this module, learners will be able to: examine applications of telehealth technologies and describe methods of engaging consumers in using health information technologies.", "video": ["Informatics and Telehealth ", "Telehealth ", "Consumer Health Informatics ", "Further Discovery", "Telehealth/ Consumer Health/ Mobile Technology"], "title": "Telehealth/ Consumer Health/ Mobile Technology"}, {"description": "This module relates informatics to community and population health. By the end of this module, learners will be able to: relate informatics to community and population health and analyze applications of geospatial information systems and health.\n", "video": ["Informatics and Population Health ", "Further Discovery", "Informatics and Population Health"], "title": "Community/ Population Health"}, {"description": "This module describes applications of gaming, simulation, and virtual reality tools in healthcare. By the end of this module, learners will be able to: analyze informatics and gaming in relationship to health and healthcare and describe use of simulations and informatics to improve healthcare quality.\n", "video": ["Informatics, Gaming, and Simulations", "Game Based Teaching and Computer Simulation ", "Computational Modeling and Simulation", "Further Discovery", "Education and Gaming"], "title": "Informatics, Gaming, and Simulation"}, {"description": "This module explores ethical issues related to healthcare informatics in the interprofessional context. By the end of this module, learners will be able to: explore ethical issues related to healthcare informatics in the interprofessional context and analyze security and privacy challenges related to healthcare informatics.", "video": ["Informatics and Ethics ", "Further Discovery", "Informatics and Ethics"], "title": "Informatics and Ethics"}, {"description": "This module explores interprofessional aspects of healthcare data exchange and interoperability. By the end of this module, learners will be able to: describe information exchange and interoperability and analyze interprofessional aspects of information exchange and interoperability in healthcare.", "video": ["Health Information Exchange ", "Standardization - Part 1 ", "Standardization - Part 2 ", "Standardization - Part 3 ", "Further Discovery", "Data Exchange and Interoperability"], "title": "Data Exchange and Interoperability"}, {"description": "This module explores the contribution of healthcare informatics to the foundation of knowledge in healthcare. By the end of this module, learners will be able to: analyze implications of Big Data for healthcare research and synthesize insights related to interprofessional healthcare informatics.", "video": ["Informatics and the Foundation of Knowledge ", "Data Visualization Part 1", "Data Visualization Part 2 ", "Synthesis ", "Further Discovery", "Informatics and the Foundation of Knowledge"], "title": "Informatics and the Foundation of Knowledge"}], "title": "Interprofessional Healthcare Informatics "}, {"course_info": "About this course: The Business Statistics and Analysis Capstone is an opportunity to apply various skills developed across the four courses in the specialization to a real life data. The Capstone, in collaboration with an industry partner uses publicly available ‘Housing Data’ to pose various questions typically a client would pose to a data analyst.\nYour job is to do the relevant statistical analysis and report your findings in response to the questions in a way that anyone can understand.", "level": null, "package_name": "Business Statistics and Analysis Specialization ", "created_by": "Rice University", "package_num": "5", "teach_by": [{"name": "Sharad Borle", "department": "Jones Graduate School of Business"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "", "video": ["A Summary of the Four Courses in the Specialization", "Lesson 1, Slides", "Pre-Course Survey", "Introducing the Capstone", "Lesson 2, Slides", "A Conversation with Garfield Fisher, the industry partner for the Capstone", "Q&A with Garfield Fisher", "Data for the Capstone Project", "The URL to download data for the Capstone Project", "List of variables to be used in Capstone", "Lesson 4, Slides", "Merging and Cleaning Data for the Capstone", "Lesson 5, Slides", "Capstone Quiz 1: Data in Excel"], "title": "Business Statistics and Analysis Capstone: An Introduction"}, {"description": "", "video": ["Introducing Assignment 1: Differences in Market Value of Housing Units", "Lesson 1, Slides", "Introducing Assignment 2: Fair Market Rent of Housing Units", "Lesson 2, Slides", "Week 2 Assessment 1 Sample Solution", "Week 2 Assessment 2 Instructions", "Week 2 Assessment 2 Sample Solution", "Week 2 Assessment 1", "Week 2 Assessment 2"], "title": "Business Statistics and Analysis Capstone: Assessments 1 & 2"}, {"description": "", "video": ["Introducing Assignment 3: A Model for Market Value of Housing Units", "Lesson 1, Slides", "Week 3 Assessment Sample Solution", "Week 3 Assessment"], "title": "Business Statistics and Analysis Capstone: Assessment 3"}, {"description": "", "video": ["Introducing Assignment 4: Building a Predictive Model for Market Value of Housing Units", "Lesson 1, Slides", "Week 4 Assessment Sample Solution", "Post-Course Survey", "Week 4 Assessment"], "title": "Business Statistics and Analysis Capstone: Assessment 4"}], "title": "Business Statistics and Analysis Capstone"}, {"course_info": "About this course: This is the second course in the Data to Insights specialization. Here we will cover how to ingest new external datasets into BigQuery and  visualize them with Google Data Studio. We will also cover intermediate SQL concepts like multi-table JOINs and UNIONs which will allow you to analyze data across multiple data sources.\n\nNote: Even if you have a background in SQL there are BigQuery specifics (like handling query cache and table wildcards) that may be new to you.", "level": "Beginner", "package_name": "From Data to Insights with Google Cloud Platform Specialization ", "created_by": "Google Cloud", "package_num": "2", "teach_by": [{"name": "Google Cloud Training", "department": null}], "target_audience": "Who is this class for: This is course is primarily aimed at new Data ​Analysts, ​Business ​Analysts, and Business ​Intelligence ​professionals. It is also intended for Cloud ​Data ​Engineers ​who ​will ​be partnering ​with ​Data ​Analysts ​to ​build scalable ​data ​solutions ​on ​Google Cloud ​Platform.", "rating": "4.2", "week_data": [{"description": "Create new permanent and temporary tables from your query results", "video": ["Creating Permanent Tables", "Temporary Tables and Query Results", "Performance Preview: Query Cache", "Creating Logical Views", "Lab 5 Overview", "Lab 5: Creating new Permanent Tables", "Lab 5 Solution", "Module 6 Quiz"], "title": "Module 6: Storing and Exporting Data"}, {"description": "Load and create new datasets inside BigQuery", "video": ["Ingesting New Data into BigQuery", "Demo: Loading Data into BigQuery", "Lab 6 Intro", "Lab 6: Ingesting and Querying New Datasets", "Lab 6 Solution", "Module 7 Quiz"], "title": "Module 7: Ingesting New Datasets into Google BigQuery"}, {"description": "Compare data visualizations and learn how to use Google Data Studio", "video": ["Overview of Data Visualization principles", "Comparing Good and Bad Visualizations", "Dimensions, Measures, and Reports", "Introducing Google Data Studio", "Demo: Data Studio Walkthrough", "Lab 7 Intro", "Lab 7: Exploring a Dataset in Google Data Studio", "Lab Solution", "Module 8 Quiz"], "title": "Module 8: Data Visualization"}, {"description": "Understand the differences between SQL JOINs and UNIONs and when to use each", "video": ["Introducing JOINs and UNIONs", "Introducing BigQuery Table Wildcards for Easy Merges", "SQL JOIN Syntax", "Walkthrough: SQL Join Types", "Avoiding Pitfalls when Merging Datasets", "Lab 8 Intro", "Lab 8: UNIONING and JOINING Datasets", "Lab 8 Solution", "Module 9 Quiz"], "title": "Module 9: Joining and Merging Datasets"}], "title": "Creating New BigQuery Datasets and Visualizing Insights"}, {"course_info": "About this course: There is a significant number of tasks when we need not just to process an enormous volume of data but to process it as quickly as possible. Delays in tsunami prediction can cost people’s lives. Delays in traffic jam prediction cost extra time. Advertisements based on the recent users’ activity are ten times more popular.\n\nHowever, stream processing techniques alone are not enough to create a complete real-time system. For example to create a recommendation system we need to have a storage that allows to store and fetch data for a user with minimal latency. These databases should be able to store hundreds of terabytes of data, handle billions of requests per day and have a 100% uptime. NoSQL databases are commonly used to solve this challenging problem.\n\nAfter you finish this course, you will master stream processing systems and NoSQL databases. You will also learn how to use such popular and powerful systems as  Kafka, Cassandra and Redis.\n\nTo get the most out of this course, you need to know Hadoop and Hive. You should also have a working knowledge of Spark, Spark SQL and Python.\n\nDo you want to learn how to build Big Data applications that can withstand modern challenges? Jump right in!", "level": "Advanced", "package_name": "Big Data for Data Engineers Specialization ", "created_by": "Yandex", "package_num": "4", "teach_by": [{"name": "Ivan Puzyrevskiy", "department": null}, {"name": "Pavel Mezentsev ", "department": "PulsePoint inc"}, {"name": "Emeli Dral ", "department": null}, {"name": "Alexey A. Dral", "department": "Algorithms and Programming Technologies dept. MIPT"}], "target_audience": "Who is this class for: This course is aimed to everybody, who feel interest in Big Data. As the technologies covered throughout the course operate in Unix environment, we expect you to have basic understanding of the subject. Things like processes and files assumed to be familiar for the learner. Python is required to complete programming assignments. To get the most out of this course, you need to know Hadoop and Hive. You should also have a working knowledge of Spark, Spark SQL and Python.", "rating": null, "week_data": [], "title": "Big Data Applications: Real-Time Streaming"}, {"course_info": "About this course: In this course you will learn how to use survey weights to estimate descriptive statistics, like means and totals, and more complicated quantities like model parameters for linear and logistic regressions.  Software capabilities will be covered with R® receiving particular emphasis.  The course will also cover the basics of record linkage and statistical matching—both of which are becoming more important as ways of combining data from different sources.  Combining of datasets raises ethical issues which the course reviews.  Informed consent may have to be obtained from persons to allow their data to be linked. You will learn about differences in the legal requirements in different countries.", "level": null, "package_name": "Survey Data Collection and Analytics  Specialization ", "created_by": "University of Maryland, College Park", "package_num": "6", "teach_by": [{"name": "Richard Valliant, Ph.D.", "department": "Joint Program in Survey Methodology"}], "target_audience": null, "rating": "4.4", "week_data": [{"description": "After completing Modules 1 and 2 of this course you will understand how to estimate descriptive statistics, overall and for subgroups, when you deal with survey data.  We will review software for estimation (R, Stata, SAS) with examples for how to estimate things like means, proportions, and totals.  You will also learn how to estimate parameters in linear, logistic, and other models and learn software options with emphasis on R. Module 3 and 4 discuss how you can add additional data to your analysis. This requires knowing about record linkage techniques, and what it takes to get permission to link data.", "video": ["Overview", "Slides", "Basic R examples", "Basic R examples (continued)", "Slides", "Degrees of Freedom", "Slides", "Estimating Means", "Multistage samples", "Slides", "Slides (continued)", "Quantile estimation in R", "Slides", "Course 6 Module 1"], "title": "Basic Estimation"}, {"description": "Module 2 covers how to estimate linear and logistic model parameters using survey data. After completing this module, you will understand how the methods used differ from the ones for non-survey data. We also cover the features of survey data sets that need to be accounted for when estimating standard errors of estimated model parameters.", "video": ["Introduction", "Slides", "Estimation Method", "Slides", "Linear Models", "Slides", "Diagnostics in R", "Slides", "Linear Models in Stata", "Slides", "Logistic Models in R", "Slides", "Odds Ratios", "Slides", "Logistic Regression in Stata", "Slides", "Course 6 Module 2"], "title": "Models"}, {"description": "Module starts with the current debate on using more (linked) administrative records in the U.S. Federal Statistical System, and a general motivation for linking records. Several examples will be given on why it is useful to link data. Challenges of record linkage will be discussed. A brief overview over key linkage techniques is included as well.", "video": ["Improving Federal Statistics Using Multiple Data Sources", "Longitudinal Employer-Household Dynamics (LEHD)", "Impact of Research on Innovation, Competition and Science", "Country specific examples", "Why we link records", "Slides", "Gentle Introduction", "Slides - Introduction", "Technical Overview - Software", "Challenges", "Slides: Challenges", "Key Techniques", "Slides", "Record Linkage (Herzog/Scheuren/Winkler 2010)", "Febrl - A Freely Available Record Linkage System (Christen)", "Machine Learning and Record Linkage (Winkler 2011)", "Privacy Preserving Record Linkage (Schnell et al. 2009)", "Quiz 3 - Record Linkage"], "title": "Record Linkage"}, {"description": "This module will discuss key issues in obtaining consent to record linkage. Failure to consent can lead to bias estimates. Current research examples will be given as well as practical suggestions on how to obtain linkage consent.\n", "video": ["Privacy and Confidentiality", "Linkage Consent and Consent Bias", "Correlates of Consent", "Bias in Administrative Estimates", "Optimizing Linkage Consent", "Slides", "Assessing the Magnitude of Non-Consent Biases (Sakshaug & Kreuter 2012)", "Placement, Wording and Interviewers (Sakshaug et al.)", "Quiz - Linkage Consent"], "title": "Ethics"}], "title": "Combining and Analyzing Complex Data"}, {"course_info": "About this course: The course presents an overview of the theory behind biological diversity evolution and dynamics and of methods for diversity calculation and estimation. We will become familiar with the major alpha, beta, and gamma diversity estimation techniques.\n\nUnderstanding how biodiversity evolved and is evolving on Earth and how to correctly use and interpret biodiversity data is important for all students interested in conservation biology and ecology, whether they pursue careers in academia or as policy makers and other professionals (students graduating from our programs do both). Academics need to be able to use the theories and indices correctly, whereas policy makers must be able to understand and interpret the conclusions offered by the academics.\n\nThe course has the following expectations and results:\n\n- covering the theoretical and practical issues involved in biodiversity theory,\n- conducting surveys and inventories of biodiversity,\n- analyzing the information gathered,\n- and applying their analysis to ecological and conservation problems.\n\nNeeded Learner Background:\n\n- basics of Ecology and Calculus\n- good understanding of English", "level": "Intermediate", "package_name": null, "created_by": "National Research Tomsk State University", "package_num": null, "teach_by": [{"name": "Roberto Cazzolla Gatti", "department": "Biological Diversity and Ecology Laboratory,  Bio-Clim-Land Centre of Excellence, Biological Institute"}], "target_audience": "Who is this class for: The course is for:\n- lower-intermediate division undergraduate\n- upper division undergraduate\n- Professional or Graduate\nMaster and Ph.D. students", "rating": "4.2", "week_data": [{"description": "This module represents the course content and its author as well as contains the additional materials to the course", "video": ["Course promo", "From the course author", "About the course team", "About the International Master's Degree in Biodiversity", "Cited literature and further readings", "Entering questionnaire", "About the university"], "title": "Welcome to the course \"Biological Diversity (Theories, Measures and Data sampling techniques)\""}, {"description": "In this module we will explore the evolution of biodiversity. In particular we will understand what is the web of life and how species interact to coexist.Moreover, we will understand the main processes that allow the evolution of biodiversity and how it is structured and structures itself. Finally, we will review the distribution patterns of biodiversity in macroscale.", "video": ["1.1. The web of life", "1.2. Species interactions and biodiversity", "1.3. The evolution of biodiversity", "1.4. The structure of biodiversity", "1.5. The distribution of biodiversity in macroscale", "Suggested readings – Module 1", "Quiz – Module 1"], "title": "Biodiversity and evolution"}, {"description": "After having analysed the distribution of biodiversity in macroscale in the previous module we will see it in microscale.\nThen I will explain you the importance of biodiversity: first we will see what are the effect of anthropogenic impacts, and second we will see why biodiversity is important for us. We will try to answer the important question about what are the causes of biodiversity decline and we will analyse the effect of climate change on species diversity, ecosystems and the whole planet. With a global perspective we will explore the implication for biodiversity of the Gaia theory.\n", "video": ["2.1. The distribution of biodiversity in microscale", "2.2. Why biodiversity is important for us", "2.3. What are the causes of biodiversity decline?", "2.4. Climate change and biodiversity", "2.5. Gaia and biodiversity", "Suggested readings – Module 2", "Quiz – Module 2"], "title": "Importance of biodiversity and anthropogenic impacts "}, {"description": "In this module we will move from a theoretical discussion to a more practical point of view and we will see hot to analyse and measure biodiversity. I will show you some sampling techniques and how to avoid the most common sampling errors. We will talk about the relevant problem of pseudoreplication.\nThen I will show you some metrics to estimate α and β –diversity and how to use them in particular cases and specific situations.\n", "video": ["3.1. Analyse and measure biodiversity – I part", "3.2. Analyse and measure biodiversity – II part", "3.3. Sampling Techniques", "3.4. α-diversity", "3.5. β-diversity", "Suggested readings – Module 3", "Quiz – Module 3"], "title": "Analyse and measure biodiversity"}, {"description": "In this module we will talk about the most common species-abundance distribution models and I will show you how to compare different communities and samples in order to achieve a quantitative and statistical measure of the changes in biological diversity due to treatments.\nI will explain some Evenness measures and how to represent them in form of curves of biodiversity. This will help to discriminate communities’ diversity and to better analyse the anthropogenic impacts on biodiversity.\n", "video": ["4.1. Evenness", "4.2. Species abundance models", "4.3. Curves of biodiversity", "4.4. How to compare communities’ diversity", "4.5. How to analyse the anthropogenic impacts on biodiversity", "Suggested readings – Module 4", "Quiz – Module 4"], "title": "Species-abundance distributions and comparisons"}, {"description": "In this module I will show you some of the most useful alternative measures of biodiversity. We will explore the meaning of functional and taxonomic (phylogenetic) diversity and I will explain you how to use these metrics for a more complete understanding of the patterns governing communities’ diversity. Then I will provide you some sketches about qualitative measures of biodiversity.", "video": ["5.1. Functional diversity", "5.2. Taxonomic and phylogenetic diversity", "5.3. Qualitative measures of biodiversity", "Suggested readings – Module 5", "Quiz – Module 5"], "title": "Alternative measures of biodiversity"}, {"description": "The last module (n° 6) of this course will be dedicated to statistics applied to the analysis of biodiversity. We will see how to apply the information gathered in the previous modules to obtain a statistical significance. We will explore parametric and non-parametric tests, the useful chi-square test, the correct application of correlation and the regression analysis, and some hints about the multivariate analysis techniques, such as ANOVA.", "video": ["6.1. Statistics applied to biodiversity – I part", "6.2. Statistics applied to biodiversity – II part", "6.3. Statistics applied to biodiversity – III part", "6.4. Statistics applied to biodiversity – IV part", "6.5. Statistics applied to biodiversity – V part", "6.6. Statistics applied to biodiversity – VI part", "6.7. Statistics applied to biodiversity – VII part", "Suggested readings – Module 6", "Final questionnaire", "Quiz – Module 6"], "title": "Statistics applied to the analysis of biodiversity "}], "title": "Biological Diversity (Theories, Measures and Data sampling techniques)"}, {"course_info": "About this course: The third course in this specialization is Achieving Advanced Insights with BigQuery. Here we will build on your growing knowledge of SQL as we dive into advanced functions and how to break apart a complex query into manageable steps. \n\nWe will cover the internal architecture of BigQuery (column-based sharded storage) and advanced SQL topics like nested and repeated fields through the use of Arrays and Structs. Lastly we will dive into optimizing your queries for performance and how you can secure your data through authorized views.", "level": "Beginner", "package_name": "From Data to Insights with Google Cloud Platform Specialization ", "created_by": "Google Cloud", "package_num": "3", "teach_by": [{"name": "Google Cloud Training", "department": null}], "target_audience": "Who is this class for: This is course is primarily aimed at new Data ​Analysts, ​Business ​Analysts, and Business ​Intelligence ​professionals. It is also intended for Cloud ​Data ​Engineers ​who ​will ​be partnering ​with ​Data ​Analysts ​to ​build scalable ​data ​solutions ​on ​Google Cloud ​Platform.", "rating": "4.8", "week_data": [{"description": "Deepen your knowledge of SQL on BigQuery by learning about more advanced functions like statistical approximations, analytical window queries, user-defined functions, and WITH clauses.", "video": ["Advanced Functions - Statistical and Approximations", "Achieving Advanced Insights with BigQuery", "BigQuery User-Defined Functions (UDFs)", "Achieving Advanced Insights with BigQuery", "Lab 9 Overview", "Lab 9: Deriving Insights from Advanced SQL Functions", "Lab 9 Solution", "Advanced Functions"], "title": "Module 10: Advanced Functions and Clauses"}, {"description": "Walkthrough the evolution of how traditional databases handle dataset scale and compare how BigQuery was developed to address scaling limitations. Deep dive into nested and repeated fields which are a key part of denormalized BigQuery data structures.", "video": ["Background: Traditional Relational Database Architecture", "Denormalized, Column-Based Storage", "Table Sharding", "Introducing Nested and Repeated Fields", "Working with Arrays and Structs in BigQuery", "BigQuery Standard SQL vs Legacy SQL", "Practicing SQL on Repeated Fields in BigQuery", "Lab 10: Practice Querying Nested and Repeated Fields", "Lab Solution Walkthrough", "BigQuery Architecture"], "title": "Module 11: Schema Design and Nested Data Structures"}, {"description": "Dive deeper into advanced visualization topics like dashboard calculated fields, filters, multi-page reports, and dashboard cache.", "video": ["Advanced Data Studio: Case Statements and Filters", "Sharing Dashboards and Data Access Pitfalls", "Advanced Google Data Studio"], "title": "Module 12: More Visualization with Google Data Studio"}, {"description": "Learn the fundamental pieces of work that impact BigQuery performance and how to optimize your queries for speed.", "video": ["Avoid BigQuery Performance Pitfalls", "Data Skew in your Datasets", "Diagnose Performance Issues with the Query Explanation map", "Lab Overview", "Lab 11: Optimizing and Troubleshooting Query Performance", "Lab Solution Walkthrough", "Performance Optimization"], "title": "Module 13: Optimizing for Performance"}, {"description": "Introducing Cloud Datalab -- a key tool in the Data Scientist toolkit -- which enables analysts to collaborate through the use of scalable cloud notebooks.", "video": ["Introducing Cloud Datalab", "Demo: Cloud Datalab Notebook Cells", "Benefits of Cloud Datalab", "Cloud Datalab"], "title": "Module 14: Advanced Insights with Cloud Datalab"}, {"description": "Securing and sharing your BigQuery datasets is critical for any organization. Learn what Google Cloud Platform and BigQuery tools are available to you to permission control and share your data. ", "video": ["Data Access Roles, Creating Authorized Views, and Common Pitfalls", "Data Access"], "title": "Module 15: Data Access"}, {"description": "Congratulations! You have made it to the end (or rather the beginning) of your data analyst journey. Learn what recommended resources are available to you for continued training.", "video": ["End of Course Recap"], "title": "Completion"}], "title": "Achieving Advanced Insights with BigQuery"}, {"course_info": "About this course: The Executive Data Science Capstone, the specialization’s culminating project, is an opportunity for people who have completed all four EDS courses to apply what they've learned to a real-world scenario developed in collaboration with Zillow, a data-driven online real estate and rental marketplace, and DataCamp, a web-based platform for data science programming. Your task will be to lead a virtual data science team and make key decisions along the way to demonstrate that you have what it takes to shepherd a complex analysis project from start to finish.  For the final project, you will prepare and submit a presentation, which will be evaluated and graded by your fellow capstone participants.\n\nCourse cover image by Luckey_sun. Creative Commons BY-SA https://flic.kr/p/bx1jvU", "level": null, "package_name": "Executive Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "5", "teach_by": [{"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Brian Caffo, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "It's time to put your skills to the test managing a data science project at Zillow, a data-driven online real estate and rental marketplace. Along the way, you'll make important decisions as you lead your team through the project. ", "video": ["A Welcome Message from Roger Peng", "Welcome to the Executive Data Science Capstone!", "Pre-Course Survey", "Instructions", "Grading", "Post-Course Survey", "Executive Data Science Capstone on DataCamp"], "title": "Executive Data Science Capstone"}], "title": "Executive Data Science Capstone"}, {"course_info": "About this course: What happens when creativity and science come together?  The power to design our world is unleashed, providing tools to inform choices about how we live!  Geodesign is the glue—it’s a process that deploys creativity to connect information to people, using collaboration to better inform how we design our world.\n\nThis course includes well-illustrated lectures by the instructor, but also guest lectures each week to ensure you are hearing a variety of viewpoints.  Each week you will also be able to examine what geodesign is through interactive mapping that showcases real-word Case Study examples of geodesign from around the globe.  As you move along in the course, you will discover the interrelationships of both the physical and human aspects that contribute to how geodesign strategies are composed.  The course concludes with you outlining your own Geodesign Challenge, and receiving feedback about that from your peers.", "level": "Beginner", "package_name": null, "created_by": "The Pennsylvania State University", "package_num": null, "teach_by": [{"name": "Prof. Kelleann Foster", "department": "Stuckeman School of Architecture and Landscape Architecture"}], "target_audience": null, "rating": "4.8", "week_data": [{"description": "This week we’ll begin your geodesign journey by helping you understand that the roots of geodesign are comprised of languages we all share.  By languages I mean commonalities everyone can relate to; for example: our awareness of the spaces around us, our connection to place (location), and the way we deal with problem solving nearly everyday.", "video": ["Course Introduction Video", "Grading and Logistics", "Community Map", "Cool Geodesign Resources (Use in Future Weeks; For Your Own Library Too)", "Main Lecture: Week 1", "Alternate Access Main Lecture as Text: Week 1", "Guest Lecture Information: Week 1", "Guest Lecture 1: Week 1", "Guest Lecture 2: Week 1", "Change Agent Overview: Week 1", "Case Study Examples: Week 1", "Activities and Explorations: Week 1", "Week 1: Shared Languages"], "title": "Week 1: Shared Languages"}, {"description": "In this second week we begin piecing together some of the key components that are central to the geodesign process. These happen to all start with \"D\": Design, Decision, and Data. By the end of this lesson I hope you will recognize how these three key components are interrelated.", "video": ["Main Lecture: Week 2", "Alternate Access Main Lecture as Text: Week 2", "Guest Lecture Information: Week 2", "Guest Lecture 1: Week 2", "Guest Lecture 2: Week 2", "Change Agent Overview: Week 2", "Case Study Examples: Week 2", "Activities and Explorations: Week 2", "Week 2: The Three D's of Geodesign"], "title": "Week 2: The Three D's of Geodesign"}, {"description": "In this third week you will begin to recognize the complexities inherent in the geodesign process.We will also get into two topics that without which, geodesign is likely to fail: computation and collaboration. As it happens these all begin with “C,” hence this week is the Three C's of Geodesign. By the end of this lesson I hope you will recognize how these three key topics are put into action to accomplish geodesign.", "video": ["Main Lecture Week 3", "Alternate Access Main Lecture as Text: Week 3", "Guest Lecture Information: Week 3", "Guest Lecture 1: Week 3", "Guest Lecture 3: Week 3", "Change Agent Overview: Week 3", "Case Study Examples: Week 3", "Activities and Explorations: Week 3", "Week 3: The Three C's of Geodesign"], "title": "Week 3: The Three C's of Geodesign"}, {"description": "In this fourth week you will gain an understanding about the interrelationships of the physical and human aspects that contribute to how geodesign strategies are composed. Those physical and human aspects comprise the context for a geodesign challenge, so this lesson is titled \"The Influence of Context.\" By the end of this lesson I hope you will recognize that through understanding the cultural and physical context of a place your geodesign team will be better equipped to propose sustainable solutions.", "video": ["Main Lecture Week 4", "Alternate Access Main Lecture as Text: Week 4", "Guest Lecture Information: Week 4", "Guest Lecture 1: Week 4", "Guest Lecture 2: Week 4", "Change Agent Overview: Week 4", "Case Study Examples: Week 4", "Activities and Explorations: Week 4", "Quiz 4: The Influence of Context"], "title": "Week 4: The Influence of Context"}, {"description": "We have been building up to this week, and now we are here-- putting all the pieces together in the geodesign process!  In this last week (so sad to say that!) you will gain awareness about the three-stage geodesign process.  The lecture includes a overview about the process through a discussion centered on one of the Case Study examples. We also have two guest lectures that share the geodesign process as applied to their work.  By the end of this week I hope you will be able to distinguish between the various components of the geodesign process and how each one's role contributes to and is valuable to the whole process.", "video": ["Main Lecture Week 5", "Alternate Access Main Lecture as Text: Week 5", "Guest Lecture Information: Week 5", "Guest Lecture 1: Week 5", "Guest Lecture 2: Week 5", "Change Agent Overview: Week 5", "Case Study Examples: Week 5", "Activities and Explorations: Week 5", "Sharing a Geodesign Challenge"], "title": "Week 5: Process and Framework"}], "title": "Geodesign: Change Your World"}, {"course_info": "About this course: In this Capstone you will recommend a business strategy based on a data model you’ve constructed. Using a data set designed by Wharton Research Data Services (WRDS), you will implement quantitative models in spreadsheets to identify the best opportunities for success and minimizing risk. Using your newly acquired decision-making skills, you will structure a decision and present this course of action in a professional quality PowerPoint presentation which includes both data and data analysis from your quantitative models.\n\nWharton Research Data Services (WRDS) is the leading data research platform and business intelligence tool for over 30,000 corporate, academic, government and nonprofit clients in 33 countries. WRDS provides the user with one location to access over 200 terabytes of data across multiple disciplines including Accounting, Banking, Economics, ESG, Finance, Insurance, Marketing, and Statistics.", "level": null, "package_name": "Business and Financial Modeling Specialization ", "created_by": "University of Pennsylvania", "package_num": "5", "teach_by": [{"name": "Richard Lambert", "department": "Accounting- Wharton School"}, {"name": "Robert W. Holthausen", "department": "Accounting"}, {"name": "Don Huesman", "department": "Innovation Group- Wharton School"}, {"name": "Richard Waterman", "department": "Statistics-Wharton School"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "Welcome!  This opening module was designed to give you an overview of the Business and Financial Modeling Capstone, in which you will be working with historical financial data to calculate individual returns and summary statistics on those returns. The project has multiple steps, which are outlined below in the \"Project Prompt\", and culminates in a recommendation for portfolio allocation that you will prepare a presentation on. You will draw on elements from all courses to complete this project, and you can use your final presentation as a work sample to improve your current job or even find a new one.  Before moving on, complete the \"Project Scope Quiz.\" The work you do this week enables you to understand the steps needed to successfully complete your final project.", "video": ["Project Description - Read me first!", "Project Prompt", "Historical Stock Data", "Module 1 Discussion: Introductions", "Questions about the Project", "Project Scope Quiz"], "title": "Getting Started"}, {"description": "In this module, which correlates to Steps 1 and 2 in the Project Prompt, you'll be working with a historical data set to calculate performance data and to provide summary statistics on that data. These calculations will allow you to practice using Spreadsheets for financial calculations, and provides the foundational skills and numbers for the next steps of the project. First, you'll use the set to calculate daily returns on a set of securities. You'll then use your Spreadsheet skills to calculate summary statistics. You'll be given the opportunity to test your knowledge with a sample return to see if your calculations are correct.  And you may want to refresh your recollection of the content from the Specialization with the lectures included here.  The work you complete this week allows you to form the basis for comparing stock performance, which you will use in creating the investment portfolio for your final project as well as the comparison to the performance of a single stock.", "video": ["More on Close Price versus Adjusted Close Price", "More on the Sharpe Ratio", "Sample Returns Spreadsheet (AAPL)", "Questions about Calculating Performance and Summary Statistics", "Definition and Uses of Models, Common Functions (Fundamentals of Quantitative Modeling)", "How Models are Used in Practice (Fundamentals of Quantitative Modeling)", "Mathematical Functions (Fundamentals of Quantitative Modeling)", "Navigating a Spreadsheet and Crafting Formulas (Introduction to Spreadsheets)", "How To Build an Optimization Model: Hudson Readers Ad Campaign (Modeling Risk and Realities)", "Data and Visualization: Graphical Representation (Modeling Risk and Realities)", "PDFs of Refresher Video Slides", "Daily Returns Quiz", "Summary Statistics Quiz"], "title": "Steps 1 and 2: Yahoo Finance"}, {"description": "In this module, you'll go beyond calculating simple returns to tackle the more advanced task of finding the minimum variance and \"optimal risk portfolio\" weights for a portfolio of selected securities (note, the \"optimal risky portfolio\" is also known as an \"optimal portfolio\" or \"tangent portfolio\").  You'll follow the tasks in Step 3 in the Project Prompt and use the resources below to calculate the portfolio weights for two securities that results in the portfolio with the minimum variance; then, you'll calculate the \"optimal risky portfolio\" on the efficient frontier for these same two securities, then for all 10 stocks in the pool.  You'll be quizzed on your calculations and other insights that emerge from this exercise. The work you complete this week gives you practice in creating an optimal risky portfolio, which is a key component of your final project. Note:  There are a number of resources available on the internet providing step-by-step instructions on how to use Excel to create an \"optimal risky portfolio\" on the efficient frontier given a certain set of available assets.  We encourage you to attempt to use the skills you gained during the Specialization to work through these steps independently; you are, however, permitted to utilize third-party resources if you find it necessary.  We've included some lectures from the underlying Specialization courses concerning Solver, optimization, and other relevant topics.  ", "video": ["Videos Explaining the Efficient Frontier and Optimal Risky Portfolio", "Third party resources for calculations", "More on Portfolio Variance", "More on the Efficient Frontier", "More on Short Selling", "Questions about minimum variance and \"optimal risky portfolio\" weights", "Introducton to Linear Models and Optimization (Fundamentals of Quantitative Modeling)", "Present and Future Value (Fundamentals of Quantitative Modeling)", "Optimization (Fundamentals of Quantitative Modeling)", "Linear Programming (incl. Solver) (Introduction to Spreadsheets)", "Optimizing with Solver, and Alternative Data Inputs (Modeling Risk and Realities)", "Adding Risk: Managing Investments at Epsilon Delta Capital (Modeling Risk and Realities)", "Using Scenarios for Optimizing Under High Uncertainty, Sensitivity Analysis and Efficient Frontier (Modeling Risk and Realities)", "PDFs of Refresher Video Slides", "The Minimum Variance and Optimal Risky Portfolio"], "title": "Step 3: Creating an optimal risky portfolio on the efficient frontier"}, {"description": "The Capital Asset Pricing Model, or CAPM, is another tool used by investors to weigh the risks and rewards of potential investments.  In this optional module covering Step 4 in the Project Prompt, you can use CAPM as a vehicle to further strengthen your financial modeling skills, including using regression concepts.  You may revisit the Specialization lectures below touching on regression.  To test whether you've grasped the concepts in the CAPM model, this module includes a short quiz.  This assessment is formative, meaning your score will not count towards your final grade. The work you do this week may inform how you build the mixed asset portfolio of your final project, but it is not necessary to complete the final project.", "video": ["Video on the Capital Asset Pricing Model (CAPM)", "More on the Capital Asset Pricing Model", "Capital Asset Pricing Model", "Questions, comments, and helpful resources for CAPM", "Introduction to Regression (Fundamentals of Quantitative Modeling)", "Use of Regression Models (Fundamentals of Quantitative Modeling)", "Correlation and Regression (Introduction to Spreadsheets)", "PDFs of Refresher Video Slides"], "title": "Step 4: Optional exercise using CAPM tables"}, {"description": "In this final module you are asked to move beyond a stock-only portfolio to one utilizing more diversified assets and to prepare a short presentation summarizing your findings.  As explained in Step 5 of the Project Prompt, you have $5 million to invest in the Vanguard Total Bond Market Index Fund (ticker: VBTLX) and Vanguard 500 Index (ticker: VFIAX) investment vehicles.  There are two assessments in this module. First, you'll complete a short quiz on the characteristics of your optimal risky portfolio.  Then, in the peer review component of this Capstone, you are tasked with preparing a short presentation that (i) explores how your portfolio of mixed asset class of funds compares to a single security (AAPL) and (ii) uses that comparison to discuss the importance of portfolio diversification.", "video": ["VBTLX and VFIAX Monthly Returns", "Module 5 Discussion - Reflect on your experience and share your insights", "How did you create a Mixed Asset Portfolio?", "Arguments for and against mixed asset portfolios", "Working with a Diversified Portfolio", "Portfolio Performance Presentation"], "title": "Step 5: Creating Your Asset Allocation & Final Presentation"}], "title": "Wharton Business and Financial Modeling Capstone"}, {"course_info": "About this course: The Library of Integrative Network-based Cellular Signatures (LINCS) is an NIH Common Fund program. The idea is to perturb different types of human cells with many different types of perturbations such as: drugs and other small molecules; genetic manipulations such as knockdown or overexpression of single genes; manipulation of the extracellular microenvironment conditions, for example, growing cells on different surfaces, and more. These perturbations are applied to various types of human cells including induced pluripotent stem cells from patients, differentiated into various lineages such as neurons or cardiomyocytes. Then, to better understand the molecular networks that are affected by these perturbations, changes in level of many different variables are measured including: mRNAs, proteins, and metabolites, as well as cellular phenotypic changes such as changes in cell morphology. The BD2K-LINCS Data Coordination and Integration Center (DCIC) is commissioned to organize, analyze, visualize and integrate this data with other publicly available relevant resources. In this course we briefly introduce the DCIC and the various Centers that collect data for LINCS. We then cover metadata and how metadata is linked to ontologies. We then present data processing and normalization methods to clean and harmonize LINCS data. This follow discussions about how data is served as RESTful APIs. Most importantly, the course covers computational methods including: data clustering, gene-set enrichment analysis, interactive data visualization, and supervised learning. Finally, we introduce crowdsourcing/citizen-science projects where students can work together in teams to extract expression signatures from public databases and then query such collections of signatures against LINCS data for predicting small molecules as potential therapeutics.", "level": "Intermediate", "package_name": null, "created_by": "Icahn School of Medicine at Mount Sinai", "package_num": null, "teach_by": [{"name": "Avi Ma’ayan, PhD", "department": "Professor, Department of Pharmacological Sciences"}], "target_audience": "Who is this class for: Learn various methods of analysis including: unsupervised clustering, gene-set enrichment analysis, interactive data visualization, and supervised machine learning with application to data from the Library of Integrated Network-based Cellular Signature (LINCS) program, and other relevant Big Data from high content molecular omics data and phenotype profiling of mammalian cells.", "rating": "5.0", "week_data": [{"description": "This module provides an overview of the concept behind the LINCS program; and tutorials on how to get started with using the LINCS L1000 dataset.", "video": ["Syllabus", "Grading and Logistics", "Layers of Cellular Regulation and Omics Technologies", "The Connectivity Map", "Geometrical View of the Connectivity Map Concept", "LINCS Data and Signature Generation Centers", "BD2K-LINCS Data Coordination and Integration Center", "Induced Pluripotent Stem Cells (iPSCs)", "Introduction to LINCS L1000 Data", "LINCS L1000 Data - Practice Exercise", "L1000 Characteristic Direction Signature Search Engine (L1000CDS2) Demo"], "title": "The Library of Integrated Network-based Cellular Signatures (LINCS) Program Overview"}, {"description": "This module includes a broad high level description of the concepts behind metadata and ontologies and how these are applied to LINCS datasets.", "video": ["Introduction to Metadata and Ontologies | Part 1", "Introduction to Metadata and Ontologies | Part 2"], "title": "Metadata and Ontologies"}, {"description": "In this module we explain the concept of accessing data through an application programming interface (API).", "video": ["Accessing and Serving Data through RESTful APIs | Part 1", "Accessing and Serving Data through RESTful APIs | Part 2", "Accessing Data through the Harmonizome's RESTful API - Practice Exercise"], "title": "Serving Data with APIs"}, {"description": "This module describes the important concept of a Bioinformatics pipeline.", "video": ["Analyzing Big Data with Computational Pipelines", "Bioinformatics Pipeline - Practice Exercise"], "title": "Bioinformatics Pipelines"}, {"description": "This module describes a project that integrates many resources that contain knowledge about genes and proteins. The project is called the Harmonizome, and it is implemented as a web-server application available at: http://amp.pharm.mssm.edu/Harmonizome/ ", "video": ["The Harmonizome Concept", "Processing Datasets | Part 1", "Processing Datasets | Part 2", "Processing Datasets | Part 3", "Harmonizome - Practice Exercise"], "title": "The Harmonizome"}, {"description": "This module describes the mathematical concepts behind data normalization.", "video": ["Data Normalization | Part 1", "Data Normalization | Part 2", "Data Normalization - Practice Exercise"], "title": "Data Normalization"}, {"description": "This module describes the mathematical concepts behind data clustering, or in other words unsupervised learning - the identification of patterns within data without considering the labels associated with the data. ", "video": ["Data Clustering | Part 1 | Introduction", "Data Clustering | Part 2 | Distance Functions ", "Data Clustering | Part 3 | Algorithms and Evaluation", "Data Clustering - Practice Exercise"], "title": "Data Clustering"}, {"description": "The Midterm Exam consists of 45 multiple choice questions which covers modules 1-7. Some of the questions may require you to perform some analysis with the methods you learned throughout the course on new datasets. ", "video": ["Midterm Exam"], "title": "Midterm Exam"}, {"description": "This module introduces the important concept of performing gene set enrichment analyses. Enrichment analysis is the process of querying gene sets from genomics and proteomics studies against annotated gene sets collected from prior biological knowledge.", "video": ["Enrichment Analysis | Part 1", "Enrichment Analysis | Part 2", "Enrichr Demo"], "title": "Enrichment Analysis"}, {"description": "This module describes the mathematical concepts of supervised machine learning, the process of making predictions from examples that associate observations/features/attribute with one or more properties that we wish to learn/predict.", "video": ["Introduction to Machine Learning | Part 1", "Introduction to Machine Learning | Part 2 ", "Introduction to Machine Learning | Part 3", "Machine Learning - Practice Exercise"], "title": "Machine Learning"}, {"description": "This module discusses how Bioinformatics pipelines can be compared and evaluated.", "video": ["Benchmarking | Part 1", "Benchmarking | Part 2", "Benchmarking - Practice Exercise"], "title": "Benchmarking"}, {"description": "This module provides programming examples on how to get started with creating interactive web-based data visualization elements/figures.", "video": ["Interactive Data Visualization with E-Charts", "Visualizing Data using Interactive Clustergrams Built with D3.js | Part 1", "Visualizing Data using Interactive Clustergrams Built with D3.js | Part 2", "Visualizing Data using Interactive Clustergrams Built with D3.js | Part 3", "Visualizing Gene Expression Data using Interactive Clustergrams Built with D3.js - Practice Exercise"], "title": "Interactive Data Visualization"}, {"description": "This final module describes opportunities to work on LINCS related projects that go beyond the course.", "video": ["Microtasks and GEO2Enrichr Demo", "L1000-2-P100 Megatask Challenge", "BD2K-LINCS DCIC Crowdsourcing Portal"], "title": "Crowdsourcing Projects"}, {"description": "The Final Exam consists of 60 multiple choice questions which covers all of the modules of the course. Some of the questions may require you to perform some analysis with the methods you learned throughout the course on new datasets. ", "video": ["Final Exam"], "title": "Final Exam"}], "title": "Big Data Science with the BD2K-LINCS Data Coordination and Integration Center"}, {"course_info": "About this course: The purpose of this course is to summarize new directions in Chinese history and social science produced by the creation and analysis of big historical datasets based on newly opened Chinese archival holdings, and to organize this knowledge in a framework that encourages learning about China in comparative perspective.\n\nOur course demonstrates how a new scholarship of discovery is redefining what is singular about modern China and modern Chinese history. Current understandings of human history and social theory are based largely on Western experience or on non-Western experience seen through a Western lens. This course offers alternative perspectives derived from Chinese experience over the last three centuries. We present specific case studies of this new scholarship of discovery divided into two stand-alone parts, which means that students can take any part without prior or subsequent attendance of the other part.\n\nPart 1 (https://www.coursera.org/learn/understanding-china-history-part-1) focuses on comparative inequality and opportunity and addresses two related questions ‘Who rises to the top?’ and ‘Who gets what?’. \n\nPart 2 (this course) turns to an arguably even more important question ‘Who are we?’ as seen through the framework of comparative population behavior - mortality, marriage, and reproduction – and their interaction with economic conditions and human values. We do so because mortality and reproduction are fundamental and universal, because they differ historically just as radically between China and the West as patterns of inequality and opportunity, and because these differences demonstrate the mutability of human behavior and values.\n\nCourse Overview video: https://youtu.be/dzUPRyJ4ETk", "level": null, "package_name": null, "created_by": "The Hong Kong University of Science and Technology", "package_num": null, "teach_by": [{"name": "James Z. Lee", "department": null}], "target_audience": "Who is this class for: Anyone interested in understanding China through its empirical data may join.", "rating": "4.6", "week_data": [{"description": "Before you start with the content for Module 1, please review the Assignments and Grading page and introduce yourself to other learners who will be studying this course with you.", "video": ["Assignments and Grading", "Meet and Greet", "1.1 Who Are We? An Introduction", "1.2: Big Data and the Scholarship of Discovery", "1.3: Big Data, New Facts and Classic Social Theory", "1.4: New Data and Eurasian Comparisons", "2.1: Who Survives: Life Under Pressure", "2.2: Mortality: Geographic and Socioeconomic Comparisons", "2.3: Mortality and Who We Are", "Module 1 Suggested Reading", "Quiz 1"], "title": "Orientation and Module 1: Who Are We and Who Survives"}, {"description": "", "video": ["3.1: Who Reproduces: Prudence and Pressure", "3.2: Reproduction and Conscious Choice", "3.3: Reproduction and Adoption", "3.4: Reproduction: Geographic and Socioeconomic Comparisons", "4.1: Who Marries: Similarity in Difference", "4.2: Universal Female and Restricted Male Marriage", "4.3: Alternative Marriage Forms", "4.4: Marriage and Socioeconomic Comparisons", "Module 2 Suggested Reading", "Quiz 2"], "title": "Module 2: Who Reproduces and Who Marries"}, {"description": "In this module, James and his post-graduate student Hao DONG will co-deliver the lectures.", "video": ["5.1: Who Cares: State, Kinship and Family", "5.2: Effects of Living with Kin (by Hao DONG)", "5.3: Family System in Comparative Perspective (by Hao DONG)", "5.4: Kin Influence Across East Asian Family Systems (by Hao DONG)", "6.1: Conclusion: The Salient Legacy of China’s Past", "Module 3 Suggested Reading", "Quiz 3"], "title": "Module 3: Who Cares and Course Conclusion"}, {"description": "Now is time to test your understanding on the entire course. Take the final exam and complete the post-course survey. Your valuable feedback will certainly help us improve future iterations of the course.", "video": ["A Farewell Message from Professor James Lee", "Post-course Survey", "Final Exam"], "title": "Final Exam and Farewell"}], "title": "Understanding China, 1700-2000: A Data Analytic Approach, Part 2"}, {"course_info": "About this course: This course seeks to turn learners into informed consumers of social science research. It introduces concepts, standards, and principles of social science research to the interested non-expert. Learners who complete the course will be able to assess evidence and critically evaluate claims about important social phenomena. It reviews the origins and development of social science, describes the process of discovery in contemporary social science research, and explains how contemporary social science differs from apparently related fields. It describes the goals, basic paradigms, and methodologies of the major social science disciplines. It offers an overview of the major questions that are the focus of much contemporary social science research, overall and for China. Special emphasis is given to explaining the challenges that social scientists face in drawing conclusions about cause and effect from their studies, and offers an overview of the approaches that are used to overcome these challenges. Explanation is non-technical and does not involve mathematics. Statistics and quantitative methods are not covered. \n\nExplore the big questions in social science and learn how you can be a critical, informed consumer of social science research. \n\nCourse Overview video: https://youtu.be/QuMOAlwhpvU\n\nAfter you complete Part 1, enroll in Part 2 to learn how to be a PRODUCER of Social science research. \nPart 2:  https://www.coursera.org/learn/social-science-research-chinese-society", "level": "Beginner", "package_name": null, "created_by": "The Hong Kong University of Science and Technology", "package_num": null, "teach_by": [{"name": "Cameron Campbell", "department": "Associate Dean of the School of Humanities and Social Science"}], "target_audience": "Who is this class for: Advanced undergraduates or college graduates without a background in social science who seek a basic understanding of the relevant disciplines’ concerns, methodologies, and challenges. It is especially aimed at anyone contemplating applying to a social science postgraduate program who has not had systematic prior exposure, and would like to assess the fit of social science to their interests, and gain a basic understanding that will help them decide whether to pursue advanced training.  The course may also be useful to students enrolled in an introductory social science research methods class as a complement to lectures and readings. ", "rating": "4.3", "week_data": [{"description": "Welcome to Social Science Approaches to the Study of Chinese Society Part 1! Part 1 focuses on being a CONSUMER of social science research.  Take some time to review the Course Overview video and the assignments for this course. In Week 1, we will explore What is Social Science.  By the end of this week, you will be able to understand the differences between social science from humanities, natural and life sciences, outline the origins of social science and have a grasp of key definitions and terms.", "video": ["Course Overview", "Assignments & Grading", "Meet and Greet your peers!", "Suggested Reading List (Optional)", "1.1 Overview", "1.2 Why social science is a science", "1.3 Theory and evidence in social science", "1.4 The origins of social science", "1.5 Social science as a new way to study society", "1.6 Differences between social science and other disciplines", "1.7 Summary", "Interest in social science", "Week 1 General Reading List", "Week 1 Classics Reading List", "Week 1 - What is social science?"], "title": "What is social science?"}, {"description": "In Week 2, we will focus on The Big Questions. By the end of this week, you should have some sense of the range of questions that are the focus of much social science research. The next week, Week 3, will expand on China specific research.", "video": ["2.1 Overview", "2.2 Inequality", "2.3 Family", "2.4 Social context and individual outcomes", "2.5 Divergence", "2.6 Political and social change", "2.7 Summary", "The Big Questions", "Week 2 The Big Questions Reading", "Week 2 - The Big Questions"], "title": "The Big Questions"}, {"description": "In Week 3, we will focus on Social Science Research on China. By the end of this week, you should have some sense of major topics in current social science research on China.", "video": ["3.1 Overview", "3.2 Change and continuity", "3.3 Family", "3.4 Population aging", "3.5 Migration", "3.6 Education, Health, and Wellbeing", "3.7 Summary", "Notable changes in (Chinese) society", "Week 3 Social Science Research on China Resources", "Week 3 - Social science research on China"], "title": "Social Science Research on China"}, {"description": "In Week 4, we will focus on The Social Science Disciplines. By the end of this week, you should have a better understanding of the emphases of each of the major social science disciplines, and the differences between the disciplines.", "video": ["4.1 Overview", "4.2 Sociology", "4.3 Political Science", "4.4 Economics", "4.5 Anthropology", "4.6 Other disciplines", "4.7 Summary", "Social science disciplines", "Week 4 Social Science Disciplines Reading", "Week 4 - Social science disciplines"], "title": "The Social Science Disciplines"}, {"description": "In Week 5, we will focus on Study Designs. By the end of this week, you should understand the differences between the most common types of study, and have some sense of the settings in which each is most relevant.", "video": ["5.1 Overview", "5.2 What is a study?", "5.3 Experimental and observational studies", "5.4 Cross-sectional studies", "5.4 Longitudinal studies", "5.6 Qualitative and quantitative studies", "5.7 Macro and micro studies", "5.8 Summary", "Study Designs", "Week 5 Study Designs Reading", "Week 5 Study Designs"], "title": "Study Designs"}, {"description": "In Week 6, we will focus on Challenges. By the end of this week, you should be able to have a better understanding of key challenges to interpreting results from social science research, and be able to reflect on potential problems with study design.", "video": ["6.1 Overview", "6.2 Representativeness", "6.3 Selection bias", "6.4 Omitted variables", "6.5 Reverse causality", "6.6 The ecological fallacy", "6.7 Validity of measures", "6.8 Summary", "Challenges in social science research", "Week 6 Causation vs Correlation", "Week 6 - Challenges"], "title": "Challenges"}, {"description": "In Week 7, we will focus on Cause and Effect. By the end of this week, you should understand the basic approaches that social scientists follow in trying to establish that an observed relationship reflects cause and effect.", "video": ["7.1 Overview", "7.2 Experimental designs", "7.3 Cause and effect in observational studies", "7.4 Control variables", "7.5 Natural/quasi experiments", "7.6 Instrumental variables", "7.7 Matching approaches", "7.8 Summary", "Cause and effect", "Week 7 Measuring Causal Relationships", "Week 7 - Cause and effect"], "title": "Cause and Effect"}, {"description": "You've reached the final exam week!  Complete the final exam and post-course survey. Your feedback can help us improve future iterations of the course.  \n\nGood luck on the exam and hope to see you in Social Science Approaches Part 2. https://www.coursera.org/learn/social-science-research-chinese-society", "video": ["Concluding Remarks", "Post-course survey", "Final Exam"], "title": "Final Exam"}], "title": "Social Science Approaches to the Study of Chinese Society Part 1"}, {"course_info": "About this course: In this project-based course, you will follow your own interests to create a portfolio worthy single-frame viz or multi-frame data story that will be shared on Tableau Public. You will use all the skills taught in this Specialization to complete this project step-by-step, with guidance from your instructors along the way. You will first create a project proposal to identify your goals for the project, including the question you wish to answer or explore with data. You will then find data that will provide the information you are seeking. You will then import that data into Tableau and  prepare it for analysis. Next you will create a dashboard that will allow you to explore the data in depth and identify meaningful insights. You will then give structure to your data story by writing the story arc in narrative form. Finally, you will consult your design checklist to craft the final viz or data story in Tableau. This is your opportunity to show the world what you’re capable of - so think big, and have confidence in your skills!", "level": "Intermediate", "package_name": "Data Visualization with Tableau Specialization ", "created_by": "University of California, Davis", "package_num": "5", "teach_by": [{"name": "Suk S. Brar, M.B.A.", "department": "Blue Shield of California"}, {"name": "Hunter Whitney", "department": "Design Strategy and Data Visualization"}], "target_audience": "Who is this class for: This course is primarily aimed at individuals with some fundamental data visualization knowledge, are familiar with Tableau, and want to increase their knowledge in explanatory analysis. This course is intended for the beginner data visualization person, but who is comfortable working with data and datasets.", "rating": "4.5", "week_data": [{"description": "In this first milestone, you will write a project proposal that will capture the “who, what, why and how” of your project plus any challenges that you foresee along the way. Your proposal will include: a specific business case or personal objective, any intended outcomes, a description of the needs of the intended audience, a description of the dataset to be used, and any foreseeable challenges.", "video": ["Project Welcome", "Save Workbooks to Tableau Public", "Need Another Tableau License?", "Your Learning Goals", "Milestone 1 Assignment Overview", "Questions For Your Peers", "Design Checklist Template", "Sample Design Checklist", "Resources on the Who, What, Why, and How", "Resources on Audience", "Examples of Single Frame Visualizations", "Examples of Multi-Frame Data Stories", "Developing a Project Proposal"], "title": "Getting Started and Milestone 1: Develop a Project Proposal"}, {"description": "In milestone two, you will acquire the dataset that supports your project proposal, import it into Tableau, and prepare the data for analysis.", "video": ["Milestone 2 Assignment Overview", "Questions For Your Peers", "Preparing Your Data for Import", "Primary Types of Connections", "Connecting and Merging Multiple Data Sources", "Data Import and Preparation"], "title": "Milestone 2: Importing and Prepping the Data"}, {"description": "In this milestone, you will use the skills that you have learned in the specialization to perform exploratory analysis of your data. You will identify key metrics in the data and create KPIs, and you will use those KPIs to create dashboards that allow for comparative views and “brushing and linking.” This will allow us to begin to think about the proper context of developing an explanatory analysis that will form the basis for the remaining milestones.  Be sure your visualizations demonstrate the visual and cognitive design principles learned throughout the Specialization, and make use of advanced features like hierarchies, actions, filters and parameters. ", "video": ["Milestone 3 Assignment Overview", "Questions For Your Peers", "What is Exploratory and Explanatory Analysis?", "Case Study: Anscombe’s Quartet", "Identifying Outliers", "Constructing a Control Chart", "Tableau Whitepaper", "Conducting Visual Analytics"], "title": "Milestone 3: Exploratory Analysis"}, {"description": "In this module, you will continue to work through Milestone 3, your exploratory analysis and dashboard creation as outlined in the third week. You will then submit your deliverables for peer review.", "video": ["Questions For Your Peers", "Key Metrics, Indicators, Decision Triggers", "Dashboard and Storytelling with Data", "Exploratory Analysis and Dashboards"], "title": "Milestone 3: Exploratory Analysis and Dashboard Submission"}, {"description": "In this milestone, you will take a short but essential break from the data visualization software and begin to give structure to your data stories. You will define the basic story arc of your data story, or draft a narrative description of what your data visualization communicates.  You have started that process in the previous milestones, but now we will start assembling our story using Story Points.", "video": ["Milestone 4 Assignment Overview", "Questions For Your Peers", "Finding the Story", "Prioritizing, Optimizing and Designing the Data Story", "Tell the Story of Your Data", "Storyboarding and Storytelling"], "title": "Milestone 4: Storytelling and Storyboarding"}, {"description": "In this final milestone, you will apply design elements from our design checklist. You have met the goals of the stakeholder’s and will complete your design to meet the needs of the audience. You will apply the cognitive and visual design concepts learned throughout this Specialization and create a visualization or data story that leaves a lasting impression with your audience.", "video": ["Milestone 5 Assignment Overview", "Questions For Your Peers", "Project Summary", "Reflections", "Final Presentation"], "title": "Milestone 5: Final Presentation"}], "title": "Data Visualization with Tableau Project"}, {"course_info": "About this course: Welcome to Data Analytics Foundations for Accountancy I! You’re joining thousands of learners currently enrolled in the course. I'm excited to have you in the class and look forward to your contributions to the learning community.\n\nTo begin, I recommend taking a few minutes to explore the course site. Review the material we’ll cover each week, and preview the assignments you’ll need to complete to pass the course. Click Discussions to see forums where you can discuss the course material with fellow students taking the class.\n\nIf you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center.\n\nGood luck as you get started, and I hope you enjoy the course!", "level": null, "package_name": null, "created_by": "University of Illinois at Urbana-Champaign", "package_num": null, "teach_by": [{"name": "Robert Brunner", "department": "Accountancy"}], "target_audience": null, "rating": null, "week_data": [{"description": "You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course.", "video": ["Welcome to Data Analytics Foundations for Accountancy I", "Meet Professor Brunner", "Syllabus", "About the Discussion Forums", "Orientation Quiz", "Updating Your Profile", "Getting to Know Your Classmates", "Social Media"], "title": "Course Orientation"}, {"description": "This module serves as the introduction to the course content and the course Jupyter server, where you will run your analytics scripts. First, you will read about specific examples of how analytics is being employed by Accounting firms. Next, you will learn about the capabilities of the course Jupyter server, and how to create, edit, and run notebooks on the course server. After this, you will learn how to write Markdown formatted documents, which is an easy way to quickly write formatted text, including descriptive text inside a course notebook. Finally, you will begin learning about Python, the programming language used in this course for data analytics.", "video": ["Module 1 Overview", "Introduction to Module 1", "The Importance of Data Analytics in Modern Accountancy", "Lesson 1-1 Readings", "Introduction to the Course JupyterHub Server", "Introduction to Jupyter Notebook", "Introduction to Markdown", "Introduction to Markdown Notebook", "Introduction to Python", "Introduction to Python Notebook", "Module 1 Programming Assignment Notebook", "Module 1 Graded Quiz", "Module 1 Programming Assignment"], "title": "Module 1: Foundations"}, {"description": "This module focuses on the basic features in the Python programming language that underlie most data analytics scripts. First, you will read about why accounting students should learn to write computer programs. Second, you will learn about basic data structures commonly used in Python programs. Third, you will learn how to write functions, which can be repeatedly called, in Python, and how to use them effectively in your own programs. Finally, you will learn how to control the execution process of your Python program by using conditional statements and looping constructs. At the conclusion of this module, you will be able to write Python scripts to perform basic data analytic tasks.", "video": ["Module 2 Overview", "Introduction to Module 2", "Why Accounting Students Should Learn to Code", "Lesson 2-1 Readings", "Python Data Structures", "Python Data Structures Notebook", "Introduction to Python Functions", "Python Functions Notebook", "Python Programming Concepts", "Python Programming Concepts", "Module 2 Programming Assignment Notebook", "Module 2 Graded Quiz", "Module 2 Programming Assignment"], "title": "Module 2: Introduction to Python"}, {"description": "This module introduces fundamental concepts in data analysis. First, you will read a report from the Association of Accountants and Financial Professionals in Business that explores Big Data in Accountancy. Next, you will learn about the Unix file system, which is the operating system used for most big data processing (as well as Linux and Mac OSX desktops and many mobile phones). Second, you will learn how to read and write data to a file from within a Python program. Finally, you will learn about the Pandas Python module that can simplify many challenging data analysis tasks, and includes the DataFrame, which programmatically mimics many of the features of a traditional spreadsheet.", "video": ["Module 3 Overview", "Introduction to Module 3", "Why Use Python Instead of Excel?", "Lesson 3-1 Readings", "Introduction to Unix", "Introduction to Unix Notebook", "Python File I/O", "Python File I/O", "Introduction to Pandas", "Introduction to Pandas Notebook", "Module 3 Programming Assignment Notebook", "Module 3 Graded Quiz", "Module 3 Programming Assignment"], "title": "Module 3: Introduction to Data Analysis"}, {"description": "This module introduces fundamental concepts in data analysis. First, you will read about how to perform many basic tasks in Excel by using the Pandas module in Python. Second, you will learn about the Numpy module, which provides support for fast numerical operations within Python. This module will focus on using Numpy with one-dimensional data (i.e., vectors or 1-D arrays), but a later module will explore using Numpy for higher-dimensional data. Third, you will learn about descriptive statistics, which can be used to characterize a data set by using a few specific measurements. Finally, you will learn about advanced functionality within the Pandas module including masking, grouping, stacking, and pivot tables.", "video": ["Module 4 Overview", "Introduction to Module 4", "How the Pandas Module Can Support Standard Business Analytics", "Lesson 4-1 Readings", "Introduction to Numpy", "Introduction to Numpy Notebook", "Introduction to Descriptive Statistics", "Introduction to Descriptive Statistics Notebook", "Advanced Pandas", "Advanced Pandas Notebook", "Module 4 Programming Assignment Notebook", "Module 4 Graded Quiz", "Module 4 Programming Assignment"], "title": "Module 4: Statistical Data Analysis"}, {"description": "This module introduces visualization as an important tool for exploring and understanding data. First, the basic components of visualizations are introduced with an emphasis on how they can be used to convey information. Also, you will learn how to identify and avoid ways that a visualization can mislead or confuse a viewer. Next, you will learn more about conveying information to a user visually, including the use of form, color, and location. Third, you will learn how to actually create a simple visualization (basic line plot) in Python, which will introduce creating and displaying a visualization within a notebook, how to annotate a plot, and how to improve the visual aesthetics of a plot by using the Seaborn module. Finally, you will learn how to explore a one-dimensional data set by using rug plots, box plots, and histograms.", "video": ["Module 5 Overview", "Introduction to Module 5", "Creating Clear and Powerful Visualizations", "Lesson 5-1 Readings and Resources", "Visualization of Quantitative Data", "Lesson 5-2 Readings and Resources", "Introduction to Plotting", "Introduction to Plotting Notebook", "Introduction to Data Visualization", "Lesson 5-4 Reading", "Introduction to Data Visualization Notebook", "Module 5 Programming Assignment Notebook", "Module 5 Graded Quiz", "Module 5 Programming Assignment"], "title": "Module 5: Introduction to Visualization"}, {"description": "In this Module, you will learn the basics of probability, and how it relates to statistical data analysis. First, you will learn about the basic concepts of probability, including random variables, the calculation of simple probabilities, and several theoretical distributions that commonly occur in discussions of probability. Next, you will learn about conditional probability and Bayes theorem. Third, you will learn to calculate probabilities and to apply Bayes theorem directly by using Python. Finally, you will learn to work with both empirical and theoretical distributions in Python, and how to model an empirical data set by using a theoretical distribution.", "video": ["Module 6 Overview", "Introduction to Module 6", "Introduction to Probability", "Lesson 6-1 Readings", "Introduction to Bayes Theorem", "Lesson 6-2 Readings", "Calculating Probabilities in Python", "Lesson 6-3 Readings", "Introduction to Probability Notebook", "Introduction to Distributions", "Lesson 6-4 Readings", "Introduction to Distributions Notebook", "Module 6 Programming Assignment Notebook", "Module 6 Graded Quiz", "Module 6 Programming Assignment"], "title": "Module 6: Introduction to Probability"}, {"description": "This modules extends what you have learned in previous modules to the visual and analytic exploration of two-dimensional data. First, you will learn how to make two-dimensional scatter plots in Python and how they can be used to graphically identify a correlation and outlier points. Second, you will learn how to work with two-dimensional data by using the Numpy module, including a discussion on analytically quantifying correlations in data. Third, you will read about statistical issues that can impact understanding multi-dimensional data, which will allow you to avoid them in the future. Finally, you will learn about ordinary linear regression and how this technique can be used to model the relationship between two variables.", "video": ["Module 7 Overview", "Introduction to Module 7", "Introduction to Scatter Plots", "Python Two-Dimensional Plotting Notebook", "Introduction to Numpy Matrices", "Advanced Numpy Notebook", "Statistical Issues When Exploring Multi-Dimensional Data", "Lesson 7-3 Readings and Resources", "Introduction to Ordinary Linear Regression", "Lesson 7-4 Readings", "Introduction to Ordinary Linear Regression Notebook", "Module 7 Programming Assignment Notebook", "Module 7 Graded Quiz", "Module 7 Programming Assignment"], "title": "Module 7: Exploring Two-Dimensional Data"}, {"description": "Often, as part of exploratory data analysis, a histogram is used to understand how data are distributed, and in fact this technique can be used to compute a probability mass function (or PMF) from a data set as was shown in an earlier module. However, the binning approach has issues, including a dependance on the number and width of the bins used to compute the histogram. One approach to overcome these issues is to fit a function to the binned data, which is known as parametric estimation. Alternatively, we can construct an approximation to the data by employing a non-parametric density estimation. The most commonly used non-parametric technique is kernel density estimation (or KDE). In this module, you will learn about density estimation and specifically how to employ KDE. One often overlooked aspect of density estimation is the model representation that is generated for the data, which can be used to emulate new data. This concept is demonstrated by applying density estimation to images of handwritten digits, and sampling from the resulting model.", "video": ["Module 8 Overview", "Introduction to Module 8", "Why Do Accounting Students Need Data Analytics Skills?", "Lesson 8-1 Readings", "Introduction to Density Estimation", "Introduction to Density Estimation Notebook", "Advanced Density Estimation", "Advanced Density Estimation Notebook", "Module 8 Programming Assignment Notebook", "Module 8 Graded Quiz", "Module 8 Programming Assignment"], "title": "Module 8: Introduction to Density Estimation"}], "title": "Data Analytics Foundations for Accountancy I"}, {"course_info": "About this course: The capstone project will be an analysis using R that answers a specific scientific/business question provided by the course team. A large and complex dataset will be provided to learners and the analysis will require the application of a variety of methods and techniques introduced in the previous courses, including exploratory data analysis through data visualization and numerical summaries, statistical inference, and modeling as well as interpretations of these results in the context of the data and the research question. The analysis will implement both frequentist and Bayesian techniques and discuss in context of the data how these two approaches are similar and different, and what these differences mean for conclusions that can be drawn from the data.\n \nA sampling of the final projects will be featured on the Duke Statistical Science department website.\n\nNote: Only learners who have passed the four previous courses in the specialization are eligible to take the Capstone.", "level": null, "package_name": "Statistics with R Specialization ", "created_by": "Duke University", "package_num": "5", "teach_by": [{"name": "Merlise A Clyde", "department": "Department of Statistical Science"}, {"name": "Colin Rundel ", "department": "Statistical Science"}, {"name": "David Banks", "department": "Statistical Science"}, {"name": "Mine Çetinkaya-Rundel", "department": "Department of Statistical Science"}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "Welcome to the capstone project! This week's content is an introduction to the project assignment and goals. The readings in this week will introduce the data set that you will be analyzing for your project and the specific questions you will answer using data analysis techniques we learned in the previous courses. It is important to understand what we will be doing in the course before jumping into the detailed analysis. So we encourage you to start with the first lecture to get the big picture, and then delve into the specifics of the analysis. Enjoy, and good luck! Remember, if you have questions, you can post them on the discussion forums.", "video": ["Welcome to the Statistics with R Capstone course", "Introduction to the Capstone Course", "Tips for Success and Suggested Work Pace", "What to Do This Week", "Feedback surveys", "Learning Objectives for Courses 1-4"], "title": "About the Capstone Project"}, {"description": "This week you will work on conducting an exploratory analysis of the housing data. Exploratory analysis is an essential first step for familiarizing yourself with and understanding the data. \n\nIn this week, you will complete a quiz which will guide you through certain important aspects of the data. The insights you gain through this assignment will help inform modeling in the future quizzes and peer assessments. \n\nFeel free to post questions about this assignment on the discussion forum. ", "video": ["What to Do This Week", "EDA Quiz - Assignment Guide", "Feedback Survey", "EDA Quiz"], "title": "Exploratory Data Analysis (EDA)"}, {"description": "This week we will dig deeper into our exploratory data analysis of the data. We now have all the information and data necessary to perform a deep dive into the EDA and it is time start your initial analysis report! We encourage you to start your analysis report (presented in peer-review format next week) early so you will have enough time to complete it. You will conduct exploratory data analysis, model selection, and model evaluation, and then complete a written report which answers several questions which will guide you through the process. This report will be your first peer-review assignment in this course. ", "video": ["What to Do This Week"], "title": "EDA and Basic Model Selection - Submission"}, {"description": "Great work so far! We hope you will also learn as much from evaluating your peers' work as completing your own assignment. Happy learning!", "video": ["What to Do This Week", "Feedback Survey", "EDA and Basic Model Selection"], "title": "EDA and Basic Model Selection - Evaluation"}, {"description": "We are half way through the course! In this week, you will continue model selection and model diagnostics, which will serve a starting point for your final project. You will be assessed on your work through a quiz. If you have any questions so far, don't hesitate to post on the forum so that others can help and discuss the question together.", "video": ["What to Do This Week", "Model Selection and Diagnostics Quiz - Assignment Guide", "Feedback Survey", "Model Selection and Diagnostics Quiz"], "title": "Model Selection and Diagnostics"}, {"description": "In this week, you will gain experience using your model to perform out-of-sample prediction and validation.  The skills honed this week will guide you through your final analysis in the weeks to come.  Please feel free to go back to prior weeks and review the necessary background knowledge. ", "video": ["What do Do This Week", "Out of Sample Prediction Quiz - Assignment Guide", "Feedback Survey", "Out of Sample Prediction Quiz"], "title": "Out of Sample Prediction"}, {"description": "In the next two weeks, you will complete your final data analysis project. You will submit your answers using the Final Data Analysis peer review assignment link in Week 8.", "video": ["What to Do This Week"], "title": "Final Data Analysis - Submission"}, {"description": "Congratulations on making through to the final week of the course! In this week, we will finish this data analysis project by completing the evaluation of three of your peers' assignments. ", "video": ["What to Do This Week", "Feedback Survey", "Final Data Analysis "], "title": "Final Data Analysis - Evaluation"}], "title": "Statistics with R Capstone"}, {"course_info": "About this course: In this final course you will complete a Capstone Project using data analysis to recommend a method for improving profits for your company, Watershed Property Management, Inc. Watershed is responsible for managing thousands of residential rental properties throughout the United States. Your job is to persuade Watershed’s management team to pursue a new strategy for managing its properties that will increase their profits. To do this, you will: (1) Elicit information about important variables relevant to your analysis; (2) Draw upon your new MySQL database skills to extract relevant data from a real estate database; (3) Implement data analysis in Excel to identify the best opportunities for Watershed to increase revenue and maximize profits, while managing any new risks; (4) Create a Tableau dashboard to show Watershed executives the results of a sensitivity analysis; and (5) Articulate a significant and innovative business process change for Watershed based on your data analysis, that you will recommend to company executives. \n\nAirbnb, our Capstone’s official Sponsor, provided input on the project design. The top 10 Capstone completers each year will have the opportunity to present their work directly to senior data scientists at Airbnb live for feedback and discussion.\n\n\"Note: Only learners who have passed the four previous courses in the specialization are eligible to take the Capstone.\"", "level": null, "package_name": "Excel to MySQL: Analytic Techniques for Business Specialization ", "created_by": "Duke University", "package_num": "5", "teach_by": [{"name": "Daniel Egger", "department": "Pratt School of Engineering, Duke University"}, {"name": "Jana Schaich Borg", "department": "Social Science Research Institute"}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "The goal for this week is to learn about the Capstone Project you are tasked with, acquire background about the business problem, and begin to outline the steps of your analysis.", "video": ["Introduction to the Capstone Project", "Course Overview", "Special Thanks!", "About the Course Team", "FAQ (Frequently Asked Questions)", "Feedback Survey Information", "Lesson Overview", "Elicitation Refresher", "Letter from Your Project Manager", "What are Project Managers, Anyway?", "What Watershed Owners Care About", "Background about the Short-term Rental Industry", "Lesson Overview", "Elicitation Interview with Your Project Manager", "Elicitation Interview with Watershed's Marketing Director", "Elicitation Interview with Watershed's Financial Director", "Outlining an SPAP", "Requirements and Assumptions", "Feedback Survey", "Your Three Elicitation Interviews", "Elicitation"], "title": "Introduction"}, {"description": "The goal of this week is for you to extract the relevant data from the MySQL database you are given access to, and to look at it briefly in Tableau to get sense of what data you have.  ", "video": ["Meet Your Data", "How to Meet and Retrieve Your Data", "How to Meet and Retrieve Your Data (Jupyter notebook)", "Visualize Your Data to Make Sure You Know What They Are", "Feedback Survey", "Verify You Have Extracted the Correct Data", "Make Sure You Understand What Your Data Mean"], "title": "Data Extraction and Visualization"}, {"description": "The goal of this week is for you to create a financial model using Excel to analyze the data you extracted from the database, and to start to predict short-term rents for some of Watershed's existing properties. ", "video": ["Week 3 Learning Goals", "Single Workbook Containing Template Spreadsheets 1-3", "Single Workbook Containing Guide Spreadsheets", "Creating a Predictive Model for Short-term Rental Rates", "Best Practices for Setting up an Excel Spreadsheet", "Using the First Best-Fit Line Template Spreadsheet", "First Best Fit Line Template (Spreadsheet 1)", "Normalizing Rents to Improve Occupancy Forecasting", "Using the Normalized Data and Model Template Spreadsheet", "Normalized Data and Model Template (Spreadsheet 2)", "Using the Dollars to Percentile Conversion Guide Spreadsheet", "Dollars to Percentile Conversion Guide Spreadsheet", "Applying Normalization to the Comparable Properties", "Lesson Overview", "Optimizing Rents to Maximize Revenues", "Using the Solver Revenue Maximization Guide Spreadsheet", "Solver Revenue Maximization Guide Spreadsheet", "Optimizing Watershed Rents", "Solver Rent Optimization Template (Spreadsheet 3)", "Alternative to Solver Template (Spreadsheet 4)", "Feedback Survey", "First Best-Fit Line", "Normalization", "Applying Normalization to the Comparable Properties", "Optimization Basics", "Optimizing Watershed Rents"], "title": "Modeling"}, {"description": "The goal for this week is for you to use your projections about the Watershed properties to estimate cash flows and profits Watershed would experience if it converted properties to short-term rentals. ", "video": ["Week 4 Learning Goals", "Single Workbook Containing Template Spreadsheets 5-6", "Estimating Watershed Cash Flow and Profits", "Using the Alternative to Solver Template Spreadsheet", "Distinguishing Cash Flow from Profits and Losses", "Using the Forecasting Cash Flow and Profits Template Spreadsheet", "Forecasting Cash Flow and Profits Template (Spreadsheet 5)", "Using the Annual Cash Flows and Profits Spreadsheet", "Annual Cash Flows and Profits Guide Spreadsheet", "Using the Sorting by Profitability Template Spreadsheet", "Sorting by Profitability Template (Spreadsheet 6)", "The Value of Considering Cash Flow Risk and Total Cash Required", "The Value of Financial Sensitivity Analysis in General", "Feedback Survey", "Alternative to Solver", "Profitability", "Cash Flow Risk and Total Cash Required", "Sensitivity Analysis: Measuring Cutoffs at 40% Transaction Fee"], "title": "Cash Flow and Profits"}, {"description": "The goal of this week and next week is to build an analytical dashboard in Tableau using the data models and assumptions you have discovered in prior weeks. ", "video": ["Using Tableau to Perform Sensitivity Analysis", "Dashboard for Analyst Use", "Dashboard Modification for a Financial Audience", "How to Get Started Making Your Dashboard", "Additional charts for your sensitivity analysis", "Bar in Bar Graphs in Tableau", "Histograms in Tableau", "Tables in Tableau", "Feedback Survey"], "title": "Data Dashboard"}, {"description": "This week complete your dashboard and add design elements so the dashboard is ready for stakeholders (Watershed executives, for example) to use it to test your model's assumptions. ", "video": ["Preparing Your Dashboard for Decision-makers", "Lesson Overview", "Jittered Maps in Tableau", "How to Use Jittering to Depict Multiple Data Points in the Same Geographic Location", "Turning Your Sensitivity Analysis into a Recommendation", "Finalizing Your Dashboard", "Tableau Tricks to Try on Your Own (Including R Integration!)", "Feedback Survey", "Sensitivity Analysis"], "title": "Dashboard for Decision-makers"}, {"description": "This week, design and give a presentation for Watershed executives with your business recommendations, and complete a white paper template. Evaluate 3 peer's dashboards, white papers and presentations. ", "video": ["Persuading Decision-makers to Follow Your Recommendations", "New Information from Your Project Manager!", "White Paper Background Information", "About the Final Project", "PART I: Tableau Dashboard Instructions", "PART 2: White Paper Instructions", "PART 3: Presentation Instructions", "Congratulations on Joining the Exciting Field of Data Analytics!", "Feedback Survey", "A Very Important Question!", "Final Project Assignment Submission"], "title": "Final Project"}], "title": "Increasing Real Estate Management Profits: Harnessing Data Analytics"}, {"course_info": "About this course: This course is intended as a first step for learners who seek to become producers of social science research. It is organized as an introduction to the design and execution of a research study. It introduces the key elements of a proposal for a research study, and explains the role of each. It reviews the major types of qualitative and quantitative data used in social science research, and then introduces some of the most important sources of existing data available freely or by application, worldwide and for China. The course offers an overview of basic principles in the design of surveys, including a brief introduction to sampling. Basic techniques for quantitative analysis are also introduced, along with a review of common challenges that arise in the interpretation of results. Professional and ethical issues that often arise in the conduct of research are also discussed.  The course concludes with an introduction to the options for further study available to the interested student, and an overview of the key steps involved in selecting postgraduate programs and applying for admission. Learners who complete the course will be able to make an informed decision about whether to pursue advanced studies, and should be adequately prepared to write an application for postgraduate study that exhibits basic understanding of key aspects of social science research paradigms and methodologies.\n\nExplore the big questions in social science and learn how you can be a producer of social science research. \n\nCourse Overview video: https://youtu.be/QuMOAlwhpvU\n\nPart 1 should be completed before taking this course:  https://www.coursera.org/learn/social-science-study-chinese-society", "level": "Beginner", "package_name": null, "created_by": "The Hong Kong University of Science and Technology", "package_num": null, "teach_by": [{"name": "Cameron Campbell", "department": "Associate Dean of the School of Humanities and Social Science"}], "target_audience": "Who is this class for: Advanced undergraduates or college graduates without a background in social science who seek a basic understanding of the relevant disciplines’ concerns, methodologies, and challenges. It is especially aimed at anyone interested in applying to a social science postgraduate program but whose original training was in a different discipline, and seeks an understanding that will help them select among programs and write an application that exhibits understanding of basic principles and paradigms. The course may also be useful to students enrolled in an introductory social science research methods class as a complement to lectures and readings. ", "rating": null, "week_data": [{"description": "Welcome to Social Science Approaches to the Study of Chinese Society Part 2! Part 2 focuses on being a PRODUCER of Social Science Research.  Take some time to review the course overview, assignments for this course and say hello in the discussion forum.  ", "video": ["Course overview", "Assignments and grading", "Suggested Reading List (Optional)", "Meet and greet!", "1.1 Designing a Study", "1.2 The Research Proposal", "1.3 Aims", "1.4 Impact and Significance", "1.5 Background-Literature Review", "1.6 Methodology", "1.7 Data", "1.8 Summary", "Interest in social science", "Week 1 General Reading List", "Week 1 Designing a study", "Week 1 Designing a study"], "title": "Designing a Study"}, {"description": "Week 2 will discuss the kind sources social scientists use for research. By the end of this week, you should be able to identify some of these major sources and perhaps pinpoint some sources that can be used in your own study.", "video": ["2.1 Evidence", "2.2 Survey data", "2.3 Public data", "2.4. Administrative and archival microdata", "2.5. Multi-generational microdata", "2.6 Aggregated data", "2.7 Sources of aggregated data", "2.8 Qualitative sources", "2.9 New sources", "2.10 Summary", "Evidence", "Week 2 Evidence", "Week 2 Evidence"], "title": "Evidence"}, {"description": "By the end of Week 3, you should be able to understand why RANDOM SAMPLING is important in a survey, outline the most common approaches to sampling and discuss key considerations when choosing a sampling strategy for your study.", "video": ["3.1. Overview: Sampling", "3.2. Surveys and sampling", "3.3. Probability sampling", "3.4. Clustered sampling", "3.5. Stratification, oversampling", "3.6. Respondent-driven sampling", "3.7(a) Conducting a survey - Preparation", "3.7(b) Conducting a survey - Designing a Questionnaire", "3.8. Response rates and follow-up", "3.9. Summary", "Sampling", "Week 3 Sampling", "Week 3 Sampling"], "title": "Sampling"}, {"description": "Week 4 discusses major sources of public data available to you.  By the end of this week you should be able to describe the opportunities as well as the challenges associated with using publicly available survey data. ", "video": ["4.1. Overview: Public data for China", "4.2. China General Social Survey", "4.3. China Health and Retirement Longitudinal Survey", "4.4. China Family Panel Studies", "4.5. China Health and Nutrition Survey", "4.6. The China Multigenerational Panel Datasets (CMGPD)", "4.7. Other major datasets", "4.8. Summary", "Public Data for China", "Week 4 Public Data for China", "Week 4 Public Data for China"], "title": "Public Data for China"}, {"description": "Week 5 will give you a taste of the basic methods for quantitative analysis.  From there you should be able to identify key issues when interpreting results and discuss implications for research.", "video": ["5.1 Overview: Quantitative Analysis", "5.2 Tabulations", "5.3a Correlation and regression I", "5.3b Correlation and regression II", "5.4 Regression to the mean", "5.5 Statistical significance", "5.6 Type I errors", "5.7 Type II errors", "5.8 Summary Quantitative Analysis", "Quantitative Analysis", "Week 5 Quantitative Analysis", "Week 5 Study Designs Quiz"], "title": "Quantitative Analysis"}, {"description": "By the end of this week you should be able to describe major ethical and professional concerns in social science research.", "video": ["6.1 Overview of research and professional ethics", "6.2 The protection of subjects", "6.3 Consent", "6.4 Confidentiality", "6.5 International research", "6.6 Research funding and publication", "6.7 Data sharing", "6.8 Mentor/student relationship", "6.9 Summary:  Research and professional ethics", "Research and professional ethics", "Week 6 Research and professional ethics", "Week 6 Research and professional ethics"], "title": "Research and Professional Ethics"}, {"description": "Welcome to the last week of Part 2!  By the end of this week you should be able to be aware of the options you have for further study in social science research and know the steps to move forward in the application process for advanced training.", "video": ["7.1 Overview - where to go from here", "7.2 Options for advanced study in social science", "7.3 Choosing a PG program", "7.4 Writing the personal statement", "7.5 Research proposals and writing samples", "7.6 Recommendation letters", "7.7 Summary - where to go from here", "Where to go from here", "Week 7 Where to go from here", "Week 7 Where to go from here"], "title": "Where to go from here"}, {"description": "You've reached the final exam week!  Complete the final exam and the post-course survey. Your feedback can help us improve the course.  Thank you for being a part of this course and good luck for your pursuit of advanced studies in social science research!", "video": ["Part 2: Concluding remarks", "Post-course survey", "Final Exam"], "title": "Final exam"}], "title": "Social Science Approaches to the Study of Chinese Society Part 2"}, {"course_info": "About this course: Note: You should complete all the other courses in this Specialization before beginning this course.\n\nThis six-week long Project course of the Data Mining Specialization will allow you to apply the learned algorithms and techniques for data mining from the previous courses in the Specialization, including Pattern Discovery, Clustering, Text Retrieval, Text Mining, and Visualization, to solve interesting real-world data mining challenges. Specifically, you will work on a restaurant review data set from Yelp and use all the knowledge and skills you’ve learned from the previous courses to mine this data set to discover interesting and useful knowledge. The design of the Project emphasizes: 1) simulating the workflow of a data miner in a real job setting; 2) integrating different mining techniques covered in multiple individual courses; 3) experimenting with different ways to solve a problem to deepen your understanding of techniques; and 4) allowing you to propose and explore your own ideas creatively. \n\nThe goal of the Project is to analyze and mine a large Yelp review data set to discover useful knowledge to help people make decisions in dining. The project will include the following outputs: \n\n1. Opinion visualization: explore and visualize the review content to understand what people have said in those reviews.\n2. Cuisine map construction: mine the data set to understand the landscape of different types of cuisines and their similarities.\n3. Discovery of popular dishes for a cuisine: mine the data set to discover the common/popular dishes of a particular cuisine.\n4. Recommendation of restaurants to help people decide where to dine: mine the data set to rank restaurants for a specific dish and predict the hygiene condition of a restaurant.\n\nFrom the perspective of users, a cuisine map can help them understand what cuisines are there and see the big picture of all kinds of cuisines and their relations. Once they decide what cuisine to try, they would be interested in knowing what the popular dishes of that cuisine are and decide what dishes to have. Finally, they will need to choose a restaurant. Thus, recommending restaurants based on a particular dish would be useful. Moreover, predicting the hygiene condition of a restaurant would also be helpful. \n\nBy working on these tasks, you will gain experience with a typical workflow in data mining that includes data preprocessing, data exploration, data analysis, improvement of analysis methods, and presentation of results. You will have an opportunity to combine multiple algorithms from different courses to complete a relatively complicated mining task and experiment with different ways to solve a problem to understand the best way to solve it. We will suggest specific approaches, but you are highly encouraged to explore your own ideas since open exploration is, by design, a goal of the Project. \n\nYou are required to submit a brief report for each of the tasks for peer grading. A final consolidated report is also required, which will be peer-graded.", "level": null, "package_name": "Data Mining  Specialization ", "created_by": "University of Illinois at Urbana-Champaign", "package_num": "6", "teach_by": [{"name": "Jiawei Han", "department": "Department of Computer Science"}, {"name": "ChengXiang Zhai", "department": "Department of Computer Science"}, {"name": "John C. Hart", "department": "Department of Computer Science"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "In this module, you will become familiar with the course, your instructor, your classmates, and our learning environment.", "video": ["Welcome to the Data Mining Project!", "Orientation Overview", "Syllabus", "About the Discussion Forums", "Updating Your Profile", "MeTA Installation and Overview", "Data Set and Toolkit Acquisition", "Getting to Know Your Classmates"], "title": "Orientation"}, {"description": "", "video": ["Task 1 Overview", "Task 1 Rubric", "Task 1 Submission"], "title": "Task 1 - Exploration of a Data Set"}, {"description": "", "video": ["Task 2 Overview", "Task 2 Rubric", "Task 2 Submission"], "title": "Task 2 - Cuisine Clustering and Map Construction"}, {"description": "", "video": ["Task 3 Overview", "Task 3 Rubric", "Task 3 Report Submission"], "title": "Task 3 - Dish Recognition"}, {"description": "", "video": ["Task 4 and 5 Overview", "Task 4 and 5 Rubric", "Task 4 and 5 Submission"], "title": "Task 4 & 5 - Popular Dishes and Restaurant Recommendation"}, {"description": "", "video": ["Task 6 Overview", "Task 6 Rubric", "Task 6 Report Submission"], "title": "Task 6"}, {"description": "", "video": ["Final Report Instructions", "Final Report Rubric", "Final Report Submission"], "title": "Final Report"}], "title": "Data Mining Project"}, {"course_info": "About this course: In the capstone, students will engage on a real world project requiring them to apply skills from the entire data science pipeline: preparing, organizing, and transforming data, constructing a model, and evaluating results.    Through a collaboration with Coursolve, each Capstone project is associated with partner stakeholders who have a vested interest in your results and are eager to deploy them in practice.  These projects will not be straightforward and the outcome is not prescribed -- you will need to tolerate ambiguity and negative results!  But we believe the experience will be rewarding and will better prepare you for data science projects in practice.", "level": null, "package_name": "Data Science at Scale Specialization ", "created_by": "University of Washington", "package_num": "4", "teach_by": [{"name": "Bill Howe", "department": "Scalable Data Analytics"}], "target_audience": null, "rating": "4.1", "week_data": [{"description": "In this project, you will build a model to predict when a building is likely to be condemned.  The data is real, the problem is real, and the impact is real.  ", "video": ["Get the Data", "Understand the Domain", "Milestone: Discuss the Problem and Approaches"], "title": "Project A: Blight Fight"}, {"description": "You are given sets of incidents with location information; you need to use some assumptions to group these incidents by location to identify specific buildings.", "video": ["Milestone: Create a list of \"buildings\" from a list of geo-located incidents", "Reflecting on defining \"buildings\""], "title": "Week 2: Derive a list of buildings"}, {"description": "Construct a training set by associating each of your buildings with a ground truth label derived from the permit data.", "video": ["Milestone: Derive labels for each building", "Reflecting on the labeling scheme"], "title": "Week 3: Construct a training dataset"}, {"description": "Use a trivial feature set to train and evaluate a simple model", "video": ["Milestone: Train a Simple Model", "Reflecting on a trivial initial model"], "title": "Week 4: Train and evaluate a simple model"}, {"description": "Derive additional features and retrain to improve the efficacy of your model.", "video": ["Milestone: Adding more features", "Reflection on your proposed features"], "title": "Week 5: Feature Engineering"}, {"description": "Enter your final report for grading.", "video": ["Final Report"], "title": "Week 6: Final Report"}], "title": "Data Science at Scale - Capstone Project"}, {"course_info": "About this course: The Business Analytics Capstone Project gives you the opportunity to apply what you've learned about how to make data-driven decisions to a real business challenge faced by global technology companies like Yahoo, Google, and Facebook. At the end of this Capstone, you'll be able to ask the right questions of the data, and know how to use data effectively to address business challenges of your own. You’ll understand how cutting-edge businesses use data to optimize marketing, maximize revenue, make operations efficient, and make hiring and management decisions so that you can apply these strategies to your own company or business. Designed with Yahoo to give you invaluable experience in evaluating and creating data-driven decisions, the Business Analytics Capstone Project provides the chance for you to devise a plan of action for optimizing data itself to provide key insights and analysis, and to describe the interaction between key financial and non-financial indicators. Once you complete your analysis, you'll be better prepared to make better data-driven business decisions of your own.", "level": "Intermediate", "package_name": "Business Analytics Specialization ", "created_by": "University of Pennsylvania", "package_num": "5", "teach_by": [{"name": "Wharton Teaching Staff", "department": "The Wharton School"}], "target_audience": null, "rating": "4.3", "week_data": [{"description": "The Business Analytics Specialization was designed to help you learn how to think about using data in making big (and small) business decisions. In this Capstone project, you'll be asked to create a strategy for a fictional digital search engine and content provider, GoYaFace, Inc. (often abbreviated as “GYF”). The strategy will be used in responding to the increasing popularity and availability of “adblocking” software, which could have significant negative repercussions for GYF’s business.  You are to assume the role of the leader of the Digital Advertising Tactics and Action (“DATA”) Team at GYF, which has been assigned the job of formulating GYF’s strategy in responding to the threat of adblocking.  Your task is to develop a strategy that will be recommended to GYF’s senior leadership.  Using what you've learned about business analytics, you'll (i) create a detailed problem statement focusing on GYF’s ad-buying customers (Module 2), (ii) develop a strategy (Module 3), (iii) describe the anticipated effects of the strategy (Module 4), and (iv) form a plan for measuring the effects of your strategy (Module 4).  You'll then put these four pieces together into a final project (Module 5).  First, please read the full description of the project in the “Project Description” link below, and then look at the background information about adblockers and the “GYF Company Profile” link in the content for Module 1. When you are ready to begin the first assignment, please move on to Module 2: Defining the Problem.", "video": ["Start Here! Project Description: Business Analytics Capstone", "GYF Company Profile", "The Importance of Mobile Advertising", "Further Exploration into Online Advertising Response Models", "How Internet Ads Work", "Project Template", "Introductions"], "title": " Module 1: Capstone Project Topic - The Problem of Adblocking"}, {"description": "In Module 2, you'll define the problem adblockers poses for GYF. GYF is intended to be a composite of leading internet platform and content providers who derive substantial revenues from mobile advertising like Google, Yahoo, and Facebook, so you should frame your research around the real-world problems these companies have faced and are facing. Defining the problem thoroughly will have a direct impact on how successful your strategy will be received by your peers. The more deeply you consider the effects of adblockers on the companies that buy advertising space from GYF, the more appropriate your overall strategy is likely to be. Please use the resources below to find out more about the problem, and then create your Problem Statement and submit it for peer review below. You can and should draw from all of the Business Analytics Specialization courses, but your Problem Statement should focus on how adblockers might adversely affect GYF’s relationship with the companies that pay GYF to place advertisements on GYF’s mobile applications and content. You should consider the issue of causality in your Problem Statement - we've included some lectures from the underlying courses to refresh you on that topci.  And you are strongly encouraged to complete and include a response to Application Exercise 1 (see link below) as part of your Problem Statement.", "video": ["Definition of the Adblocking Problem", "Whiting Out the Ads, but at What Cost?", "Apple's Support of Adblocking", "Why Adblockers Are Spurring a New Technology Arms Race", "Application Exercise 1 – Recommending Customer Analytics Research Methods to Explore Your Problem", "Approaching Problems with an Analytical Mindset", "What is Descriptive Analytics? (Customer Analytics)", "Descriptive Data Collection (Customer Analytics)", "Passive Data Collection (Customer Analytics)", "Beyond Period 2 (Customer Analytics)", "Causality 1 (People Analytics)", "Causality 2 (People Analytics)", "Reverse Causality (People Analytics)", "Causal Data Collection and Summary (Customer Analytics)", "Problem Statement"], "title": "Module 2:  Defining the Problem"}, {"description": "In Module 3, you will focus on creating your recommended strategy for GYF to address adblockers. Your strategy does not have to be lengthy, but it must be clear, and it must address the problem. (Hint: if you have a clearly defined problem, your strategy is much more likely to be clearly defined as well). You'll be submitting your strategy for peer review, and then also reviewing the work of at least 3 of your peers. It's OK if reviewing the strategies of other learners in this course gives you further ideas for revising your own strategy. One of the primary benefits of peer review is to expand the range of feedback you can get, and we designed this Module around peer review so that you can get as much feedback as possible before moving on to the next phase of the project.  You may find the resources and lectures below helpful in formulating your strategy and considering how data can be leveraged and appropriately understood.  You are strongly encouraged to complete and include your response to Application Exercise 2 as part of your Strategy.  ", "video": ["Does Your Strategy Need a Strategy?", "How Advertisers Can Beat Adblockers", "Application Exercise 2 - Using People Analytics Methods to Hire a Leader to Implement Your Strategy", "The Importance of Execution in Strategy", "Performance Evaluation: the Challenge of Noisy Data (People Analytics)", "Finding Persistence: Regression to the Mean (People Analytics)", "Extrapolating from Small Samples (People Analytics)", "The Wisdom of Crowds: Signal Independence (People Analytics)", "Process vs. Outcome (People Analytics)", "Hiring 1 (People Analytics)", "Hiring 2 (People Analytics)", "Strategy"], "title": "Module 3:  Your Strategy"}, {"description": "Module 4 was designed to give you the opportunity to focus on the effects of your strategy. Effects and Measurement can be often overlooked in strategy development; creating a thoughtful and thorough plan for measuring the effects will improve your final project tremendously. In this part of the project, you will describe two events: what you think will happen and how you will measure it. Look to the courses in the Business Analytics Specialization to see what kind of data companies use to measure effects to create a measurement plan of your own. You are strongly encouraged to complete and include your responses to Application Exercises 3 and 4 as part of your Effects and Measurement components.  You may create a scenario (Operations Analytics) to predict some of the intended effects of your strategy, either following the outline of Application Exercise 3, or of your own design.  Once you submit your own plan for effects and measurement, please review the work of at least three of your peers. You may find new ideas, or new ways of looking at data and measurement from this exercise.  We encourage you to incorporate what you've learned into your final submission!", "video": ["Resources for Thinking About Effects and Outcomes", "Application Exercise 3 - Using Operations Analytics Methods to Understand the Allocation of Scarce Resources in Pursuing a Strategy", "Application Exercise 3 Spreadsheet", "Application Exercise 4 - Using Accounting Analytics Methods to Measure the Key Drivers of Your Proposed Strategy", "Questions and Tips for Your Fellow Learners", "The Newsvendor Problem (Operations Analytics)", "How to Build an Optimization Model (Operations Analytics)", "Optimizing with Solver (Operations Analytics)", "Simulating Uncertain Outcomes in Excel (Operations Analytics)", "Decision Trees (Operations Analytics)", "Linking Non-financial Metrics to Financial Performance: Overview (Accounting Analytics)", "Steps to Linking Non-financial Metrics to Financial Performance (Accounting Analytics)", "Incorporating Analysis Results in Financial Models (Accounting Analytics)", "Effects and Measurement Metrics"], "title": "Module 4:  Effects of Your Strategy/Measuring these Effects"}, {"description": "In this final Module, you will combine the four revised elements of your presentation (Problem Statement, Strategy, Effects, and Measurement, including any responses to the Application Exercises you've completed) into one presentation and submit it for peer review. You'll then be asked to review the work of at least three of your peers. Once you have gotten feedback on your plan, you may use it as an example of strategic thinking at your current job, or as a work sample when you are applying for a new one. A successful strategic analysis which describes the use of data-driven decision making will make you much more marketable in almost any field. Good luck!", "video": ["Putting Analytics into Action", "Applications: ROI (Customer Analytics)", "Radically New Data Sets in Marketing (Customer Analytics)", "Analytics Applied: Kohl's, Netflix, AmEx and more (Customer Analytics)", "Final Project: Complete Strategic Analysis "], "title": "Module 5:  Final Project Submission"}], "title": "Business Analytics Capstone"}, {"course_info": "About this course: Welcome to the Capstone Project for Big Data! In this culminating project, you will build a big data ecosystem using tools and methods form the earlier courses in this specialization. You will analyze a data set simulating big data generated from a large number of users who are playing our imaginary game \"Catch the Pink Flamingo\". During the five week Capstone Project, you will walk through the typical big data science steps for acquiring, exploring, preparing, analyzing, and reporting. In the first two weeks, we will introduce you to the data set and guide you through some exploratory analysis using tools such as Splunk and Open Office. Then we will move into more challenging big data problems requiring the more advanced tools you have learned including KNIME, Spark's MLLib and Gephi. Finally, during the fifth and final week, we will show you how to bring it all together to create engaging and compelling reports and slide presentations. As a result of our collaboration with Splunk, a software company focus on analyzing machine-generated big data, learners with the top projects will be eligible to present to Splunk and meet Splunk recruiters and engineering leadership.", "level": null, "package_name": "Big Data Specialization ", "created_by": "University of California, San Diego", "package_num": "6", "teach_by": [{"name": "Ilkay Altintas", "department": "San Diego Supercomputer Center"}, {"name": "Amarnath Gupta", "department": "San Diego Supercomputer Center (SDSC)"}], "target_audience": null, "rating": "4.3", "week_data": [{"description": "This week we provide an overview of the Eglence, Inc. Pink Flamingo game, including various aspects of the data which the company has access to about the game and users and what we might be interested in finding out.", "video": ["Welcome to the Big Data Capstone Project", "Welcome from Splunk: Rob Reed World Education Evangelist", "Planning, Preparation, and Review", "A Game by Eglence Inc. : Catch The Pink Flamingo", "A Summary of Catch the Pink Flamingo", "A Conceptual Schema for Catch the Pink Flamingo", "Overview of the Catch the Pink Flamingo Data Model", "Overview of Final Project Design"], "title": "Simulating Big Data for an Online Game"}, {"description": "Next, we begin working with the simulated game data by exploring and preparing the data for ingestion into big data analytics applications.", "video": ["Downloading the Game Data and Associated Scripts", "Understanding the CSV Files Generated by the Scripts", "Optional Review of Splunk", "“Catch the Pink Flamingo” Data Exploration with Splunk", "Aggregate Calculations Using Splunk", "Filtering the Data With Splunk", "Data Exploration With Splunk", "Data Exploration Technical Appendix"], "title": "Acquiring, Exploring, and Preparing the Data"}, {"description": "This week we do some data classification using KNIME. ", "video": ["Review:  Classification Using Decision Tree in KNIME", "Review:  Interpreting a Decision Tree in KNIME", "Workflow Overview for Building a Decision Tree in KNIME", "Description of combined_data.csv", "Classifying in KNIME to identify big spenders in Catch the Pink Flamingo"], "title": "Data Classification with KNIME"}, {"description": "This week we do some clustering with Spark. ", "video": ["Informing business strategies based on client base", "Is there only “one way” to cluster a client base?", "How many clusters?", "What kind of criteria might provide actionable information for Eglence Inc.?", "Practice with PySpark MLlib Clustering", "Recommending Actions from Clustering Analysis"], "title": "Clustering with Spark"}, {"description": "This week we apply what we learned from the 'Graph Analytics With Big Data' course to simulated chat data from Catch the Pink Flamingos using Neo4j. We analyze player chat behavior to find ways of improving the game. ", "video": ["Understanding the Simulated Chat Data Generated by the Scripts", "Graph Analytics of Catch the Pink Flamingo Chat Data Using Neo4j", "Graph Analytics With Chat Data Using Neo4j"], "title": "Graph Analytics of Simulated Chat Data With Neo4j"}, {"description": "", "video": ["Week 5: Bringing It All Together", "Final project preparation"], "title": "Reporting and Presenting Your Work"}, {"description": "", "video": ["Congratulations! Some Final Words...", "Optional 3-minute video: Splunk opportunity", "Part 2: Help us connect your video to your LinkedIn profile", "Final Project"], "title": "Final Submission"}], "title": "Big Data - Capstone Project"}, {"course_info": "About this course: In this Capstone Project, you'll bring together all the new skills and insights you've learned through the four courses. You'll be given a 'mock' client problem and a data set. You'll need to analyze the data to gain business insights, research the client's domain area, and create recommendations. You'll then need to visualize the data in a client-facing presentation. You'll bring it all together in a recorded video presentation.\n\nThis course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017.", "level": "Beginner", "package_name": "Data Analysis and Presentation Skills: the PwC Approach Specialization ", "created_by": "PwC", "package_num": "5", "teach_by": [{"name": "Alex Mannella", "department": null}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "In this course, you will be using the skills you’ve been developing throughout the series of PwC courses to create and present a deliverable for a client.You’re going to determine your approach for analysis, use the information provided by the client to conduct an analysis, determine the best way to show your findings and then develop a plan to deliver your results and recommendations to the client in a video presentation.In this course, you will be acting as a consultant interacting with a client, Electric Growers.  The client needs your strong data analytics and Excel skills to analyze data and help them make smart decisions to enable the company’s move into the security devices and services market.  Each week, the client, represented by Michele Tran, a Vice President of Operations from Electric Growers, and the management team, will ask you to use and analyze the data, provided by the client, to answer a series of questions that will help them make decisions.You’ll receive peer feedback that you can use to enhance future presentations. This course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017.", "video": ["Welcome to Course 5", "Course Overview and Syllabus", "Meet the PwC Instructors and Content Developers", " Introduce yourselves and share your goals for this course", "A Message from our Chief People Officer at PwC", "Overview of the first assignment", "Electric Growers Dataset", "Prior courses' resources", "Week 1 - Understanding the Business Problem", "Experiencing challenges with the outcomes expected for this week? - Week 1", "Electric Growers Simulation - Ask questions to understand the business problem", "What questions do you have?", "Electric Growers Simulation Quiz"], "title": "Understanding the Business Problem "}, {"description": "This week, you will be using the results of last week’s analysis to come up with some hypotheses. You will be answering questions such as: Who are the ideal customers that should be targeted? How should they be approached to maximize the sales of the client’s new home security systems?  Using the data, figure out the attributes of the customer who wants to install an advanced, hi-tech security system and the attributes of a person who would switch security system providers. Continue to use the knowledge you’ve acquired in the other courses that you’ve taken in this series.", "video": ["Week 2 - Overview of the second assignment", "Prior courses' resources", "Week 2 - Analyzing the business problem", "Experiencing challenges with the outcomes expected for this week? - Week 2", "Alternative tools to solve business problems", "Peer review assignment 2"], "title": "Analyzing the Business Problem"}, {"description": "This week, the client has more questions for you to answer to further develop the profile of potential customers.  This includes finding which cities to target and how to evaluate them.  You will have access to crime rates, population, housing growth city wide data and segmentation distribution dumps to help in your analysis.", "video": ["Week 3 - Overview of the third assignment", "Prior courses' resources", "Week 3: Creating a visual representation of your analysis results", "Experiencing challenges with the outcomes expected for this week? - Week 3", "Share tips and best practices", "Peer review assignment 3"], "title": "Creating a visual representation of your analysis results "}, {"description": "This week, you will be taking your findings and detailed analysis on plans to launch the new home security system and put them into a PowerPoint presentation deck for Michele Tran and the management team. The presentation should be about 10 slides and should follow the guidelines and best practices of presenting to a client that were covered in the earlier courses within this specialization. Ms. Tran wants to review it and give you feedback, if needed, before having it presented to the management team.This is a chance to use what you learned about creating a presentation, using PowerPoint tools, creating charts and other methods of enhancing a presentation.", "video": ["Week 4 - Overview of the fourth assignment", "Prior courses' resources", "Week 4 - Building a presentation for the client meeting", "Experiencing challenges with the outcomes expected for this week? - Week 4", "What challenges did you face?", "Peer review assignment 4"], "title": "Building a presentation for the client meeting "}, {"description": "This week, Ms. Tran has reviewed your draft presentation so it is now time to refine and make changes or additions to enhance the presentation before presenting it to the management team via video.\n\nOnce you’ve finalized your presentation, you will present it in a video using your smartphone or computer. When you’re satisfied with the PowerPoint presentation and video, you will be submitting both for peer review. You can use this feedback for current and future presentations that you will make during your career.", "video": ["Week 5 - Overview of the fifth assignment", "Prior courses' resources", "Week 5: Presenting your results to the client", "Best tips for recording your own video", "Personal or professional application of your new skills", "Course closing - Outcomes from Electric Growers", "Course closing & Thank you", "Specialization closing", "Get Personal with Alex Mannella Podcast - Day in the Life", "Peer review assignment 5:  Deliver a virtual presentation"], "title": "Presenting your results to the client"}], "title": "Data Analysis and Presentation Skills: the PwC Approach Final Project"}, {"course_info": "About this course: The capstone course, Design and Build a Data Warehouse for Business Intelligence Implementation, features a real-world case study that integrates your learning across all courses in the specialization. In response to business requirements presented in a case study, you’ll design and build a small data warehouse, create data integration workflows to refresh the warehouse, write SQL statements to support analytical and summary query requirements, and use the MicroStrategy business intelligence platform to create dashboards and visualizations.\n\nIn the first part of the capstone course, you’ll be introduced to a medium-sized firm, learning about their data warehouse and business intelligence requirements and existing data sources. You’ll first architect a warehouse schema and dimensional model for a small data warehouse. You’ll then create data integration workflows using Pentaho Data Integration to refresh your data warehouse. Next, you’ll write SQL statements for analytical query requirements and create materialized views to support summary data management. Finally, you will use MicroStrategy OLAP capabilities to gain insights into your data warehouse. In the completed project, you’ll have built a small data warehouse containing a schema design, data integration workflows, analytical queries, materialized views, dashboards and visualizations that you’ll be proud to show to your current and prospective employers.", "level": null, "package_name": "Data Warehousing for Business Intelligence Specialization ", "created_by": "University of Colorado System", "package_num": "5", "teach_by": [{"name": "Michael Mannino", "department": "Business School, University of Colorado Denver"}, {"name": "Jahangir Karimi", "department": "Information Systems University of Colorado Denver"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "Module 1 introduces the objectives and topics in the course and provides background on the case and software requirements. The capstone course is organized around a realistic case study based on the business situation faced by CPI Card Group in 2015. ", "video": ["Course introduction", "Slides for lesson 1", "Course topics and assignments video lesson", "Slides for lesson 2", "Executive interview", "Slides for lesson 3", "Background on CPI Card Group", "Overview of software requirements", "Oracle database server installation", "Pentaho Data Integration installation", "Microstrategy Desktop installation", "Database diagramming tools"], "title": "Course Overview"}, {"description": "Module 2 presents the requirements of the first part of the case study involving data warehouse design. To provide a context for the case study, you can listen to an executive interview with a CPI Card Group executive.", "video": ["Executive interview", "Slides for lesson 1", "Data warehouse design background", "Documents for the module 2 assignment", "Documents to review after the assignment", "Data warehouse design assignment"], "title": "Data Warehouse Design"}, {"description": "Module 3 presents requirements for the second part of the case study involving data integration. To provide a context for the case study, you can listen to executive interviews with executives from CPI Card Group, First Bank, and Pinnacol Assurance.", "video": ["Executive interview", "Slides for lesson 1", "Executive interview", "Slides for lesson 2", "Executive interview", "Slides for lesson 3", "Data integration background", "Documents for the module 3 assignment", "Practice Quiz for module 5 assignment-Test DW", "Assignment for module 3", "Quiz for module 5 assignment-Production DW"], "title": "Data Integration"}, {"description": "Module 4 presents requirements for the third part of the case study involving analytical queries and summary data management. ", "video": ["Executive Interview with Kellyn Gorman of Oracle", "Slides for lesson 1", "Documents for the module 4 assignment", "Solutions to challenge problems", "Analytical Query Assignment"], "title": "Analytical Queries and Summary Data Management"}, {"description": "Module 5 presents the data visualization and dashboard design requirements for the fourth part of the case study. ", "video": ["Executive Interview with Matthew Caton of Data Source Consulting", "Slides for executive interview with Matthew Caton", "Executive Interview with Tyler Wilson on BI Platform Capabilities at CPI Card Group", "Capstone Project Data Visualizations and Dashboard Design Requirements", "Earlier Assignments from Course 4", "Q & A and Error Knowledge base"], "title": " Data Visualization and Dashboard Design Requirements  "}, {"description": "This is an extension of Module 5. The peer assessment from module 5 is moved to module 6 to give you more time completing the assignments in prior modules as well as for you to do your peer assessment in this module.", "video": ["Executive Interview with James Gualke on the State of BI Maturity and Strategy at PDC Energy", "Background Information on Data Visualization and Dashboard Design", "Course conclusion video lecture", "Data Visualization and Dashboard Design Assignment"], "title": "Wrap Up and Project Submission"}], "title": "Design and Build a Data Warehouse for Business Intelligence Implementation"}, {"course_info": "About this course: In this capstone project we’ll combine  all of the skills from all four specialization courses to do something really fun: analyze social networks!  \n\nThe opportunities for learning are practically endless in a social network.  Who are the “influential” members of the network?  What are the sub-communities in the network?   Who is connected to whom, and by how many links?   These are just some of the questions you can explore in this project.\n\nWe will provide you with a real-world data set and some infrastructure for getting started, as well as some warm up tasks and basic project requirements, but then it’ll be up to you where you want to take the project.  If you’re running short on ideas, we’ll have several suggested directions that can help get your creativity and imagination going.  Finally, to integrate the skills you acquired in course 4 (and to show off your project!) you will be asked to create a video showcase of your final product.", "level": null, "package_name": "Object Oriented Java Programming: Data Structures and Beyond Specialization ", "created_by": "University of California, San Diego", "package_num": "5", "teach_by": [{"name": "Christine Alvarado", "department": "Computer Science and Engineering"}, {"name": "Mia Minnes", "department": "Computer Science and Engineering"}, {"name": "Leo Porter", "department": "Computer Science and Engineering"}], "target_audience": null, "rating": "4.7", "week_data": [{"description": "Welcome to our capstone project!  In the last four courses in this specialization you've learned many core data structures and algorithms, and applied them to three different real-world projects. In this capstone project you'll be doing a project very much like the projects from these other courses, only it will be almost entirely directed by you!  In this first week you'll get warmed up by playing around with the data that will form the backbone of this project: social network data.  Then you'll get back into writing code by implementing a couple of graph algorithms to answer questions about this data.  ", "video": ["Welcome and Course Introduction", "By the end of this capstone, you will be able to ...", "Specialization Completion Rewards", "Capstone Project Overview", "Capstone Project Exemplar", "Project overview and timeline", "Introduction to (some) social network data", "Representing social network data as a graph", "Warm up algorithm 1: Extracting egonets", "Warm up algorithm 2: Strongly Connected Components", "Warm up algorithm 2: Strongly connected components, part 2", "Capstone Warmup Assignment: Where to get help", "What questions do you have about the project?", "Capstone warm up", "Warm-up Feedback"], "title": "Introduction and Warm up"}, {"description": "Now that you're warmed up, it's time to get started planning for the bulk of your capstone project.  This week you will identify several questions you'd like to answer about the social network data.  For each of these questions, you'll research and evaluate data structures and algorithms that would be useful in implementing a solution.  Defining the scope of your project and anticipating bottlenecks and tricky spots is tough but extremely valuable.  You'll use asymptotic analysis to guide and refine your design.", "video": ["Week 2 introduction", "Preparing to watch the \"Project ideas\" videos", "Project idea: information flow in a social network", "Project idea: broadcasting to a Twitter network, part 1", "Project idea: broadcasting to a Twitter network, part 2", "Project idea: detecting communities", "Analyzing algorithms before implementing", "Example Report: Scope and Problem Definition", "Scope and Problem Definition", "End of Week Feedback"], "title": "Project Definition and Scope"}, {"description": "Now that you've identified the two problems you want to solve, this week you'll work to solve the easier of the two. This week you are predominately on your own to work independently.  To solve the problem, you'll likely create small datasets for testing, research existing solutions to related problems, implement a solution, test your solution, and analyze the algorithmic runtime of the solution.  You can optionally write-up a report of your work for peer-review feedback. ", "video": ["Week 3 introduction", "Overview of report", "Sample Project Reports", "Optional mini-project report", "End of Week Feedback"], "title": "Capstone Implementation: Mini-project"}, {"description": "This week, you will work on your own on the larger problem you aim to solve.  You'll have two weeks (this and the next) to solve the larger problem and submit a report for peer feedback.  For this week, you should aim to create small test datasets, research exist solutions, and analyze the runtime of your potential solutions.  You should also research datasets which might be particularly interesting for your problem.", "video": ["Week 4 introduction", "Content Links to Prevous Courses", "Optional progress report"], "title": "Capstone Implementation: Full project checkpoint"}, {"description": "Now you get to finalize your project!  This week, you will finish your solution to the larger problem and submit a report for peer feedback.  This is also an opportunity for reflection about what went well and what went poorly in the process of completing the project.  It is also an opportunity to reflect on how far your technical skills have advanced since the beginning of this specialization.", "video": ["Week 5 introduction", "Peer review best practices", "Capstone Project Written Report and Code", "End of Week Feedback"], "title": "Capstone Implementation: Full project final deadline"}, {"description": "In this week, you get to present your project to the learner community!  This will combine all the skills you've learned in the specialization: algorithm analysis, object oriented programming, design and use of data structures, and presenting your work with confidence.  We look forward to seeing what you've created!", "video": ["Week 6 introduction", "Demo presentation", "End of Specialization message", "Capstone Oral Report", "End of Capstone Feedback", "End of Specialization Feedback"], "title": "Capstone oral report"}], "title": "Capstone: Analyzing (Social) Network Data"}, {"course_info": "About this course: The Capstone project is an individual assignment.\nParticipants decide the theme they want to explore and define the issue they want to solve. Their “playing field” should provide data from various sectors (such as farming and nutrition, culture, economy and employment, Education & Research, International & Europe, Housing, Sustainable, Development & Energies, Health & Social, Society, Territories & Transport). Participants are encouraged to mix the different fields and leverage the existing information with other (properly sourced) open data sets.\n\nDeliverable 1 is the preliminary preparation and problem qualification step. The objectives is to define the what, why & how. What issue do we want to solve? Why does it promise value for public authorities, companies, citizens? How do we want to explore the provided data? \n\nFor Deliverable 2, the participant needs to present the intermediary outputs and adjustments to the analysis framework. The objectives is to confirm the how and the relevancy of the first results. \n\nFinally, with Deliverable 3, the participant needs to present the final outputs and the value case. The objective is to confirm the why. Why will it create value for public authorities, companies, and citizens.\n\nAssessment and grading: the participants will present their results to their peers on a regular basis. An evaluation framework will be provided for the participants to assess the quality of each other’s deliverables.", "level": null, "package_name": "Strategic Business Analytics Specialization ", "created_by": "ESSEC Business School", "package_num": "4", "teach_by": [{"name": "Nicolas Glady ", "department": "Marketing Department "}], "target_audience": null, "rating": "3.6", "week_data": [{"description": "Module 1 in the Business Analytics capstone project provides you with a clear idea of how to successfully complete the ESSEC Business Analytics MOOC. It is dedicated to ensuring that you understand the objectives of the capstone project and lets you consult the datasets to be used for the project as well as examples of what the expected deliverable should look like and contain. Before beginning the project, you are advised to review two previous modules dealing with how to effectively structure and present your findings, and how to approach and explore datasets (\"Foundation of Business Analytics\", the wrap up of \"Case Studies in Business Analytics with Accenture\"). This module  also gives you the opportunity to try out the preparation of deliverable 1 and receive non-graded feedback from your peers, thereby giving you an essential insight into how the other deliverables and peer review steps will work. ", "video": ["Objective of the capstone", "Datasets used for the capstones", "Example deliverable 1", "Example: Final deliverable", "Guidelines for watching the following video", "Reporting your results: introduction", "Guidelines for watching the following video", "It's all about the story", "Think of an excellent presentation you have seen...", "Guidelines for watching the following video", "One slide, One idea", "Guidelines for watching the following video", "A picture is worth a thousand words", "Should presentations be considered shows?", "Guidelines for watching the following video", "Recital M5 - How to present your findings", "Among the techniques used to effectively present your findings, which are most useful? ", "Guidelines for watching the following video", "Wrap-up: Reporting your results", "How should I define what’s relevant to a business for organization? ", "Guidelines for watching the following video", "How to create value from data? - Fabrice Marque", "Guidelines for watching the following video", "Wrap up  - Mickael Svilar", "Guidelines for watching the following video", "Data exploration is an iterative process - Nicolas Glady", "How can you effectively manage this part of the exploration process? ", "Guidelines for watching the following video", "Analytics exploration - Oonagh O’Shea & Noelle Doody", "Guidelines for watching the following video", "Wrap up & Capstone guidelines  - Nicolas Glady", "Visual", "Optional ungraded peer review information", "Optional Deliverable 1: Define the analysis framework. Share your working draft"], "title": "Introduction and step 1 : Define the analysis framework"}, {"description": "The module 2 sets the assignment of preparing deliverable 1 – your analysis framework – for assessment by your peers. ", "video": ["Assignment:Define the analysis framework.", "Required deliverable 1:Define the analysis framework"], "title": "Required assignement 1: Define the analysis framework"}, {"description": "In Module 3 you will be involved in reviewing the deliverables of a minimum of 3 other students( if you can manage more than 3, then all the better!) as well as preparing deliverable 2", "video": ["This week is dedicated to the required feed back on deliverable 1>>", "Prepare deliverable 2: Present the intermediary outputs and adjustments to the analysis framework"], "title": "Required feed back on Delivery 1:Define the analysis framework and preparation of deliverable 2"}, {"description": "Module 4 enables you to submit your draft proposal for deliverable 2 to your peers for review and feedback. \n", "video": ["Optional delivery 2: Present the intermediary outputs and adjustments to the analysis framework", "Optional delivery 2: Present the intermediary outputs and adjustments to the analysis framework"], "title": "Practice for Deliverable 2"}, {"description": "The module 5 sets the assignment of preparing deliverable 2 – Present the intermediary outputs and adjustments to the analysis framework – for assessment by your peers. ", "video": ["Required deliverable 2: Present the intermediary outputs and adjustments to the analysis framework"], "title": "Required assignement 2: Present the intermediary outputs and adjustments to the analysis framework"}, {"description": "In Module 6 you will be involved in reviewing the deliverables of a minimum of 3 other students( if you can manage more than 3, then all the better!) as well as preparing deliverable 3", "video": ["This week is dedicated to the required feedback on delivery 2", "Prepare deliverable 3: Present the final outputs and value case"], "title": "Required feedback for delivery 2 and preparation of delivery 3"}, {"description": "The module 7 sets the assignment of preparing deliverable 3 – Present the final outputs and value case – for assessment by your peers. ", "video": ["Present the final outputs and value case"], "title": "Required Delivery 3: Present the final outputs and value case"}, {"description": "In Module 8 you will be involved in reviewing the deliverables of a minimum of 3 other students ( if you can manage more than 3, then all the better!) as well as preparing deliverable 3", "video": ["This week is dedicated to the required feedback on deliverable 3>>"], "title": "Required feedback on Assignment 3: Present the final outputs and value case"}], "title": "Capstone:  Create Value from Open Data"}, {"course_info": "About this course: In this culminating project, you will deploy the tools and techniques that you've mastered over the course of the specialization. You'll work with a real data set to perform analyses and prepare a report of your findings.", "level": null, "package_name": "Genomic Data Science Specialization ", "created_by": "Johns Hopkins University", "package_num": "8", "teach_by": [{"name": "Jeff Leek, PhD", "department": "Bloomberg School of Public Health "}, {"name": "Kasper Daniel Hansen, PhD", "department": "Bloomberg School of Public Health"}], "target_audience": null, "rating": "4.5", "week_data": [{"description": "In this first week, we'll introduce the project and get you oriented to the tasks that you'll be performing over the next several weeks. ", "video": ["Introduction to the Capstone - Kasper Hansen", "Welcome", "The big question", "What you will be doing in this capstone project", "Passing the Capstone", "Why re-analysis?", "Why is this so hard?"], "title": "Introduction"}, {"description": "This week, we'll really dig into the dataset by providing an introduction from Andrew Jaffe, the lead scientist on the analysis. You should also be looking ahead to Task 2, which is due in Week 4; the alignment will take a long time to perform, so you should start early.", "video": ["The Dataset - Andrew Jaffe", "Phenotype information", "Getting the data from SRA", "Which samples to use"], "title": "Introduction to the Dataset - Andrew Jaffe"}, {"description": "The purpose of genomic data science is to answer fundamental questions in biology. Before starting on the data analysis process, the first step is always to understand the scientific question you are trying to answer. Don't forget to stay on top of the alignment task due in Week 4; it will take a long time to accomplish and shouldn't be put off.", "video": ["Task 1 Instructions", "The Results - Andrew Jaffe", "What did you find challenging about task 1?", "Understand the problem"], "title": "Understand the Problem"}, {"description": "Once you have understood the problem, the next step is to obtain the raw data so that you can perform your analysis. ", "video": ["What did you find challenging about task 2?", "Alignment"], "title": "Alignment"}, {"description": "Now you have aligned the data, the next step is to do some quality control to make sure that the data are in good shape. ", "video": ["What did you find challenging about task 3?", "QC the Alignment"], "title": "QC the Alignment"}, {"description": "Now that you have performed alignment and quality control, the next step is to calculate the abundance of every gene in every sample.", "video": ["What did you find challenging about task 4?", "Get Feature Counts "], "title": "Get Feature Counts "}, {"description": "After summarizing your genomic data the next step is to load the data into R for analysis with Bioconductor.", "video": ["What did you find challenging about task 5?", "Exploratory Analysis"], "title": "Exploratory Analysis "}, {"description": "The next step is to perform a statistical analysis to detect genes that are differentially expressed. ", "video": ["What did you find challenging about task 6?", "Statistical Analysis"], "title": "Statistical Analysis"}, {"description": "In task 6, we have identified genes differentially expressed between fetal and adult brain. Now we will examine these results in a wider context. ", "video": ["What did you find challenging about task 7?", "Optional Peer Review of Your Report Draft", "Gene Set Analysis"], "title": "Gene Set Analysis"}, {"description": "The next step is to document your work. One of the major issues in genomic data science is that there are so many steps in the process. If these steps are not documented well the result can be major problems. ", "video": ["What did you find challenging about task 8?", "Describe Your Analysis"], "title": "Describe Your Analysis "}], "title": "Genomic Data Science Capstone"}, {"course_info": "About this course: The Capstone project will allow you to continue to apply and refine the data analytic techniques learned from the previous courses in the Specialization to address an important issue in society. You will use real world data to complete a project with our industry and academic partners. For example, you can work with our industry partner, DRIVENDATA, to help them solve some of the world's biggest social challenges! DRIVENDATA at www.drivendata.org, is committed to bringing cutting-edge practices in data science and crowdsourcing to some of the world's biggest social challenges and the organizations taking them on. \n\nOr, you can work with our other industry partner, The Connection (www.theconnectioninc.org) to help them better understand recidivism risk for people on parole seeking substance use treatment. For more than 40 years, The Connection has been one of Connecticut’s leading private, nonprofit human service and community development agencies. Each month, thousands of people are assisted by The Connection’s diverse behavioral health, family support and community justice programs. The Connection’s Institute for Innovative Practice was created in 2010 to bridge the gap between researchers and practitioners in the behavioral health and criminal justice fields with the goal of developing maximally effective, evidence-based treatment programs. \n\nA major component of the Capstone project is for you to be able to choose the information from your analyses that best conveys results and implications, and to tell a compelling story with this information. By the end of the course, you will have a professional quality report of your findings that can be shown to colleagues and potential employers to demonstrate the skills you learned by completing the Specialization.", "level": null, "package_name": "Data Analysis and Interpretation Specialization ", "created_by": "Wesleyan University", "package_num": "5", "teach_by": [{"name": "Jen Rose", "department": "Psychology"}, {"name": "Lisa Dierker", "department": "Psychology"}], "target_audience": null, "rating": "4.6", "week_data": [{"description": "In this Module, your goal is to review the lectures and readings in the Overview of the Capstone Project, and 1) decide which data set you will use to complete your capstone project. In addition 2) identify your research question, 3) propose a title for your final report, and 4) complete Milestone Assignment 1 as described in the assignment. By the end of this Module you will have drafted a final report Title and Introduction to the Research Question. Your Introduction to the Research Question should include a statement of your research question, your motivation or rationale for testing the research question, and some potential implications of answering your research question.", "video": ["Read Me First", "Introduction to the Capstone Course", "Industry Partner: DRIVENDATA", "Industry Partner: The Connection", "Overview of Each Module and Assignments", "About the Assignments", "Data Sets and Codebooks", "Creating a Final Report", "Your Capstone Project", "SAS and python code for sample final report", "SAS Data Sets", "Sample Final Report Data Set", "Writing Your Report: Title and Introduction", "Writing Your Report: What to Avoid", "Milestone Assignment 1: Title and Introduction to the Research Question"], "title": "Module 1. Identify Your Data and Research Question"}, {"description": "In this Module, your goals are to 1) complete the majority of your data management so that you are ready to begin your preliminary statistical analyses; and 2) complete Milestone Assignment 2 as described in the assignment. By the end of this Module you will have drafted a final report Methods section. Your Methods section should include a description of your sample, measures, and the analyses you plan to use to test your research question.", "video": ["Writing Your Report: Methods", "Milestone Assignment 2: Methods"], "title": "Module 2. Data Management"}, {"description": "In this Module, your goals are to 1) explore your data more extensively through descriptive and basic statistical analyses and data visualization; and 2) complete Milestone Assignment 3 as described in the Assignment. By the end of this module, you will have begun to draft your final report Results section, including some figures.", "video": ["Writing Your Report: Results", "Milestone Assignment 3: Preliminary Results"], "title": "Module 3. Exploratory Data Analysis"}, {"description": "In this Module, you 1) will complete your analyses; 2) finish writing your final report, and 3) submit your completed Final Report as the fourth and final assignment. A complete description of what is required for your final report and a detailed grading rubric can be found with the assignment; a sample final report is provided with the materials in the first module. ", "video": ["Writing Your Report: Conclusions and Limitations", "Final Assignment Grading Rubric", "Final Report"], "title": "Complete Your Final Report"}], "title": "Data Analysis and Interpretation Capstone"}, {"course_info": "About this course: In this project-based course, you will design and execute a complete GIS-based analysis – from identifying a concept, question or issue you wish to develop, all the way to final data products and maps that you can add to your portfolio. Your completed project will demonstrate your mastery of the content in the GIS Specialization and is broken up into four phases:\n\nMilestone 1: Project Proposal - Conceptualize and design your project in the abstract, and write a short proposal that includes the project description, expected data needs, timeline, and how you expect to complete it.\n\nMilestone 2: Workflow Design - Develop the analysis workflow for your project, which will typically involve creating at least one core algorithm for processing your data. The model need not be complex or complicated, but it should allow you to analyze spatial data for a new output or to create a new analytical map of some type.\n\nMilestone 3: Data Analysis – Obtain and preprocess data, run it through your models or other workflows in order to get your rough data products, and begin creating your final map products and/or analysis.\n\nMilestone 4: Web and Print Map Creation – Complete your project by submitting usable and attractive maps and your data and algorithm for peer review and feedback.", "level": null, "package_name": "Geographic Information Systems (GIS) Specialization ", "created_by": "University of California, Davis", "package_num": "5", "teach_by": [{"name": "Nick Santos", "department": "Center for Watershed Sciences"}], "target_audience": "Who is this class for: This course is targeted at those who have taken the first four courses in the GIS specialization, or who have equivalent foundational GIS experience. We encourage you to audit or enroll in those courses before taking this course.", "rating": "4.9", "week_data": [{"description": "In this milestone, you will have weeks 1 and 2 to build a project proposal that contains your research question or hypothesis, background information, potential data sources and methods, and your expected results. This proposal will lead you into future milestones by providing a guide to help keep your analysis on track. You will start by getting an overview of the entire project and the assignment for this first milestone. From there, you will learn about some sources for project ideas and data sources and look at an example project proposal.", "video": ["Course Overview", "Course Project Overview", "Getting Help in this Course", "Project Proposal Overview", "Milestone 1 Assignment Preview", "Data Sources and Repositories", "Finding Project Ideas", "Share Your Project Ideas!", "Data Sources For Your Project", "Example Project Proposal", "Extra Practice: Make Your Project a Website", "Vector-Based Suitability Analysis (Review from Imagery, Automation, and Applications)", "Classifying Imagery and Derived Products (Review from Imagery, Automation, and Applications)"], "title": "Course Overview and Milestone 1: Project Proposal"}, {"description": "In this module, you will continue to work through Milestone 1, your project proposal as outlined in the first week. You will then submit your proposal for peer review.", "video": ["Milestone 1: Project Proposal"], "title": "Milestone 1: Project Proposal Submission"}, {"description": "In this milestone, you will have week 3 to practice your algorithmic development. In the previous milestone, you posed a question you want to answer - now you'll develop a plan, your algorithm, for how to answer that question with GIS. In practice, this means you'll develop a ModelBuilder model that shows your planned analysis workflow, or some part of it. For those of you who are conducting an analysis that's not conducive to making a model, you can write out your series of steps instead. Regardless, by the end of this module, you'll have a plan for how to produce your results.", "video": ["Planning Your Workflow Overview", "Milestone 2 Assignment Preview", "Extra Practice: Data Analysis", "What is ModelBuilder? (Review from Imagery, Automation, and Applications)", "Creating Toolboxes and Tools with ModelBuilder (Review from Imagery, Automation, and Applications)", "Setting Up a Larger Model (Review from Imagery, Automation, and Applications)", "Using Interface Tools as Geoprocessing Tools in ModelBuilder (Review from Imagery, Automation, and Applications)", "Feature Layers and Selections in Models (Review from Imagery, Automation, and Applications)", "Branching, Preconditions, and Viewing Progress Interactively (Review from Imagery, Automation, and Applications)", "Polishing Models for Reuse (Review from Imagery, Automation, and Applications)", "Advanced Models and Exporting Models to Python (Review from Imagery, Automation, and Applications)", "Milestone 2: Workflow"], "title": "Milestone 2: Planning Your Workflow"}, {"description": "For this milestone, you will have weeks 4, 5, and 6 to process your data according to the model you created in the previous milestone, reinforcing your data analysis concepts and practice. When you complete your analysis, you will add metadata to any resulting layers, and you will also write an assessment of what the results mean and how they answer your research question.", "video": ["Data Analysis Overview", "Milestone 3 Assignment Preview", "Example Methods and Results Document"], "title": "Milestone 3: Data Analysis"}, {"description": "In this module, you will continue to work through Milestone 3, analyzing your data as outlined in the fourth week. Pay close attention to data quality issues and your metadata, as reviewed in this week's videos. You will have one more week to complete your data analysis.", "video": ["Assessing Data Quality and Uncertainty (Review from Data Formats, Design, & Quality)", "Data Quality (Review from Data Formats, Design, & Quality)", "Metadata (Review from Fundamentals of GIS)"], "title": "Milestone 3: Data Analysis Continue"}, {"description": "In this module, you will continue to work through Milestone 3, analyzing your data as outlined in the fourth and fifth week. You will then submit your data analysis for peer review.", "video": ["Milestone 3: Data Analysis"], "title": "Milestone 3: Data Analysis Submission"}, {"description": "In this module, you will have weeks 7 and 8 to hone your map-making skills, building at least two maps that visually interpret the results of your analysis. In making both a web map and a print-layout map, as well as through extra practice materials, you'll refine cartographic techniques that you previously learned as well as new ones to help you to better display information in map form. Should you choose to, you will also build a small website for your project by the time you complete this module.", "video": ["Creating Your Maps Overview", "Milestone 4 Assignment Preview", "Uploading Rasters to ArcGIS Online", "Extra Practice: Cartography and Online Maps", "Loading Layers in ArcGIS Online (Review from GIS Data Formats, Design, and Quality)", "Applying Symbology in ArcGIS Online (Review from GIS Data Formats, Design, and Quality)", "Map Annotations and Scaling in ArcGIS Online (Review from GIS Data Formats, Design, and Quality)", "Saving and Sharing Maps with ArcGIS Online (Review from GIS Data Formats, Design, and Quality)"], "title": "Milestone 4: Creating Your Maps"}, {"description": "In this module, you will continue to work through Milestone 4, creating your maps as outlined in the seventh week. You will then submit your maps for peer review.", "video": ["Extra Practice: Make Your Project a Website", "Course Summary", "Creating Your Maps"], "title": "Milestone 4: Creating Your Maps Submission"}], "title": "Geospatial Analysis Project"}, {"course_info": "About this course: R Programming Capstone", "level": "Intermediate", "package_name": "Mastering Software Development in R Specialization ", "created_by": "Johns Hopkins University", "package_num": "5", "teach_by": [{"name": "Roger D. Peng, PhD", "department": "Bloomberg School of Public Health"}, {"name": "Brooke Anderson", "department": "Colorado State University"}], "target_audience": null, "rating": "4.1", "week_data": [{"description": "The overall goal of the capstone project is to integrate the skills you have developed over the courses in this Specialization and to build a software package that can be used to work with the NOAA Significant Earthquakes dataset.", "video": ["Welcome to the Capstone Project", "Introduction", "Module 1: Obtain and Clean the Data"], "title": "Obtain and Clean the Data"}, {"description": "Show us when earthquakes occurred in different countries, their magnitude, and their toll on human life.", "video": ["Module 2: Visualization Tools"], "title": "Building Geoms"}, {"description": "Show and annotate the earthquake epicenters.", "video": ["Module 3: Mapping Tools"], "title": "Building a Leaflet Map"}, {"description": "Documentation is one of the most important and most commonly overlooked steps when writing software, but you're not going to let that happen in your project.", "video": ["Documentation and Packaging Tasks", "Testing", "Documentation, Packaging, and Testing"], "title": "Documentation and Packaging"}, {"description": "The moment of truth. It's time to push your package to GitHub.", "video": ["GitHub and Travis"], "title": "Deployment"}, {"description": "It's time to submit your deployed package for evaluation and to evaluate the work of a few of your classmates.", "video": ["Package Evaluation"], "title": "Final Assessment"}], "title": "Mastering Software Development in R Capstone"}, {"course_info": "About this course: The analytics process is a collection of interrelated activities that lead to better decisions and to a higher business performance. The capstone of this specialization is designed with the goal of allowing you to experience this process. The capstone project will take you from data to analysis and models, and ultimately to presentation of insights. \n\nIn this capstone project, you will analyze the data on financial loans to help with the investment decisions of an investment company. You will go through all typical steps of a data analytics project, including data understanding and cleanup, data analysis, and presentation of analytical results. \nFor the first week, the goal is to understand the data and prepare the data for analysis. As we discussed  in this specialization, data preprocessing and cleanup is often the first step in data analytics projects. Needless to say, this step is crucial for the success of this project.  \n\nIn the second week, you will perform some predictive analytics tasks, including classifying loans and predicting losses from defaulted loans. You will try a variety of tools and techniques  this week, as the predictive accuracy of different tools can vary quite a bit. It is rarely the case that the default model produced by ASP is the best model possible. Therefore, it is important for you to tune the different models in order to improve the performance.\n\nBeginning in the third week, we turn our attention to prescriptive analytics, where you will provide some concrete suggestions on how to allocate investment funds using analytics tools, including clustering and simulation based optimization. You will see that allocating funds wisely is crucial for the financial return of the investment portfolio.\n\nIn the last week, you are expected to present your analytics results to your clients. Since you will obtain many results in your project, it is important for you to judiciously choose what to include in your presentation. You are also expected to follow the principles we covered in the courses in preparing your presentation.", "level": null, "package_name": "Advanced Business Analytics Specialization ", "created_by": "University of Colorado Boulder", "package_num": "5", "teach_by": [{"name": "Manuel Laguna", "department": "Leeds School of Business"}, {"name": "Dan Zhang", "department": "Leeds School of Business"}, {"name": "David Torgerson", "department": null}], "target_audience": null, "rating": "4.2", "week_data": [{"description": "This week your goal is to understand the data and prepare the data for analysis. As we discussed in this specialization, data preprocessing and cleanup is often the first step in data analytics projects. Needless to say, this step is crucial for the success of this project.  We've selected a few videos from Courses 2 and 4 for you to review before completing this week's assignments. Dealing With Missing Values and Dealing with Outliers videos will remind you how to perform preliminary data cleanups.  The last part of the assignments ask you to construct data visualizations. You may find the ideas discussed in What is Good Data Visualization? and Graphical Excellence useful. ", "video": ["Data Cleanup and Transformation", "Dealing with Missing Values", "Dealing with Outliers", "What is Good Data Visualization", "Graphical Excellence", "Introduction to the Project", "Register for Analytic Solver Platform for Education (ASPE)", "Understand the data and prepare your data for analysis"], "title": "Module 1 - Understand the data and prepare your data for analysis"}, {"description": "This week you will perform some predictive analytics tasks, including classifying loans and predicting losses from defaulted loans. You will try a variety of tools and techniques this week, as the predictive accuracy of different tools can vary quite a bit. It is rarely the case that the default model produced by ASP is the best model possible. Therefore, it is important for you to tune the different models in order to improve the performance.This week’s assignments require you to build predictive models for both classification and regression tasks. <p> Before working on the assignments, you may review a few videos to remind yourself several important concepts, such as cross validation. These concepts are discussed in the videos  Cross Validation and Confusion Matrix and Assessing Predictive Accuracy Using Cross-Validation. You may also find a refresher on XLMiner useful. The videos Building Logistic Regression Models using XLMiner  and How to Build a Model using XLMiner discuss how to build logistic regression and linear regression models. Depending on your needs, you may also go back to the videos that discuss how to build trees and neural networks. </p>", "video": ["Cross Validation and Confusion Matrix", "Assessing Predictive Accuracy Using Cross-Validation", "Building Logistic Regression Models using XLMiner", "How to Build a Model using XLMiner", "Perform predictive analytics tasks"], "title": "Module 2 - Perform predictive analytics tasks"}, {"description": "This week we turn our attention to prescriptive analytics, where you will provide some concrete suggestions on how to allocate investment funds using analytics tools, including clustering and simulation-based optimization. You will see that allocating funds wisely is crucial for the financial return of the investment portfolio. \n <p>The relevant videos for this week are from Course 3: Week 1: Cluster analysis with XLMiner, Week 2: Adding uncertainty to spreadsheet model, Week 2: Defining output variables and analyzing results. </p>", "video": ["Provide suggestions on how to allocate investment funds using prescriptive analytics tools"], "title": "Module 3 - Provide suggestions on how to allocate investment funds using prescriptive analytics tools"}, {"description": "You have done a lot so far! In this last week, you will present to your analytics results to your clients. Since you have many results in your project, it is important for you to judiciously choose what to include in your presentation. Several videos in Course 4 offer some guidelines on communicating analytics results. This assignment will give you an opportunity to apply the skills you learned there. \nGood luck!", "video": ["Present your analytics results to your clients"], "title": "Module 4  -  Present your analytics results to your clients"}], "title": "Advanced Business Analytics Capstone"}, {"course_info": "About this course: This course provides an introduction to the field of Natural Language Processing. It includes relevant background material in Linguistics, Mathematics, Probabilities, and Computer Science. Some of the topics covered in the class are Text Similarity, Part of Speech Tagging, Parsing, Semantics, Question Answering, Sentiment Analysis, and Text Summarization.\nThe course includes quizzes, programming assignments in Python, and a final exam.\n\nCourse Syllabus\n\nWeek One (Introduction 1/2) (1:35:31)\nWeek Two (Introduction 2/2) (1:36:26)\nWeek Three (NLP Tasks and Text Similarity) (1:42:52)\nWeek Four (Syntax and Parsing, Part 1) (1:48:14)\nWeek Five (Syntax and Parsing, Part 2) (1:50:29)\nWeek Six (Language Modeling and Word Sense Disambiguation) (1:40:33)\nWeek Seven (Part of Speech Tagging and Information Extraction) (1:33:21)\nWeek Eight (Question Answering) (1:16:59)\nWeek Nine (Text Summarization) (1:33:55)\nWeek Ten (Collocations and Information Retrieval) (1:29:40)\nWeek Eleven (Sentiment Analysis and Semantics) (1:09:38)\nWeek Twelve (Discourse, Machine Translation, and Generation) (1:30:57)\n\n\nThe course assignments will all be in Python.\n\n\nCourse Format\n\nThe class will consist of lecture videos, which are typically between 10 and 25 minutes in length. The lectures contain 1-2 integrated quiz questions per video. Grading is based on three programming assignments, weekly quizzes, and a final exam.", "level": "Intermediate", "package_name": null, "created_by": "University of Michigan", "package_num": null, "teach_by": [{"name": "Dragomir R. Radev, Ph.D.", "department": "College of Engineering, School of Information, School of Literature, Science and the Arts"}], "target_audience": "Who is this class for: This class is for students interested in Computational Linguistics and Natural Language Processing. Some previous experience with probabilities will be helpful. \nPrior or concurrent experience with programming, preferably in Python, is expected.\n", "rating": "4.1", "week_data": [{"description": "In Week One, you will be watching an introductory lecture that covers the motivation for NLP, examples of difficult cases, as well as the first part of the Introduction to Linguistics needed for this class. ", "video": ["Course Details", "Help us learn more about you!", "Credits", "Preview: Python dependencies for HW1", "Miscellaneous Notes", "01.01 - Introduction (8:38)", "01.02 - Examples of Text (7:51)", "01.03 - Funny Sentences (6:32) - optional", "01.04 - Administrative (8:06)", "01.05 - Why is NLP hard? (25:55)", "01.06 - Background (16:54)", "01.07 - Linguistics (21:27)", "Quiz 1"], "title": "Week One: Introduction 1/2"}, {"description": "Week Two will cover Parts of Speech, Morphology, Text Similarity, and Text Preprocessing. I will also introduce NACLO, the North American Computational Linguistics Olympiad (www.nacloweb.org), a competition for high school students interested in NLP and Linguistics.", "video": ["Welcome to Week Two", "02.01 - Parts of speech (15:49)", "02.02 - Morphology and the Lexicon (20:20)", "02.03 - Text Similarity: Introduction (7:26)", "02.04 - Morphological Similarity: Stemming (14:54)", "02.05 - Spelling Similarity: Edit Distance (20:53)", "02.06 - NACLO (3:46)", "02.07 - Preprocessing (11:30)", "Quiz 2 (note that this quiz refers to some material taught in Week Three)"], "title": "Week Two: Introduction 2/2"}, {"description": "Week Three will cover Vector Semantics, Text Similarity, and Dimensionality Reduction. I will also go through a long list of sample NLP tasks (e.g., Information Extraction, Text Summarization, and Semantic Role Labeling) and introduce each of them briefly. ", "video": ["Welcome to Week Three", "03.01 - Semantic Similarity: Synonymy and other Semantic Relations (14:16)", "03.02 - Thesaurus-based Word Similarity Methods (7:38)", "03.03 - The Vector Space Model (9:20)", "03.04 - Dimensionality Reduction (23:09)", "03.05 - NLP Tasks 1/3 (14:32)", "03.06 - NLP Tasks 2/3 (15:21)", "03.07 - NLP Tasks 3/3 (16:21)", "Quiz 3"], "title": "Week Three: NLP Tasks and Text Similarity"}, {"description": "Week Four will cover the basics of Syntax and Parsing, including CKY parsing and the Earley parser.", "video": ["Welcome to Week Four", "04.01 - Syntax (31:02)", "04.02 - Parsing (17:12)", "04.03 - Classic Parsing Methods (25:02)", "04.04 - Earley Parser (16:24)", "04.05 - The Penn Treebank (17:39)", "Quiz 4"], "title": "Week Four: Syntax and Parsing, Part 1"}, {"description": "Week Five will continue with topics related to parsing, including Statistical, Lexicalized, and Dependency Parsing as well as Noun Sequence Parsing, Prepositional Phrase Attachment, and Alternative Grammatical Formalisms.", "video": ["Welcome to Week Five", "05.01 - Parsing Introduction and recap/Parsing noun sequences (15:16)", "05.02 - Prepositional phrase attachment 1/3 (14:42) - optional", "05.03 - Prepositional phrase attachment 2/3 (16:44) - optional", "05.04 - Prepositional phrase attachment 3/3 (12:34) - optional", "05.05 - Statistical Parsing (12:44)", "05.06 - Lexicalized Parsing (8:59)", "05.07 - Dependency Parsing (18:57)", "05.08 - Alternative Parsing Formalisms (9:58)", "Quiz 5", "Dependency Parsing - you can start this assignment now"], "title": "Week Five: Syntax and Parsing, Part 2"}, {"description": "Week Six will cover Probabilities, Language Modeling, and Word Sense Disambiguation (WSD). The first two, along with some material coming up in Week Seven, will be the basis for Assignment 2. The WSD unit will be needed later for Assignment 3.", "video": ["Welcome to Week Six", "06.01 - Probabilities (21:11)", "06.02 - Bayes Theorem (10:48)", "06.03 - Language Modeling 1/3 (19:21)", "06.04 - Language Modeling 1/3 (cont'd) (3:05)", "06.05 - Language Modeling 2/3 (10:56)", "06.06 - Language Modeling 3/3 (15:06)", "06.07 - Word Sense Disambiguation (20:06)", "Quiz 6", "Language Modeling and Part of Speech Tagging - you can start this assignment now"], "title": "Week Six: Language Modeling"}, {"description": "Week Seven includes the Noisy Channel Model, Hidden Markov Models, Part of Speech Tagging (all needed for the second programming assignment) and a short introduction to Information Extraction.", "video": ["Welcome to Week Seven", "07.01 - Noisy Channel Model (8:33)", "07.02 - Part of Speech Tagging (17:57)", "07.03 - Hidden Markov Models 1/2 (24:41)", "07.04 - Hidden Markov Models 2/2 (5:28)", "07.05 - Statistical POS Tagging (9:20)", "07.06 - Information Extraction (5:34)", "07.07 - Relation Extraction (21:11)", "Quiz 7", "Word Sense Disambiguation - you can start this assignment now"], "title": "Week Seven: Part of Speech Tagging and Information Extraction"}, {"description": "Week Eight covers different topics related to Question Answering, including Question Type Classification and Evaluation of Question Answering Systems.", "video": ["Welcome to Week Eight", "08.01 - Question Answering (21:20)", "08.02 - Evaluation of QA , System Architecture (21:40)", "08.03 - QA System Architecture (7:56)", "08.04 - Question Answering Systems 1/2 (14:07)", "08.05 - Question Answering Systems 2/2 (10:39)", "Quiz 8"], "title": "Week Eight: Question Answering"}, {"description": "Week Nine covers Text Summarization and related topics such as Sentence Compression.", "video": ["Welcome to Week Nine", "09.01 - Summarization (11:37)", "09.02 - Summarization Techniques 1/3 (19:21)", "09.03 - Summarization Techniques 2/3 (20:12)", "09.04 - Summarization Techniques 3/3 (10:10)", "09.05 - Summarization Evaluation (25:18)", "09.06 - Sentence Simplification (5:18) - optional", "Quiz 9"], "title": "Week Nine: Text Summarization"}, {"description": "Week Ten covers Information Retrieval (including Document Indexing, Ranking, Evaluation), Text Classification and Text Clustering, as well as a short lecture on Collocations.", "video": ["Welcome to Week Ten", "10.01 - Collocations (11:33) - optional", "10.02 - Information Retrieval (21:10)", "10.03 - Evaluation of IR (11:09)", "10.04 - Text Classification (26:07)", "10.05 - Text Clustering (15:19)", "10.06 - Information Retrieval Toolkits (2:23)", "Quiz 10"], "title": "  Week Ten: Collocations and Information Retrieval"}, {"description": "Week Eleven covers Semantics and related topics such as Sentiment Analysis, Semantic Parsing, and Knowledge Representation.", "video": ["Welcome to Week Eleven", "11.01 - Sentiment Analysis (8:43)", "11.02 - Sentiment Lexicons (7:47)", "11.03 - Semantics (6:55)", "11.04 - Representing and Understanding Meaning (9:16)", "11.05 - First Order Logic (7:31)", "11.06 - Knowledge Representation (12:00)", "11.07 - Inference (6:36)", "11.08 - Semantic Parsing (9:17)"], "title": "Week Eleven: Sentiment Analysis and Semantics"}, {"description": "Week Twelve briefly covers Discourse Analysis, Dialogue, Machine Translation, and Text Generation.", "video": ["Welcome to Week Twelve", "12.01 - Discourse Analysis (14:56)", "12.02 - Coherence (16:39)", "12.03 - Dialogue Systems (9:22) - optional", "12.04 - Machine Translation (10:55)", "12.05 - Machine Translation Basic Techniques (11:48)", "12.06 -  Machine Translation Noisy Channel Methods (11:53)", "12.07 - Machine Translation Advanced Methods (9:36)", "12.08 - Text Generation (5:49) - optional", "Post-course Survey", "Final Exam"], "title": " Week Twelve: Discourse, Machine Translation, and Generation (Includes Final Exam)"}], "title": "Introduction to Natural Language Processing"}]