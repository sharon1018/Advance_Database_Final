{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1eef7a9eda44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import collections\n",
    "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from smart_open import smart_open \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from collections import Counter\n",
    "import heapq #check n largest\n",
    "import json\n",
    "import warnings\n",
    "from sqlalchemy import create_engine \n",
    "import spacy\n",
    "import nltk\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath_path = r''\n",
    "\n",
    "computer_security_and_networks = json.load(open(bath_path + r'computer-security-and-networks.json', encoding='utf8'))\n",
    "algorithms = json.load(open(bath_path + r'algorithms.json', encoding='utf8'))\n",
    "cloud_computing = json.load(open(bath_path + r'cloud-computing.json', encoding='utf8'))\n",
    "data_management = json.load(open(bath_path + r'data-management.json', encoding='utf8'))\n",
    "design_and_product = json.load(open(bath_path + r'design-and-product.json', encoding='utf8'))\n",
    "electrical_engineering = json.load(open(bath_path + r'electrical-engineering.json', encoding='utf8'))\n",
    "math_and_logic = json.load(open(bath_path + r'math-and-logic.json', encoding='utf8'))\n",
    "mobile_and_web_development = json.load(open(bath_path + r'mobile-and-web-development.json', encoding='utf8'))\n",
    "networking = json.load(open(bath_path + r'networking.json', encoding='utf8'))\n",
    "probability_and_statistics = json.load(open(bath_path + r'probability-and-statistics.json', encoding='utf8'))\n",
    "security = json.load(open(bath_path + r'security.json', encoding='utf8'))\n",
    "machine_learning = json.load(open(bath_path + r'machine-learning.json', encoding='utf8'))\n",
    "data_analysis = json.load(open(bath_path + r'data-analysis.json', encoding='utf8'))\n",
    "software_development = json.load(open(bath_path + r'software-development.json', encoding='utf8'))\n",
    "\n",
    "all_data = computer_security_and_networks + \\\n",
    "algorithms + \\\n",
    "cloud_computing + \\\n",
    "data_management + \\\n",
    "design_and_product + \\\n",
    "electrical_engineering + \\\n",
    "math_and_logic + \\\n",
    "mobile_and_web_development + \\\n",
    "networking + \\\n",
    "probability_and_statistics + \\\n",
    "security + \\\n",
    "machine_learning + \\\n",
    "data_analysis + \\\n",
    "software_development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]; week_data=[]; target_audience=[]; created_by=[]; teach_by=[]\n",
    "week_data_title=[]; course_info=[]; department=[]; week_data_desc=[]\n",
    "package_num=[];package_name=[]; level=[]; rating=[]; index=[]\n",
    "# teach_by, week_data: 多值\n",
    "for n in range(len(all_data)):\n",
    "    for i in range(len(all_data[n]['teach_by'])):\n",
    "            title.append(all_data[n]['title'])\n",
    "            target_audience.append(all_data[n]['target_audience'])\n",
    "            created_by.append(all_data[n]['created_by'])\n",
    "            course_info.append(all_data[n]['course_info'])\n",
    "            package_num.append(all_data[n]['package_num'])\n",
    "            package_name.append(all_data[n]['package_name'])\n",
    "            level.append(all_data[n]['level'])\n",
    "            rating.append(all_data[n]['rating'])\n",
    "            teach_by.append(list((all_data[n]['teach_by'][i].values()))[0])  \n",
    "            department.append(list((all_data[n]['teach_by'][i].values()))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'title': title,\n",
    "                    'target_audience': target_audience,\n",
    "                    'created_by' : created_by,  \n",
    "                    'course_info': course_info,\n",
    "                    'teach_by': teach_by,\n",
    "                     'department': department,\n",
    "                     'level': level,\n",
    "                     'rating': rating,\n",
    "                     'package_num': package_num,\n",
    "                     'package_name': package_name          \n",
    "                    },\n",
    "                    columns = ['title', 'course_info', 'target_audience', 'created_by','teach_by',\n",
    "                               'department','package_num','package_name', 'level','rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in data.iterrows():\n",
    "    if not isEnglish(row.title):\n",
    "        data.drop(i, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.title.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]; created_by=[]; course_info=[]\n",
    "data_title=[]; data_desc=[]; data_video=[]\n",
    "# teach_by, week_data: 多值\n",
    "for n in range(len(all_data)):\n",
    "    week_data_title=[];  week_data_desc=[];week_data_video=[]\n",
    "    title.append(all_data[n]['title'])\n",
    "    created_by.append(all_data[n]['created_by'])\n",
    "    for i in range(len(all_data[n]['week_data'])):\n",
    "        week_data_desc.append(all_data[n]['week_data'][i]['description'])\n",
    "        week_data_title.append(all_data[n]['week_data'][i]['title'])\n",
    "        week_data_video += all_data[n]['week_data'][i]['video']\n",
    "    data_title.append(week_data_title)\n",
    "    data_desc.append(week_data_desc)\n",
    "    data_video.append(week_data_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_created_by = pd.DataFrame({'title': title,\n",
    "                    'created_by':created_by},\n",
    "                    columns = ['title', 'created_by'])\n",
    "\n",
    "data_detail = pd.DataFrame({     \n",
    "                    'week_data_desc': data_desc,\n",
    "                    'week_data_title': data_title,\n",
    "                    'week_data_video': data_video\n",
    "                    },\n",
    "                    columns = ['week_data_desc', 'week_data_title','week_data_video'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_created_by.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_created_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in data_created_by.iterrows():\n",
    "    if not isEnglish(row.title):\n",
    "        data_created_by.drop(i, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_detail = pd.merge(data_created_by, data_detail, left_index=True, right_index=True, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in data_detail.iterrows():\n",
    "    if len(row.week_data_video) == 0:\n",
    "        data_detail.drop(i, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_detail = data_detail.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_video_list = []\n",
    "for video_data in data_detail.week_data_video:\n",
    "    video_str = ''\n",
    "    x = ' '.join(video_data)\n",
    "    doc = nlp(x.lower())\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.is_alpha:\n",
    "            video_str += token.lemma_\n",
    "            video_str += ' '\n",
    "    week_video_list.append(video_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_detail['week_data_video_str'] = pd.Series(week_video_list)\n",
    "data_detail.dropna(inplace=True)\n",
    "data_detail = data_detail.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorize = TfidfVectorizer(max_df=150)\n",
    "bows = vectorize.fit(data_detail.week_data_video_str)\n",
    "document_matrix = bows.transform(data_detail.week_data_video_str).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bows.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "result = cosine_similarity(document_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_detail.loc[np.argsort(result[20])[::-1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "svd = TruncatedSVD(100)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "X = lsa.fit_transform(document_matrix)\n",
    "inertia_list = []  \n",
    "for i in range(2,15):\n",
    "    kmeans = KMeans(n_clusters=i).fit(X)\n",
    "    inertia_list.append(kmeans.inertia_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(2,15), inertia_list, linestyle='-', marker='o')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IsDuplicated = data_to_mysql.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate_course=0\n",
    "# for i in range(len(data_to_mysql)):\n",
    "#     if IsDuplicated[i] == True:\n",
    "#         duplicate_course=duplicate_course+1\n",
    "#         #print(i)\n",
    "# print(duplicate_course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data_to_mysql.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_to_mysql.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v=[]\n",
    "# for n in range(len(all_data)):\n",
    "#     for j in range(len(all_data[n]['week_data'])):     \n",
    "#         v.append(list((all_data[n]['week_data'][j]).values())[0])\n",
    "#         #a.append(pd.Series(v).str.cat(sep='')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v= pd.Series(v).str.cat(sep='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
